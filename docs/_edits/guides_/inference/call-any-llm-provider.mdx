---
title: "Call any LLM provider"
sidebarTitle: "Call any LLM provider"
description: "TensorZero provides a unified inference API that supports every major LLM provider, including API-based and open-source models."
---

Do you want to use multiple LLM providers (e.g. OpenAI, Anthropic, Google) but don't want to manage separate integrations for each provider?

This starter guide shows how to use the TensorZero Gateway to access every major LLM provider with a unified API.

## How it works

- You can use it with the TensorZero Python client, with any OpenAI SDK (Python, Node, Go, etc.), or via its HTTP API in any programming language.
- TensorZero supports every major LLM provider, including API models (e.g. OpenAI, Anthropic, Google) and open-source models (e.g. vLLM, Ollama, SGlang).
  See Integrations for a complete list.
- The gateway is blazing fast (&lt;1ms latency overhead, thanks to Rust) and offers additional functionality like X, Y, Z... The broader TensorZero stack also offers X, Y, Z.

## Make an inference API call

TODO: Native vs. OpenAI Compatible

TODO: Short-hand vs. long-form models

<Steps>
  <Step title="Generate credentials for your LLM providers">
    Generate credentials (e.g. API keys) for the LLM providers you're planning to use.

    For example, if you're planning to use LLMs from OpenAI, you should generate an API key on the OpenAI API dashboard.

    See Model Providers (TODO) for a list of supported providers and the relevant credentials.

  </Step>
  <Step title="Set up TensorZero">
    If you're building in Python, you only need to install the `tensorzero` library.
    For other programming languages, you can run the gateway as a lightweight standalone Docker container.

    <Tabs>
      <Tab title="Python">


        Install the TensorZero Python client:

        ```bash
        pip install tensorzero
        ```

        TODO: Set the `OPENAI_API_KEY`
      </Tab>
      <Tab title="Docker CLI">
        x
      </Tab>
      <Tab title="Docker Compose">
        ```yaml
        x
        ```


      </Tab>
    </Tabs>

    In this simple setup, we're not using ...
    See the TensorZero Gateway Deployment Guide for production-grade deployments and other details.

  </Step>
  <Step title="Make an inference API call">

    <Tabs>
      <Tab title="Python">

        ```python Call any LLM with the TensorZero Python client
        from tensorzero import TensorZeroGateway  # or AsyncTensorZeroGateway

        t0 = TensorZeroGateway.build_embedded()

        response = t0.inference(
            model_name="openai::gpt-4o-mini",
            input={
                "system": "You are a masterful poet.",
                "messages": [
                    {
                        "role": "user",
                        "content": "Write a haiku about artificial intelligence.",
                    }
                ]
            },
        )

        print(response)
        ```

      </Tab>
      <Tab title="Python (OpenAI SDK)">
        ...

        ```python Call any LLM with the OpenAI Python SDK
        from openai import OpenAI  # or AsyncOpenAI
        from tensorzero import patch_openai_client

        client = OpenAI()
        patch_openai_client(client, async_setup=False)

        response = client.chat.completions.create(
            model="tensorzero::model_name::openai::gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are a masterful poet.",
                },
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ],
        )

        print(response)
        ```

      </Tab>
      <Tab title="Node (OpenAI SDK)">
        ...

        ```typescript Call any LLM with the OpenAI Node SDK
        import OpenAI from "openai";

        const client = new OpenAI({
          baseURL: "http://localhost:3000/openai/v1",
        });

        const response = await client.chat.completions.create({
          model: "tensorzero::model_name::openai::gpt-4o-mini",
          messages: [
            {
              "role": "system",
              "content": "You are a masterful poet.",
            },
            {
              role: "user",
              content: "Write a haiku about artificial intelligence.",
            },
          ],
        });

        console.log(JSON.stringify(response, null, 2));
        ```

      </Tab>
      <Tab title="Go (OpenAI SDK)">
        ...

        ```go Call any LLM with the OpenAI Go SDK
        ...
        ```

      </Tab>
      <Tab title="HTTP API">
        ...

        ```bash Call any LLM with our HTTP API using any programming language
        curl -X POST "http://localhost:3000/inference" \
          -H "Content-Type: application/json" \
          -d '{
            "model_name": "openai::gpt-4o-mini",
            "input": {
              "messages": [
                {
                  "role": "system",
                  "content": "You are a masterful poet."
                },
                {
                  "role": "user",
                  "content": "Write a haiku about artificial intelligence."
                }
              ]
            }
          }'
        ```

      </Tab>
    </Tabs>

  </Step>

  <Step title="Call an LLM from another provider">
    You can calll...

    For example, you can use `anthropic:...` (make sure to set up `ANTHROPIC_API_KEY`).

    Some providers (e.g. GCP Vertex AI) require a bit of configuration.
    See XXX for guides for each provider.

  </Step>
</Steps>
