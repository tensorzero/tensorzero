---
title: TensorZero Gateway Deployment Guide
sidebarTitle: Deployment
description: Learn how to deploy the TensorZero Gateway.
---

It's easy to get started with the TensorZero Gateway.

To deploy the TensorZero Gateway, you need to:

- Setup a ClickHouse database
- Optional: Create a [configuration file](/gateway/configuration-reference/)
- Run the gateway

<Tip>

See the [TensorZero UI Deployment Guide](/ui/deployment/) for more details on how to deploy the TensorZero UI.

</Tip>

## ClickHouse (optional)

The TensorZero Gateway can optionally collect inference and feedback data for observability, optimization, evaluations, and experimentation.

After setting up your database, you need to configure the `TENSORZERO_CLICKHOUSE_URL` environment variable with the connection details.
See the [Deploy ClickHouse](/deployment/deploy-clickhouse) guide for details.

See `[gateway.observability]` in the [Configuration Reference](/gateway/configuration-reference/) for the relevant configuration (e.g. customizing the port).

<Accordion title="Disabling Observability (Not Recommended)">

You can disable observability features if you're not interested in storing any data for experimentation and optimization.
In this case, you won't need to set up ClickHouse, and the TensorZero Gateway will act as a simple model gateway.

To disable observability, set the following configuration in the `tensorzero.toml` file:

```toml title="tensorzero.toml"
[gateway]
observability.enabled = false
```

If you only need to disable observability temporarily, you can pass a `dryrun: true` parameter to the inference and feedback API endpoints.

</Accordion>

<Accordion title="Disabling Pseudonymous Usage Analytics">

TensorZero collects _pseudonymous_ usage analytics to help our team improve the product.

The collected data includes _aggregated_ metrics about TensorZero itself, but does NOT include your application's data.
To be explicit: TensorZero does NOT share any inference input or output.
TensorZero also does NOT share the name of any function, variant, metric, or similar application-specific identifiers.

See `howdy.rs` in the GitHub repository to see exactly what usage data is collected and shared with TensorZero.

To disable usage analytics, set the following configuration in the `tensorzero.toml` file:

```toml title="tensorzero.toml"
[gateway]
disable_pseudonymous_usage_analytics = true
```

Alternatively, you can also set the environment variable `TENSORZERO_DISABLE_PSEUDONYMOUS_USAGE_ANALYTICS=1`.

</Accordion>

## TensorZero Gateway

<Tip>

**The TensorZero Python client includes a built-in embedded gateway, so you don't need to run a separate service for it.**
The gateway is only needed if you want to use the OpenAI Python client or interact with TensorZero via its HTTP API (for other programming languages).
The TensorZero UI also requires the gateway service.

</Tip>

### Development

<Accordion title="Running with Docker (Recommended)" defaultOpen="true">

You can easily run the TensorZero Gateway locally using Docker.

You need to provide it with a path to a folder containing your `tensorzero.toml` file as well as its dependencies (e.g. schemas and templates), as well as the environment variables discussed above.

```bash title="Running with Docker"
docker run \
  --name tensorzero-gateway \
  -v "./config:/app/config" \
  --env-file .env \
  -p 3000:3000 \
  -d \
  tensorzero/gateway
```

</Accordion>

<Accordion title="Building from source">

Alternatively, you can build the TensorZero Gateway from source and run it directly on your host machine using <a href="https://doc.rust-lang.org/cargo/" target="_blank">Cargo</a>:

```bash title="Building from source"
cargo run --release --bin gateway -- path/to/your/tensorzero.toml
```

</Accordion>

### Production

You can deploy the TensorZero Gateway alongside your application (e.g. as a sidecar container) or as a standalone service.

A single gateway instance can handle over 1k QPS/core with sub-millisecond latency (see [Benchmarks](/gateway/benchmarks/)), so a simple deployment should suffice for the vast majority of applications. If you deploy it as an independent service, we recommend deploying at least two instances behind a load balancer for high availability. The gateway is stateless, so you can easily scale horizontally and don't need to worry about persistence.

<Accordion title="Running with Docker (Recommended)" defaultOpen="true">

The recommended way to run the TensorZero Gateway in production is to use Docker.

There are many ways to run Docker containers in production.
A simple solution is to use Docker Compose.
We provide an example [`docker-compose.yml`](https://github.com/tensorzero/tensorzero/blob/main/examples/production-deployment/docker-compose.yml) for reference.

</Accordion>

<Accordion title="Running with Kubernetes (k8s) and Helm">

We provide a reference Helm chart _contributed by the community_ in our [GitHub repository](https://github.com/tensorzero/tensorzero/tree/main/examples/production-deployment-k8s-helm).
You can use it to run TensorZero in Kubernetes.

</Accordion>

<Accordion title="Building from source">

Alternatively, you can build the TensorZero Gateway from source and run it directly on your host machine using <a href="https://doc.rust-lang.org/cargo/" target="_blank">Cargo</a>.
For production deployments, we recommend enabling performance optimizations:

```bash title="Building from source"
cargo run --profile performance --bin gateway -- path/to/your/tensorzero.toml
```

</Accordion>

<Tip>

See the [optimizing latency and throughput](/deployment/optimize-latency-and-throughput/) guide to learn how to configure the gateway for high-performance deployments.

</Tip>

### Command Line Arguments

The TensorZero Gateway requires either `--config-file` to specify a custom configuration file (e.g. `--config-file /path/to/tensorzero.toml`) or `--default-config` to use default settings (i.e. no custom functions, metrics, etc.). You can also use `--log-format` to set the logging format to either `pretty` (default) or `json`.

### Clients

See the [Clients](/gateway/clients/) page for more details on how to interact with the TensorZero Gateway.

### Configuration

To run the TensorZero Gateway, first you need to create a `tensorzero.toml` configuration file. Read more about the configuration file [here](/gateway/configuration-reference/).

### Model Provider Credentials

In addition to the `TENSORZERO_CLICKHOUSE_URL` environment variable discussed above, the TensorZero Gateway accepts the following environment variables for provider credentials.
Unless you specify an alternative credential location in your configuration file, these environment variables are required for the providers that are used in a variant with positive weight.
If required credentials are missing, the gateway will fail on startup.

| Provider                | Environment Variable(s)                                               |
| ----------------------- | --------------------------------------------------------------------- |
| Anthropic               | `ANTHROPIC_API_KEY`                                                   |
| AWS Bedrock             | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` (optional) |
| AWS SageMaker           | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` (optional) |
| Azure OpenAI            | `AZURE_OPENAI_API_KEY`                                                |
| Fireworks               | `FIREWORKS_API_KEY`                                                   |
| GCP Vertex AI Anthropic | `GCP_VERTEX_CREDENTIALS_PATH` (see below for details)                 |
| GCP Vertex AI Gemini    | `GCP_VERTEX_CREDENTIALS_PATH` (see below for details)                 |
| Google AI Studio Gemini | `GOOGLE_AI_STUDIO_GEMINI_API_KEY`                                     |
| Groq                    | `GROQ_API_KEY`                                                        |
| Hyperbolic              | `HYPERBOLIC_API_KEY`                                                  |
| Mistral                 | `MISTRAL_API_KEY`                                                     |
| OpenAI                  | `OPENAI_API_KEY`                                                      |
| OpenRouter              | `OPENROUTER_API_KEY`                                                  |
| Together                | `TOGETHER_API_KEY`                                                    |
| xAI                     | `XAI_API_KEY`                                                         |

**Notes:**

- AWS Bedrock supports many authentication methods, including environment variables, IAM roles, and more. See the AWS documentation for more details.
- If you're using the GCP Vertex provider, you also need to mount the credentials for a service account in JWT form (described [here](https://cloud.google.com/docs/authentication/provide-credentials-adc#service-account)) to `/app/gcp-credentials.json` using an additional `-v` flag.

<Tip>

See [`.env.example`](https://github.com/tensorzero/tensorzero/blob/main/examples/production-deployment/.env.example) for a complete example with every supported environment variable.

</Tip>
