---
title: TensorZero Gateway Deployment Guide
sidebarTitle: Deployment
description: Learn how to deploy the TensorZero Gateway.
---

It's easy to get started with the TensorZero Gateway.

To deploy the TensorZero Gateway, you need to:

- Setup a ClickHouse database
- Optional: Create a [configuration file](/gateway/configuration-reference/)
- Run the gateway

<Tip>

See the [TensorZero UI Deployment Guide](/ui/deployment/) for more details on how to deploy the TensorZero UI.

</Tip>

## ClickHouse

The TensorZero Gateway stores inference and feedback data in a ClickHouse database.
This data is later used for model observability, experimentation, and optimization.

### Development

For development purposes, you can run a single-node ClickHouse instance locally (e.g. using Homebrew or Docker) or a cheap `Development`-tier cluster on ClickHouse Cloud.

See the <a href="https://clickhouse.com/docs/en/install" target="_blank">ClickHouse documentation</a> for more details on configuring your ClickHouse deployment.

### Production

#### Managed Services

For production deployments, the easiest setup is to use a managed service like <a href="https://clickhouse.com/cloud" target="_blank">ClickHouse Cloud</a>.

ClickHouse Cloud is also available through the <a href="https://aws.amazon.com/marketplace/pp/prodview-jettukeanwrfc" target="_blank">AWS Marketplace</a>, <a href="https://console.cloud.google.com/marketplace/product/clickhouse-public/clickhouse-cloud" target="_blank">GCP Marketplace</a>, and <a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/clickhouse.clickhouse_cloud" target="_blank">Azure Marketplace</a>.

Other options for managed ClickHouse deployments include <a href="https://www.tinybird.co/" target="_blank">Tinybird</a> (serverless) and <a href="https://www.altinity.com/" target="_blank">Altinity</a> (hands-on support).

#### Self-Managed Deployment

You can alternatively run your own self-managed ClickHouse instance or cluster.

<Warning>

The TensorZero Gateway does _not_ automatically enable data replication for ClickHouse tables.

If you are using a _self-managed distributed_ ClickHouse deployment, you must set up replication yourself.
See the ClickHouse [replication documentation](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication#converting-from-mergetree-to-replicatedmergetree) for more details.

ClickHouse Cloud automatically sets up data replication, so this step is not necessary if you're using the managed service.

</Warning>

### Configuration

After setting up your database, you need to configure the `TENSORZERO_CLICKHOUSE_URL` environment variable with the connection details.
The variable takes a standard format.

```bash title=".env"
TENSORZERO_CLICKHOUSE_URL="http[s]://[username]:[password]@[hostname]:[port]/[database]"

# Example: ClickHouse running locally
TENSORZERO_CLICKHOUSE_URL="http://chuser:chpassword@localhost:8123/tensorzero"

# Example: ClickHouse Cloud
TENSORZERO_CLICKHOUSE_URL="https://USERNAME:PASSWORD@XXXXX.clickhouse.cloud:8443/tensorzero"

# Example: TensorZero Gateway running in a container, ClickHouse running on host machine
TENSORZERO_CLICKHOUSE_URL="http://host.docker.internal:8123/tensorzero"
```

The TensorZero Gateway automatically applies database migrations on startup.

See `[gateway]` in the [Configuration Reference](/gateway/configuration-reference/) for the relevant configuration (e.g. customizing the port).

<Accordion title="Disabling Observability (Not Recommended)">

You can disable observability features if you're not interested in storing any data for experimentation and optimization.
In this case, you won't need to set up ClickHouse, and the TensorZero Gateway will act as a simple model gateway.

To disable observability, set the following configuration in the `tensorzero.toml` file:

```toml
// tensorzero.toml
[gateway]
observability.enabled = false
```

If you only need to disable observability temporarily, you can pass a `dryrun: true` parameter to the inference and feedback API endpoints.

</Accordion>

<Accordion title="Disabling Pseudonymous Usage Analytics">

TensorZero collects _pseudonymous_ usage analytics to help our team improve the product.

The collected data includes _aggregated_ metrics about TensorZero itself, but does NOT include your application's data.
To be explicit: TensorZero does NOT share any inference input or output.
TensorZero also does NOT share the name of any function, variant, metric, or similar application-specific identifiers.

See `howdy.rs` in the GitHub repository to see exactly what usage data is collected and shared with TensorZero.

You can disable usage analytics by setting the environment variable `TENSORZERO_DISABLE_PSEUDONYMOUS_USAGE_ANALYTICS=1`.

</Accordion>

## TensorZero Gateway

<Tip>

**The TensorZero Python client includes a built-in embedded gateway, so you don't need to run a separate service for it.**
The gateway is only needed if you want to use the OpenAI Python client or interact with TensorZero via its HTTP API (for other programming languages).
The TensorZero UI also requires the gateway service.

</Tip>

### Development

<Accordion title="Running with Docker (Recommended)" defaultOpen="true">

You can easily run the TensorZero Gateway locally using Docker.

You need to provide it with a path to a folder containing your `tensorzero.toml` file as well as its dependencies (e.g. schemas and templates), as well as the environment variables discussed above.

```bash title="Running with Docker"
docker run \
  --name tensorzero-gateway \
  -v "./config:/app/config" \
  --env-file .env \
  -p 3000:3000 \
  -d \
  tensorzero/gateway
```

</Accordion>

<Accordion title="Building from source">

Alternatively, you can build the TensorZero Gateway from source and run it directly on your host machine using <a href="https://doc.rust-lang.org/cargo/" target="_blank">Cargo</a>:

```bash title="Building from source"
cargo run --release --bin gateway -- path/to/your/tensorzero.toml
```

</Accordion>

### Production

You can deploy the TensorZero Gateway alongside your application (e.g. as a sidecar container) or as a standalone service.

A single gateway instance can handle over 1k QPS/core with sub-millisecond latency (see [Benchmarks](/gateway/benchmarks/)), so a simple deployment should suffice for the vast majority of applications. If you deploy it as an independent service, we recommend deploying at least two instances behind a load balancer for high availability. The gateway is stateless, so you can easily scale horizontally and don't need to worry about persistence.

<Accordion title="Running with Docker (Recommended)" defaultOpen="true">

The recommended way to run the TensorZero Gateway in production is to use Docker.

There are many ways to run Docker containers in production.
A simple solution is to use Docker Compose.
We provide an example [`docker-compose.yml`](https://github.com/tensorzero/tensorzero/blob/main/examples/production-deployment/docker-compose.yml) for reference.

</Accordion>

<Accordion title="Running with Kubernetes (k8s) and Helm">

We provide a reference Helm chart _contributed by the community_ in our [GitHub repository](https://github.com/tensorzero/tensorzero/tree/main/examples/production-deployment-k8s-helm).
You can use it to run TensorZero in Kubernetes.

</Accordion>

<Accordion title="Building from source">

Alternatively, you can build the TensorZero Gateway from source and run it directly on your host machine using <a href="https://doc.rust-lang.org/cargo/" target="_blank">Cargo</a>.
For production deployments, we recommend enabling performance optimizations:

```bash title="Building from source"
cargo run --profile performance --bin gateway -- path/to/your/tensorzero.toml
```

</Accordion>

### Command Line Arguments

The TensorZero Gateway requires either `--config-file` to specify a custom configuration file (e.g. `--config-file /path/to/tensorzero.toml`) or `--default-config` to use default settings (i.e. no custom functions, metrics, etc.). You can also use `--log-format` to set the logging format to either `pretty` (default) or `json`.

### Clients

See the [Clients](/gateway/clients/) page for more details on how to interact with the TensorZero Gateway.

### Configuration

To run the TensorZero Gateway, first you need to create a `tensorzero.toml` configuration file. Read more about the configuration file [here](/gateway/configuration-reference/).

### Model Provider Credentials

In addition to the `TENSORZERO_CLICKHOUSE_URL` environment variable discussed above, the TensorZero Gateway accepts the following environment variables for provider credentials.
Unless you specify an alternative credential location in your configuration file, these environment variables are required for the providers that are used in a variant with positive weight.
If required credentials are missing, the gateway will fail on startup.

| Provider                | Environment Variable(s)                                               |
| ----------------------- | --------------------------------------------------------------------- |
| Anthropic               | `ANTHROPIC_API_KEY`                                                   |
| AWS Bedrock             | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` (optional) |
| AWS SageMaker           | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` (optional) |
| Azure OpenAI            | `AZURE_OPENAI_API_KEY`                                                |
| Fireworks               | `FIREWORKS_API_KEY`                                                   |
| GCP Vertex AI Anthropic | `GCP_VERTEX_CREDENTIALS_PATH` (see below for details)                 |
| GCP Vertex AI Gemini    | `GCP_VERTEX_CREDENTIALS_PATH` (see below for details)                 |
| Google AI Studio Gemini | `GOOGLE_AI_STUDIO_GEMINI_API_KEY`                                     |
| Groq                    | `GROQ_API_KEY`                                                        |
| Hyperbolic              | `HYPERBOLIC_API_KEY`                                                  |
| Mistral                 | `MISTRAL_API_KEY`                                                     |
| OpenAI                  | `OPENAI_API_KEY`                                                      |
| OpenRouter              | `OPENROUTER_API_KEY`                                                  |
| Together                | `TOGETHER_API_KEY`                                                    |
| xAI                     | `XAI_API_KEY`                                                         |

**Notes:**

- AWS Bedrock supports many authentication methods, including environment variables, IAM roles, and more. See the AWS documentation for more details.
- If you're using the GCP Vertex provider, you also need to mount the credentials for a service account in JWT form (described [here](https://cloud.google.com/docs/authentication/provide-credentials-adc#service-account)) to `/app/gcp-credentials.json` using an additional `-v` flag.

<Tip>

See [`.env.example`](https://github.com/tensorzero/tensorzero/blob/main/examples/production-deployment/.env.example) for a complete example with every supported environment variable.

</Tip>
