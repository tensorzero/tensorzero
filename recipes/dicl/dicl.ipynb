{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic In-Context Learning\n",
    "\n",
    "This recipe allows TensorZero users to set up a dynamic in-context learning variant for any function.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to query a set of good examples and retrieve the most relevant ones to put them into context for future inferences.\n",
    "Since TensorZero allows users to add demonstrations for any inference it is also easy to include them in the set of examples as well.\n",
    "This recipe will show use the OpenAI embeddings API only, but we are working towards support for all embedding providers over time as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Set the `CLICKHOUSE_URL` environment variable. For example: `CLICKHOUSE_URL=\"http://localhost:8123/tensorzero\"`\n",
    "- Set the `OPENAI_API_KEY` environment variable.\n",
    "- Update the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "CONFIG_PATH = \"../../examples/haiku-dicl/config/tensorzero.toml\"\n",
    "\n",
    "\n",
    "FUNCTION_NAME = \"write_haiku\"\n",
    "\n",
    "# Can also set this to None if you do not want to use a metric and only want to use demonstrations\n",
    "METRIC_NAME: Optional[str] = \"haiku_score\"\n",
    "\n",
    "# The name of the DICL variant you will want to use. Set this to a meaningful name that does not conflict\n",
    "# with other variants for the function selected above.\n",
    "DICL_VARIANT_NAME = \"haiku_gpt4o_mini_dicl\"\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Whether to use demonstrations for DICL examples\n",
    "USE_DEMONSTRATIONS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from asyncio import Semaphore\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from openai import AsyncOpenAI\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from uuid_extensions import uuid7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the function we are interested in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert (\n",
    "    FUNCTION_NAME in config[\"functions\"]\n",
    "), f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    "\n",
    "function_config = config[\"functions\"][FUNCTION_NAME]\n",
    "function_type = function_config[\"type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the metric configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if METRIC_NAME is None:\n",
    "    metric = None\n",
    "else:\n",
    "    assert \"metrics\" in config, \"No `[metrics]` section found in config\"\n",
    "    assert (\n",
    "        METRIC_NAME in config[\"metrics\"]\n",
    "    ), f\"No metric named `{METRIC_NAME}` found in config\"\n",
    "    metric = config[\"metrics\"][METRIC_NAME]\n",
    "\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"CLICKHOUSE_URL\" in os.environ, \"CLICKHOUSE_URL environment variable not set\"\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    function_type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_table_name = (\n",
    "    {\n",
    "        \"float\": \"FloatMetricFeedback\",\n",
    "        \"boolean\": \"BooleanMetricFeedback\",\n",
    "    }.get(metric[\"type\"])\n",
    "    if metric is not None\n",
    "    else None\n",
    ")\n",
    "\n",
    "if feedback_table_name is None and metric is not None:\n",
    "    raise ValueError(f\"Unsupported metric type: {metric['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the correct join key to use for the metric on the inference table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_join_key = (\n",
    "    {\n",
    "        \"episode\": \"episode_id\",\n",
    "        \"inference\": \"id\",\n",
    "    }.get(metric[\"level\"])\n",
    "    if metric is not None\n",
    "    else None\n",
    ")\n",
    "\n",
    "if inference_join_key is None and metric is not None:\n",
    "    raise ValueError(f\"Unsupported metric level: {metric['level']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if metric is not None:\n",
    "    assert \"optimize\" in metric, \"Metric is missing the `optimize` field\"\n",
    "\n",
    "    threshold = FLOAT_METRIC_THRESHOLD if metric[\"type\"] == \"float\" else 0.5\n",
    "    comparison_operator = \">=\" if metric[\"optimize\"] == \"max\" else \"<=\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        i.input, \n",
    "        i.output, \n",
    "    FROM \n",
    "        {inference_table_name} i\n",
    "    JOIN \n",
    "        (SELECT\n",
    "            target_id,\n",
    "            value,\n",
    "            ROW_NUMBER() OVER (PARTITION BY target_id ORDER BY timestamp DESC) as rn\n",
    "        FROM \n",
    "            {feedback_table_name}\n",
    "        WHERE\n",
    "            metric_name = %(metric_name)s\n",
    "            AND value {comparison_operator} %(threshold)s\n",
    "        ) f ON i.{inference_join_key} = f.target_id and f.rn = 1\n",
    "    WHERE \n",
    "        i.function_name = %(function_name)s\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"function_name\": FUNCTION_NAME,\n",
    "        \"metric_name\": METRIC_NAME,\n",
    "        \"comparison_operator\": comparison_operator,\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "    metric_df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "    metric_df.head()\n",
    "else:\n",
    "    metric_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    i.input, \n",
    "    f.value AS output\n",
    "FROM \n",
    "    {inference_table_name} i\n",
    "JOIN \n",
    "    (SELECT\n",
    "        inference_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY inference_id ORDER BY timestamp DESC) as rn\n",
    "    FROM \n",
    "        DemonstrationFeedback\n",
    "    ) f ON i.id = f.inference_id AND f.rn = 1\n",
    "WHERE \n",
    "    i.function_name = %(function_name)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "}\n",
    "\n",
    "if USE_DEMONSTRATIONS:\n",
    "    demonstration_df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "    demonstration_df.head()\n",
    "else:\n",
    "    demonstration_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine metric_df and demonstration_df into example_df\n",
    "example_df = pd.concat(\n",
    "    [df for df in [metric_df, demonstration_df] if df is not None], ignore_index=True\n",
    ")\n",
    "\n",
    "# Assert that at least one of the dataframes is not None\n",
    "assert (\n",
    "    example_df is not None and not example_df.empty\n",
    "), \"Both metric_df and demonstration_df are None or empty\"\n",
    "\n",
    "# Display the first few rows of the combined dataframe\n",
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_embedding(\n",
    "    text: str, semaphore: Semaphore, model: str = \"text-embedding-3-small\"\n",
    ") -> Optional[list[float]]:\n",
    "    try:\n",
    "        async with semaphore:\n",
    "            response = await openai_client.embeddings.create(input=text, model=model)\n",
    "            return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_EMBEDDING_REQUESTS = 50\n",
    "semaphore = Semaphore(MAX_CONCURRENT_EMBEDDING_REQUESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the 'input' column using the get_embedding function\n",
    "tasks = [\n",
    "    get_embedding(str(input_text), semaphore) for input_text in example_df[\"input\"]\n",
    "]\n",
    "embeddings = await tqdm_asyncio.gather(*tasks, desc=\"Embedding inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the embeddings as a new column to the dataframe\n",
    "example_df[\"embedding\"] = embeddings\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "print(example_df[[\"input\", \"embedding\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for the DiclExample table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'function_name' with the value FUNCTION_NAME for every row\n",
    "example_df[\"function_name\"] = FUNCTION_NAME\n",
    "\n",
    "# Overwrite the 'variant_name' column with the value DICL_VARIANT_NAME for every row\n",
    "example_df[\"variant_name\"] = DICL_VARIANT_NAME\n",
    "\n",
    "# Add a new column 'id' with a UUID for every row\n",
    "example_df[\"id\"] = [uuid7() for _ in range(len(example_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the data into the DiclExample table\n",
    "clickhouse_client.insert_df(\n",
    "    \"DiclExample\",\n",
    "    example_df,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
