{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune OpenAI models using their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Set the `CLICKHOUSE_URL` environment variable. For example: `CLICKHOUSE_URL=\"http://localhost:8123/tensorzero\"`\n",
    "- Set the `OPENAI_API_KEY` environment variable.\n",
    "- Update the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../examples/haiku-hidden-preferences/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"write_haiku\"\n",
    "\n",
    "METRIC_NAME = \"haiku_score\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"initial_prompt_gpt4o_mini\"\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://platform.openai.com/docs/guides/fine-tuning)\n",
    "MODEL_NAME = \"gpt-4o-mini-2024-07-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the metric configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'boolean', 'level': 'inference', 'optimize': 'max'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert \"metrics\" in config, \"No `[metrics]` section found in config\"\n",
    "assert (\n",
    "    METRIC_NAME in config[\"metrics\"]\n",
    "), f\"No metric named `{METRIC_NAME}` found in config\"\n",
    "\n",
    "metric = config[\"metrics\"][METRIC_NAME]\n",
    "\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'chat_completion',\n",
       " 'weight': 0,\n",
       " 'model': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_template': 'functions/write_haiku/initial_prompt/system_template.minijinja',\n",
       " 'user_template': 'functions/write_haiku/initial_prompt/user_template.minijinja'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert (\n",
    "    FUNCTION_NAME in config[\"functions\"]\n",
    "), f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    "assert (\n",
    "    \"variants\" in config[\"functions\"][FUNCTION_NAME]\n",
    "), f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    "assert (\n",
    "    TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"]\n",
    "), f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"CLICKHOUSE_URL\" in os.environ, \"CLICKHOUSE_URL environment variable not set\"\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_table_name = {\n",
    "    \"float\": \"FloatMetricFeedback\",\n",
    "    \"boolean\": \"BooleanMetricFeedback\",\n",
    "}.get(metric[\"type\"])\n",
    "\n",
    "if feedback_table_name is None:\n",
    "    raise ValueError(f\"Unsupported metric type: {metric['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the correct join key to use for the metric on the inference table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_join_key = {\n",
    "    \"episode\": \"episode_id\",\n",
    "    \"inference\": \"id\",\n",
    "}.get(metric[\"level\"])\n",
    "\n",
    "if inference_join_key is None:\n",
    "    raise ValueError(f\"Unsupported metric level: {metric['level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences and feedback from ClickHouse.\n",
    "\n",
    "If the metric is a float metric, we need to filter the data based on the threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant_name</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>value</th>\n",
       "      <th>episode_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"\\\"Capacity\\\" evokes th...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-3de0-7652-827c-7f2114fc584b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"Preoccupation can ofte...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-ae66-7833-8763-7d15cc6853ae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"Neurons spark and fire...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-4c08-7e20-87d2-27f7d5f6a5c2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"A sunbonnet is a charm...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-17f4-7231-8838-8c10b31a8a04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"Length can be perceive...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-041f-7383-891f-cd13727fec7b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                variant_name  \\\n",
       "0  initial_prompt_gpt4o_mini   \n",
       "1  initial_prompt_gpt4o_mini   \n",
       "2  initial_prompt_gpt4o_mini   \n",
       "3  initial_prompt_gpt4o_mini   \n",
       "4  initial_prompt_gpt4o_mini   \n",
       "\n",
       "                                               input  \\\n",
       "0  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "1  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "2  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "3  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "4  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "\n",
       "                                              output  value  \\\n",
       "0  [{\"type\":\"text\",\"text\":\"\\\"Capacity\\\" evokes th...   True   \n",
       "1  [{\"type\":\"text\",\"text\":\"Preoccupation can ofte...   True   \n",
       "2  [{\"type\":\"text\",\"text\":\"Neurons spark and fire...   True   \n",
       "3  [{\"type\":\"text\",\"text\":\"A sunbonnet is a charm...   True   \n",
       "4  [{\"type\":\"text\",\"text\":\"Length can be perceive...   True   \n",
       "\n",
       "                             episode_id  \n",
       "0  0191d773-3de0-7652-827c-7f2114fc584b  \n",
       "1  0191d773-ae66-7833-8763-7d15cc6853ae  \n",
       "2  0191d773-4c08-7e20-87d2-27f7d5f6a5c2  \n",
       "3  0191d773-17f4-7231-8838-8c10b31a8a04  \n",
       "4  0191d773-041f-7383-891f-cd13727fec7b  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert \"optimize\" in metric, \"Metric is missing the `optimize` field\"\n",
    "\n",
    "threshold = FLOAT_METRIC_THRESHOLD if metric[\"type\"] == \"float\" else 0.5\n",
    "comparison_operator = \">=\" if metric[\"optimize\"] == \"max\" else \"<=\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    f.value,\n",
    "    i.episode_id\n",
    "FROM \n",
    "    Inference i\n",
    "JOIN \n",
    "    (SELECT\n",
    "        target_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY target_id ORDER BY timestamp DESC) as rn\n",
    "    FROM \n",
    "        {feedback_table_name}\n",
    "    WHERE\n",
    "        metric_name = %(metric_name)s\n",
    "        AND value {comparison_operator} %(threshold)s\n",
    "    ) f ON i.{inference_join_key} = f.target_id and f.rn = 1\n",
    "WHERE \n",
    "    i.function_name = %(function_name)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"metric_name\": METRIC_NAME,\n",
    "    \"comparison_operator\": comparison_operator,\n",
    "    \"threshold\": threshold,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the inputs using the templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant_name</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>value</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>openai_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"\\\"Capacity\\\" evokes th...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-3de0-7652-827c-7f2114fc584b</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"Preoccupation can ofte...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-ae66-7833-8763-7d15cc6853ae</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"Neurons spark and fire...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-4c08-7e20-87d2-27f7d5f6a5c2</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"A sunbonnet is a charm...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-17f4-7231-8838-8c10b31a8a04</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>initial_prompt_gpt4o_mini</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>[{\"type\":\"text\",\"text\":\"Length can be perceive...</td>\n",
       "      <td>True</td>\n",
       "      <td>0191d773-041f-7383-891f-cd13727fec7b</td>\n",
       "      <td>{'messages': [{'role': 'system', 'content': 'Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                variant_name  \\\n",
       "0  initial_prompt_gpt4o_mini   \n",
       "1  initial_prompt_gpt4o_mini   \n",
       "2  initial_prompt_gpt4o_mini   \n",
       "3  initial_prompt_gpt4o_mini   \n",
       "4  initial_prompt_gpt4o_mini   \n",
       "\n",
       "                                               input  \\\n",
       "0  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "1  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "2  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "3  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "4  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "\n",
       "                                              output  value  \\\n",
       "0  [{\"type\":\"text\",\"text\":\"\\\"Capacity\\\" evokes th...   True   \n",
       "1  [{\"type\":\"text\",\"text\":\"Preoccupation can ofte...   True   \n",
       "2  [{\"type\":\"text\",\"text\":\"Neurons spark and fire...   True   \n",
       "3  [{\"type\":\"text\",\"text\":\"A sunbonnet is a charm...   True   \n",
       "4  [{\"type\":\"text\",\"text\":\"Length can be perceive...   True   \n",
       "\n",
       "                             episode_id  \\\n",
       "0  0191d773-3de0-7652-827c-7f2114fc584b   \n",
       "1  0191d773-ae66-7833-8763-7d15cc6853ae   \n",
       "2  0191d773-4c08-7e20-87d2-27f7d5f6a5c2   \n",
       "3  0191d773-17f4-7231-8838-8c10b31a8a04   \n",
       "4  0191d773-041f-7383-891f-cd13727fec7b   \n",
       "\n",
       "                                     openai_messages  \n",
       "0  {'messages': [{'role': 'system', 'content': 'Y...  \n",
       "1  {'messages': [{'role': 'system', 'content': 'Y...  \n",
       "2  {'messages': [{'role': 'system', 'content': 'Y...  \n",
       "3  {'messages': [{'role': 'system', 'content': 'Y...  \n",
       "4  {'messages': [{'role': 'system', 'content': 'Y...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def render_message(content: List[Dict[str, Any]], role: str) -> str:\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "\n",
    "    if len(content) != 1:\n",
    "        raise ValueError(f\"Message must have exactly one content block: {content}\")\n",
    "\n",
    "    if content[0][\"type\"] != \"text\":\n",
    "        raise ValueError(f\"Content block must be of type text: {content}\")\n",
    "\n",
    "    content = content[0][\"value\"]\n",
    "\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return env.render_template(role, **content)\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample) -> List[Dict[str, str]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    rendered_messages = []\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        else:\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message[\"content\"], message[\"role\"])\n",
    "        rendered_messages.append({\"role\": message[\"role\"], \"content\": rendered_message})\n",
    "\n",
    "    # Add the output to the messages\n",
    "    output = json.loads(sample[\"output\"])\n",
    "\n",
    "    if function_type == \"chat\":\n",
    "        if len(output) != 1:\n",
    "            raise ValueError(f\"Output {output} must have exactly one content block.\")\n",
    "\n",
    "        if output[0][\"type\"] != \"text\":\n",
    "            raise ValueError(f\"Output {output} must be a text block.\")\n",
    "\n",
    "        rendered_messages.append({\"role\": \"assistant\", \"content\": output[0][\"text\"]})\n",
    "    elif function_type == \"json\":\n",
    "        rendered_messages.append({\"role\": \"assistant\", \"content\": output[\"raw\"]})\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "    return {\"messages\": rendered_messages}\n",
    "\n",
    "\n",
    "df[\"openai_messages\"] = df.apply(sample_to_openai_messages, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 40\n",
      "Validation set size: 11\n",
      "Actual validation fraction: 0.22\n"
     ]
    }
   ],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = df[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - VAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = df[df[\"episode_id\"].isin(train_episode_ids)]\n",
    "val_df = df[df[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Actual validation fraction: {len(val_df) / len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training and validation datasets to OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_openai(df: pd.DataFrame, openai_client: openai.OpenAI) -> str:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n",
    "        # Write the openai_messages to the temporary file\n",
    "        for item in df[\"openai_messages\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        # Upload the file to OpenAI\n",
    "        with open(f.name, \"rb\") as file:\n",
    "            file_object = openai_client.files.create(file=file, purpose=\"fine-tune\")\n",
    "\n",
    "        return file_object.id\n",
    "\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "train_file_object_id = upload_dataset_to_openai(train_df, openai_client)\n",
    "val_file_object_id = upload_dataset_to_openai(val_df, openai_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the fine-tuning job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
    "    training_file=train_file_object_id,\n",
    "    validation_file=val_file_object_id,\n",
    "    model=MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the fine-tuning job to complete.\n",
    "\n",
    "This cell will take a while to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': 1725896773,\n",
      " 'error': {},\n",
      " 'estimated_finish': 1725897095,\n",
      " 'fine_tuned_model': None,\n",
      " 'finished_at': None,\n",
      " 'hyperparameters': {'batch_size': 1,\n",
      "                     'learning_rate_multiplier': 1.8,\n",
      "                     'n_epochs': 3},\n",
      " 'id': 'ftjob-86aMde6jQcflp3Cse1rWaaNo',\n",
      " 'integrations': [],\n",
      " 'model': 'gpt-4o-mini-2024-07-18',\n",
      " 'object': 'fine_tuning.job',\n",
      " 'organization_id': 'org-fewHWgmYjDeYGco5co60C7fh',\n",
      " 'result_files': [],\n",
      " 'seed': 1628356953,\n",
      " 'status': 'running',\n",
      " 'trained_tokens': None,\n",
      " 'training_file': 'file-1WQxUMMsYXqAODcTD4BHJ7u6',\n",
      " 'user_provided_suffix': None,\n",
      " 'validation_file': 'file-kJZdrKjqTSZGWEywklVtDaaV'}\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "        pprint(job_status.to_dict())\n",
    "        if job_status.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, you can add the fine-tuned model to your config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = job_status.fine_tuned_model\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"openai\"],\n",
    "            \"providers\": {\"openai\": {\"type\": \"openai\", \"model_name\": fine_tuned_model}},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"weight\": 0,\n",
    "    \"model\": fine_tuned_model,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {fine_tuned_model: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model.\n",
    "\n",
    "You might also add other parameters (e.g. `max_tokens`, `temperature`) to the variant section in the config file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
