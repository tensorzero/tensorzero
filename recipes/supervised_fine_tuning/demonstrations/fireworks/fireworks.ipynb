{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fireworks Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune open-source LLMs using their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "We follow the Fireworks [docs](https://docs.fireworks.ai/fine-tuning/fine-tuning-models) on fine-tuning a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Set the `TENSORZERO_CLICKHOUSE_URL` environment variable. For example: `TENSORZERO_CLICKHOUSE_URL=\"http://localhost:8123/tensorzero\"`\n",
    "- You'll also need to [install](https://docs.fireworks.ai/tools-sdks/firectl/firectl) the CLI tool `firectl` on your machine and sign in with `firectl signin`. You can test that this all worked with `firectl whoami`.\n",
    "- Update the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CLICKHOUSE_URL = os.getenv(\"TENSORZERO_CLICKHOUSE_URL\")\n",
    "\n",
    "assert CLICKHOUSE_URL is not None, \"TENSORZERO_CLICKHOUSE_URL is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt4o_mini_initial_prompt\"  # It is OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# Number of epochs to train for\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning (for Fireworks, NUM_EPOCHS * MAX_SAMPLES should be <= 3,000,000)\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://docs.fireworks.ai/fine-tuning/fine-tuning-models#supported-base-models)\n",
    "MODEL_NAME = \"accounts/fireworks/models/llama-v3p1-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment\n",
    "from tensorzero.util import uuid7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_client = get_client(dsn=CLICKHOUSE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    function_type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences and demonstrations from ClickHouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    f.value,\n",
    "    i.episode_id\n",
    "FROM \n",
    "    {inference_table_name} i\n",
    "JOIN \n",
    "    (SELECT\n",
    "        inference_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY inference_id ORDER BY timestamp DESC) as rn\n",
    "    FROM \n",
    "        DemonstrationFeedback\n",
    "    ) f ON i.id = f.inference_id AND f.rn = 1\n",
    "WHERE \n",
    "    i.function_name = %(function_name)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.sample(5)[[\"output\", \"value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the inputs using the templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(content: List[Dict[str, Any]], role: str) -> str:\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "\n",
    "    if len(content) != 1:\n",
    "        raise ValueError(f\"Message must have exactly one content block: {content}\")\n",
    "\n",
    "    if content[0][\"type\"] != \"text\":\n",
    "        raise ValueError(f\"Content block must be of type text: {content}\")\n",
    "\n",
    "    content = content[0][\"value\"]\n",
    "\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return env.render_template(role, **content)\n",
    "\n",
    "\n",
    "def sample_to_conversational_messages(sample) -> List[Dict[str, str]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    rendered_messages = []\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        else:\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message[\"content\"], message[\"role\"])\n",
    "        rendered_messages.append({\"role\": message[\"role\"], \"content\": rendered_message})\n",
    "\n",
    "    # Add the demonstration to the messages NOT the output here\n",
    "    output = json.loads(sample[\"value\"])\n",
    "\n",
    "    if function_type == \"chat\":\n",
    "        if len(output) != 1:\n",
    "            raise ValueError(f\"Output {output} must have exactly one content block.\")\n",
    "\n",
    "        if output[0][\"type\"] != \"text\":\n",
    "            raise ValueError(f\"Output {output} must be a text block.\")\n",
    "\n",
    "        rendered_messages.append({\"role\": \"assistant\", \"content\": output[0][\"text\"]})\n",
    "    elif function_type == \"json\":\n",
    "        rendered_messages.append({\"role\": \"assistant\", \"content\": output[\"raw\"]})\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "    return {\"messages\": rendered_messages}\n",
    "\n",
    "\n",
    "df[\"conversational_messages\"] = df.apply(sample_to_conversational_messages, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    print(row[\"value\"])\n",
    "    print(row[\"conversational_messages\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write the conversational messages to a temporary file for the Fireworks CLI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = str(uuid7())\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".jsonl\") as f:\n",
    "    for _, row in df.iterrows():\n",
    "        f.write((json.dumps(row[\"conversational_messages\"]) + \"\\n\").encode(\"utf-8\"))\n",
    "\n",
    "    dataset_path = f.name\n",
    "    result = subprocess.run(\n",
    "        [\"firectl\", \"create\", \"dataset\", dataset_id, dataset_path], capture_output=True\n",
    "    )\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run([\"firectl\", \"get\", \"dataset\", dataset_id], capture_output=True)\n",
    "print(result.stdout.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_id(stdout: str) -> str:\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Name:\"):\n",
    "            return line.split(\"/\")[-1].strip()\n",
    "    raise ValueError(\"Job ID not found in output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the fine-tuning job. This cell will block until the job is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = [\n",
    "    \"firectl\",\n",
    "    \"create\",\n",
    "    \"fine-tuning-job\",\n",
    "    \"--display-name\",\n",
    "    f\"tensorzero-ft-job-{dataset_id}\",\n",
    "    \"--dataset\",\n",
    "    dataset_id,\n",
    "    \"--kind\",\n",
    "    \"conversation\",\n",
    "    \"--base-model\",\n",
    "    MODEL_NAME,\n",
    "]\n",
    "\n",
    "if NUM_EPOCHS is not None:\n",
    "    command.append(\"--epochs\")\n",
    "    command.append(str(NUM_EPOCHS))\n",
    "\n",
    "print(\"Command: \", \" \".join(command))\n",
    "\n",
    "result = subprocess.run(command, capture_output=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(result.stderr.decode(\"utf-8\"))\n",
    "else:\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    print(stdout)\n",
    "    job_id = get_job_id(stdout)\n",
    "    print(f\"job_id: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        command = [\"firectl\", \"get\", \"fine-tuning-job\", job_id]\n",
    "        result = subprocess.run(command, capture_output=True)\n",
    "        stdout = result.stdout.decode(\"utf-8\")\n",
    "        print(stdout)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    if \"State: FAILED\" in stdout:\n",
    "        raise ValueError(\"Fine-tuning job failed\")\n",
    "\n",
    "    if \"State: COMPLETED\" in stdout:\n",
    "        break\n",
    "\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id(stdout: str) -> str:\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Model Id:\"):\n",
    "            return line.split(\":\")[1].strip()\n",
    "    raise ValueError(\"Model ID not found in output\")\n",
    "\n",
    "\n",
    "model_id = get_model_id(stdout)\n",
    "\n",
    "assert model_id\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is done training, we need to [deploy](https://docs.fireworks.ai/fine-tuning/fine-tuning-models#deploying-and-using-a-model) it to Fireworks serverless inference. If you need high or guaranteed throughput you can also deploy the model to [reserved capacity](https://docs.fireworks.ai/deployments/reservations) or an on-demand [deployment](https://docs.fireworks.ai/guides/ondemand-deployments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = [\"firectl\", \"deploy\", model_id]\n",
    "print(\" \".join(command))\n",
    "result = subprocess.run(command, capture_output=True)\n",
    "if result.returncode != 0:\n",
    "    print(result.stderr.decode(\"utf-8\"))\n",
    "else:\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    print(stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_identifier(model_id: str) -> str:\n",
    "    command = [\"firectl\", \"get\", \"model\", model_id]\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Name:\"):\n",
    "            return line.split(\":\")[1].strip()\n",
    "    raise ValueError(\"Model identifier not found in output\")\n",
    "\n",
    "\n",
    "model_identifier = get_model_identifier(model_id)\n",
    "\n",
    "assert model_identifier\n",
    "\n",
    "model_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, you can add the fine-tuned model to your config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"models\": {\n",
    "        model_identifier: {\n",
    "            \"routing\": [\"fireworks\"],\n",
    "            \"providers\": {\n",
    "                \"fireworks\": {\"type\": \"fireworks\", \"model_name\": model_identifier}\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"weight\": 0,\n",
    "    \"model\": model_identifier,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {model_identifier: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model.\n",
    "\n",
    "You might also add other parameters (e.g. `max_tokens`, `temperature`) to the variant section in the config file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
