{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e329f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfb279",
   "metadata": {},
   "source": [
    "# Fireworks Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune open-source LLMs using their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "We follow the Fireworks [docs](https://docs.fireworks.ai/fine-tuning/fine-tuning-via-api) on fine-tuning a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1b06e",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "Set the `TENSORZERO_CLICKHOUSE_URL`, `FIREWORKS_API_KEY`, and `FIREWORKS_ACCOUNT_ID` environment variable. See the .env.example file.\n",
    "Update the following parameters:.\n",
    "- Update the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd242ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLICKHOUSE_URL = os.getenv(\"TENSORZERO_CLICKHOUSE_URL\")\n",
    "FIREWORKS_API_KEY = os.getenv(\"FIREWORKS_API_KEY\")\n",
    "account_id = os.getenv(\"FIREWORKS_ACCOUNT_ID\")\n",
    "\n",
    "assert CLICKHOUSE_URL is not None, \"TENSORZERO_CLICKHOUSE_URL is not set\"\n",
    "assert FIREWORKS_API_KEY is not None, \"FIREWORKS_API_KEY is not set\"\n",
    "assert account_id is not None, \"FIREWORKS_ACCOUNT_ID is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"  # It's OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# Number of epochs to train for\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning (for Fireworks, NUM_EPOCHS * MAX_SAMPLES should be <= 3,000,000)\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://docs.fireworks.ai/fine-tuning/fine-tuning-models#supported-base-models)\n",
    "MODEL_NAME = \"accounts/fireworks/models/llama-v3p1-8b-instruct\"\n",
    "\n",
    "# At the time of writing, Fireworks does not support tool call content blocks in assistant messages. Or the tool role.\n",
    "# We will drop these invalid messages from the dataset by default.\n",
    "# You can set this to False to keep the invalid messages in the dataset.\n",
    "DROP_INVALID_MESSAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e736b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import requests\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment\n",
    "from tensorzero.util import uuid7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f386f60",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa18237",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18cb55",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f041d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b456479",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf34dd",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_client = get_client(dsn=CLICKHOUSE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fde7b4",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    function_type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ca824",
   "metadata": {},
   "source": [
    "Query the inferences and demonstrations from ClickHouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6603d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "    i.variant_name,\n",
    "    i.input,\n",
    "    i.output,\n",
    "    f.value,\n",
    "    i.episode_id\n",
    "FROM\n",
    "    {inference_table_name} i\n",
    "JOIN\n",
    "    (SELECT\n",
    "        inference_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY inference_id ORDER BY timestamp DESC) as rn\n",
    "    FROM\n",
    "        DemonstrationFeedback\n",
    "    ) f ON i.id = f.inference_id AND f.rn = 1\n",
    "WHERE\n",
    "    i.function_name = %(function_name)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.sample(5)[[\"output\", \"value\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6018c",
   "metadata": {},
   "source": [
    "Render the inputs using the templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51030453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warning_message(role: str) -> str:\n",
    "    return (\n",
    "        f\"Fireworks does not support multiple content blocks per message. \"\n",
    "        f\"We have chosen to concatenate the text across all content blocks for the message with role '{role}'. \"\n",
    "        f\"You may want to manually review this behavior.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def render_message(message: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message[\"role\"]\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message[\"content\"]:\n",
    "        if content_block[\"type\"] not in [\"text\", \"raw_text\"] and DROP_INVALID_MESSAGES:\n",
    "            warnings.warn(\n",
    "                f\"Fireworks may not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "        if content_block[\"type\"] == \"text\":\n",
    "            parsed_content = content_block[\"value\"]\n",
    "            if not isinstance(parsed_content, str):\n",
    "                parsed_content = env.render_template(role, **parsed_content)\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block[\"type\"] == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block[\"value\"]})\n",
    "        elif content_block[\"type\"] == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "            )\n",
    "        elif (\n",
    "            content_block[\"type\"] == \"tool_call\"\n",
    "            and role == \"assistant\"\n",
    "            and not DROP_INVALID_MESSAGES\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Fireworks may not support tool calls in assistant messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif (\n",
    "            content_block[\"type\"] == \"tool_result\"\n",
    "            and role == \"user\"\n",
    "            and not DROP_INVALID_MESSAGES\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Fireworks may not support tool results in user messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            if len(content) > 1:\n",
    "                warnings.warn(warning_message(role), UserWarning)\n",
    "            role_message[\"content\"] = \"\\n\".join([c[\"text\"] for c in content])\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output: List[Dict[str, Any]],\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[str] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    if function_type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    elif function_type == \"chat\":\n",
    "        for content_block in output:\n",
    "            if content_block[\"type\"] != \"text\" and DROP_INVALID_MESSAGES:\n",
    "                warnings.warn(\n",
    "                    f\"Fireworks may not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "            if content_block[\"type\"] == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": content_block[\"text\"]})\n",
    "            elif content_block[\"type\"] == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "                )\n",
    "            elif content_block[\"type\"] == \"tool_call\" and not DROP_INVALID_MESSAGES:\n",
    "                warnings.warn(\n",
    "                    \"Fireworks may not support tool calls in assistant messages. Including it may cause the fine-tuning job to fail.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        if len(content) > 1:\n",
    "            warnings.warn(warning_message(\"assistant\"), UserWarning)\n",
    "        output_message[\"content\"] = \"\\n\".join([c[\"text\"] for c in content])\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_conversational_messages(sample) -> List[Dict[str, Any]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    rendered_messages = []\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        else:\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        rendered_messages.extend(rendered_message)\n",
    "\n",
    "    # Add the output to the messages\n",
    "    output = json.loads(sample[\"value\"])\n",
    "    rendered_output = render_output(output)\n",
    "    if rendered_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    rendered_messages.append(rendered_output)\n",
    "\n",
    "    return {\"messages\": rendered_messages}\n",
    "\n",
    "\n",
    "df[\"conversational_messages\"] = df.apply(sample_to_conversational_messages, axis=1)\n",
    "\n",
    "# Drop null rows\n",
    "df = df[df[\"conversational_messages\"].notna()]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    print(row[\"value\"])\n",
    "    print(row[\"conversational_messages\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87a461",
   "metadata": {},
   "source": [
    "We'll write the conversational messages to a temporary file for the Fireworks API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = f\"t0-{uuid7()}\"\n",
    "api_base = \"https://api.fireworks.ai/v1\"\n",
    "base_headers = {\"Authorization\": f\"Bearer {FIREWORKS_API_KEY}\"}\n",
    "json_headers = base_headers.copy()\n",
    "json_headers.update({\"Content-Type\": \"application/json\"})\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".jsonl\") as f:\n",
    "    for _, row in df.iterrows():\n",
    "        f.write((json.dumps(row[\"conversational_messages\"]) + \"\\n\").encode(\"utf-8\"))\n",
    "    create_record_url = f\"{api_base}/accounts/{account_id}/datasets/\"\n",
    "    # Create dataset\n",
    "    create_record_result = requests.post(\n",
    "        create_record_url,\n",
    "        json={\n",
    "            \"datasetId\": dataset_id,\n",
    "            \"dataset\": {\n",
    "                \"displayName\": dataset_id,\n",
    "                \"format\": \"CHAT\",\n",
    "                \"exampleCount\": len(df),\n",
    "            },\n",
    "        },\n",
    "        headers=json_headers,\n",
    "    )\n",
    "    print(create_record_result)\n",
    "    # Upload dataset\n",
    "    upload_file_url = f\"{api_base}/accounts/{account_id}/datasets/{dataset_id}:upload\"\n",
    "    try:\n",
    "        with open(f.name, \"r\") as file:\n",
    "            dataset = {\"file\": file}\n",
    "            upload_file_result = requests.post(\n",
    "                upload_file_url, headers=base_headers, files=dataset\n",
    "            )\n",
    "            upload_file_result.raise_for_status()\n",
    "            print(json.dumps(upload_file_result.json(), indent=2))\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found: {f.name}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"HTTP request failed: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_state_url = f\"{api_base}/accounts/{account_id}/datasets/{dataset_id}\"\n",
    "result = requests.get(check_state_url, headers=base_headers)\n",
    "print(json.dumps(result.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0387d",
   "metadata": {},
   "source": [
    "Now we start the fine-tuning job. This cell will block until the job is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cf3ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sft_url = f\"{api_base}/accounts/{account_id}/supervisedFineTuningJobs\"\n",
    "json_to_send = {\n",
    "    \"dataset\": f\"accounts/{account_id}/datasets/{dataset_id}\",\n",
    "    \"base_model\": MODEL_NAME,\n",
    "}\n",
    "if NUM_EPOCHS is not None:\n",
    "    json_to_send[\"epochs\"] = NUM_EPOCHS\n",
    "result = requests.post(url=create_sft_url, headers=json_headers, json=json_to_send)\n",
    "if result.status_code != 200:\n",
    "    print(json.dumps(result.json(), indent=2))\n",
    "else:\n",
    "    response = result.json()\n",
    "    print(json.dumps(response, indent=2))\n",
    "    job_id = response[\"name\"]\n",
    "    print(f\"job_id: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job_status_url = f\"{api_base}/{job_id}\"\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        result = requests.get(\n",
    "            url=get_job_status_url,\n",
    "            headers=base_headers,\n",
    "        )\n",
    "        response = result.json()\n",
    "        print(json.dumps(result.json(), indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    if response[\"state\"] == \"JOB_STATE_FAILED\":\n",
    "        raise ValueError(\"Fine-tuning job failed\")\n",
    "\n",
    "    if response[\"state\"] == \"JOB_STATE_COMPLETED\":\n",
    "        break\n",
    "\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc47847",
   "metadata": {},
   "source": [
    "Now that the model is done training, we need to [deploy](https://docs.fireworks.ai/fine-tuning/fine-tuning-models#deploying-and-using-a-model) it to Fireworks serverless inference. If you need high or guaranteed throughput you can also deploy the model to [reserved capacity](https://docs.fireworks.ai/deployments/reservations) or an on-demand [deployment](https://docs.fireworks.ai/guides/ondemand-deployments).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc33b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = response[\"outputModel\"]\n",
    "deploy_model_url = f\"{api_base}/accounts/{account_id}/deployedModels\"\n",
    "result = requests.post(\n",
    "    url=deploy_model_url,\n",
    "    headers=json_headers,\n",
    "    json={\n",
    "        \"model\": model_id,\n",
    "        \"default\": True,\n",
    "        \"serverless\": True,\n",
    "        \"public\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identifier = model_id\n",
    "\n",
    "assert model_identifier\n",
    "\n",
    "model_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b0caf",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, you can add the fine-tuned model to your config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d37954",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"models\": {\n",
    "        model_identifier: {\n",
    "            \"routing\": [\"fireworks\"],\n",
    "            \"providers\": {\n",
    "                \"fireworks\": {\"type\": \"fireworks\", \"model_name\": model_identifier}\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be76ed",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"weight\": 0,\n",
    "    \"model\": model_identifier,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {model_identifier: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a1cd4",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model.\n",
    "\n",
    "You might also add other parameters (e.g. `max_tokens`, `temperature`) to the variant section in the config file.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
