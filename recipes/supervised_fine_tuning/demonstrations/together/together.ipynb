{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd17435",
   "metadata": {},
   "source": [
    "# Together Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune Together models using their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a521f3",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Set the `TENSORZERO_CLICKHOUSE_URL` environment variable. For example: `TENSORZERO_CLICKHOUSE_URL=\"http://chuser:chpassword@localhost:8123/tensorzero\"`\n",
    "- Set the `TOGETHER_API_KEY` environment variable.\n",
    "- Update the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069384c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"  # It's OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://docs.together.ai/docs/lora-inference#supported-base-models)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "\n",
    "# At the time of writing, Together.ai does not support tool call content blocks in assistant messages. Or the tool role.\n",
    "# We will drop these invalid messages from the dataset by default.\n",
    "# You can set this to False to keep the invalid messages in the dataset.\n",
    "DROP_INVALID_MESSAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac070ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f700b",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0301e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc8c7cf",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a869c9",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8200cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5d97d",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663cc15",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e492712",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    function_type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a83266",
   "metadata": {},
   "source": [
    "Query the inferences and feedback from ClickHouse.\n",
    "\n",
    "If the metric is a float metric, we need to filter the data based on the threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79999d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "    i.variant_name,\n",
    "    i.input,\n",
    "    i.output,\n",
    "    f.value,\n",
    "    i.episode_id\n",
    "FROM\n",
    "    {inference_table_name} i\n",
    "JOIN\n",
    "    (SELECT\n",
    "        inference_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY inference_id ORDER BY timestamp DESC) as rn\n",
    "    FROM\n",
    "        DemonstrationFeedback\n",
    "    ) f ON i.id = f.inference_id AND f.rn = 1\n",
    "WHERE\n",
    "    i.function_name = %(function_name)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239104fb",
   "metadata": {},
   "source": [
    "Render the inputs using the templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599dc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warning_message(role: str) -> str:\n",
    "    return (\n",
    "        f\"Together.ai does not support multiple content blocks per message. \"\n",
    "        f\"We have chosen to concatenate the text across all content blocks for the message with role '{role}'. \"\n",
    "        f\"You may want to manually review this behavior.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def render_message(message: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message[\"role\"]\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message[\"content\"]:\n",
    "        if content_block[\"type\"] not in [\"text\", \"raw_text\"] and DROP_INVALID_MESSAGES:\n",
    "            warnings.warn(\n",
    "                f\"Together.ai may not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "        if content_block[\"type\"] == \"text\":\n",
    "            parsed_content = content_block[\"value\"]\n",
    "            if not isinstance(parsed_content, str):\n",
    "                parsed_content = env.render_template(role, **parsed_content)\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block[\"type\"] == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block[\"value\"]})\n",
    "        elif content_block[\"type\"] == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "            )\n",
    "        elif (\n",
    "            content_block[\"type\"] == \"tool_call\"\n",
    "            and role == \"assistant\"\n",
    "            and not DROP_INVALID_MESSAGES\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Together.ai may not support tool calls in assistant messages.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif (\n",
    "            content_block[\"type\"] == \"tool_result\"\n",
    "            and role == \"user\"\n",
    "            and not DROP_INVALID_MESSAGES\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"Together.ai may not support tool results in user messages.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            if len(content) > 1:\n",
    "                warnings.warn(warning_message(role), UserWarning)\n",
    "            role_message[\"content\"] = \"\\n\".join([c[\"text\"] for c in content])\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output: List[Dict[str, Any]],\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    if function_type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    elif function_type == \"chat\":\n",
    "        for content_block in output:\n",
    "            if content_block[\"type\"] != \"text\" and DROP_INVALID_MESSAGES:\n",
    "                warnings.warn(\n",
    "                    f\"Together.ai may not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "            if content_block[\"type\"] == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": content_block[\"text\"]})\n",
    "            elif content_block[\"type\"] == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "                )\n",
    "            elif content_block[\"type\"] == \"tool_call\" and not DROP_INVALID_MESSAGES:\n",
    "                warnings.warn(\n",
    "                    \"Together.ai may not support tool calls in assistant messages.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        if len(content) > 1:\n",
    "            warnings.warn(warning_message(\"assistant\"), UserWarning)\n",
    "        output_message[\"content\"] = \"\\n\".join([c[\"text\"] for c in content])\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_conversational_messages(sample) -> List[Dict[str, Any]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    rendered_messages = []\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        else:\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        rendered_messages.extend(rendered_message)\n",
    "\n",
    "    # Add the output to the messages\n",
    "    output = json.loads(sample[\"value\"])\n",
    "    rendered_output = render_output(output)\n",
    "    if rendered_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    rendered_messages.append(rendered_output)\n",
    "\n",
    "    return {\"messages\": rendered_messages}\n",
    "\n",
    "\n",
    "df[\"conversational_messages\"] = df.apply(sample_to_conversational_messages, axis=1)\n",
    "\n",
    "# Drop null rows\n",
    "df = df[df[\"conversational_messages\"].notna()]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a7dd7",
   "metadata": {},
   "source": [
    "Check the messages format and size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26d2bf",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60deea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = df[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - VAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = df[df[\"episode_id\"].isin(train_episode_ids)]\n",
    "val_df = df[df[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Actual validation fraction: {len(val_df) / len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995225f",
   "metadata": {},
   "source": [
    "Upload the training and validation datasets to Together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_together(df: pd.DataFrame) -> str:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n",
    "        # Write the conversational_messages to the temporary file\n",
    "        for item in df[\"conversational_messages\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        dataset_path = f.name\n",
    "        result = subprocess.run(\n",
    "            [\"together\", \"files\", \"upload\", dataset_path], capture_output=True\n",
    "        )\n",
    "        print(\"Stdout:\")\n",
    "        print(result.stdout.decode())\n",
    "        print(\"Stderr:\")\n",
    "        print(result.stderr.decode())\n",
    "        together_result = json.loads(result.stdout)\n",
    "        return together_result[\"id\"]\n",
    "\n",
    "\n",
    "train_file_object_id = upload_dataset_to_together(train_df)\n",
    "val_file_object_id = upload_dataset_to_together(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7065b56",
   "metadata": {},
   "source": [
    "Launch the fine-tuning job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aba140",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.together.xyz/v1/fine-tunes\"\n",
    "print(\"MODEL: \", MODEL_NAME)\n",
    "print(\"Train: \", train_file_object_id)\n",
    "print(\"Val: \", val_file_object_id)\n",
    "\n",
    "payload = {\n",
    "    \"training_file\": train_file_object_id,\n",
    "    \"validation_file\": val_file_object_id,\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"n_epochs\": 1,\n",
    "    \"n_checkpoints\": 1,\n",
    "    \"n_evals\": 0,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.00001,\n",
    "    \"lr_scheduler\": {\"lr_scheduler_args\": {\"min_lr_ratio\": 0}},\n",
    "    \"warmup_ratio\": 0,\n",
    "    \"max_grad_norm\": 1,\n",
    "    \"weight_decay\": 0,\n",
    "    \"train_on_inputs\": \"auto\",\n",
    "    \"training_type\": {\"type\": \"Lora\", \"lora_r\": 8, \"lora_alpha\": 32},\n",
    "}\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": f\"Bearer {os.environ['TOGETHER_API_KEY']}\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "print(\"Response status: \", response.status_code)\n",
    "print(\"Response body: \")\n",
    "print(response.text)\n",
    "response_json = json.loads(response.text)\n",
    "fine_tune_id = response_json[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eab84d",
   "metadata": {},
   "source": [
    "Wait for the fine-tuning job to complete.\n",
    "\n",
    "This cell will take a while to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba70361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        job_status = requests.get(\n",
    "            f\"https://api.together.xyz/v1/fine-tunes/{fine_tune_id}\",\n",
    "            headers={\n",
    "                \"accept\": \"application/json\",\n",
    "                \"authorization\": f\"Bearer {os.environ['TOGETHER_API_KEY']}\",\n",
    "            },\n",
    "        ).json()\n",
    "        pprint(job_status)\n",
    "        print(\"Status: \", job_status[\"status\"])\n",
    "        if job_status[\"status\"] in (\"completed\", \"failed\", \"cancelled\"):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ef8d0",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, you can add the fine-tuned model to your config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = job_status[\"model_output_name\"]\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"together\"],\n",
    "            \"providers\": {\n",
    "                \"together\": {\"type\": \"together\", \"model_name\": fine_tuned_model}\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0998126",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c847765",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"model\": fine_tuned_model,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {fine_tuned_model: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b223351",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model.\n",
    "\n",
    "You might also add other parameters (e.g. `temperature`) to the variant section in the config file.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
