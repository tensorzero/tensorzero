{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune OpenAI models using their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "\n",
    "To use this recipe, you need to set the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Note: you should also set the CLICKHOUSE_URL environment variable to your ClickHouse URL\n",
    "METRIC_NAME = \"haiku_score\"\n",
    "FUNCTION_NAME = \"write_haiku\"\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"initial_prompt_gpt4o_mini\"\n",
    "CONFIG_PATH = Path(\"../../examples/haiku_hidden_preferences/config/tensorzero.toml\")\n",
    "# Only relevant if the metric is a float metric\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "# Max number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100_000\n",
    "# The name of the model to fine-tune (supported models [here](https://platform.openai.com/docs/guides/fine-tuning))\n",
    "MODEL_NAME = \"gpt-4o-mini-2024-07-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "import toml\n",
    "from clickhouse_driver import Client\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CONFIG_PATH.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics are always float- or boolean-valued (we handle \"comment\" and \"demonstration\" specifically)\n",
    "metric = config[\"metrics\"][METRIC_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "config_dir = CONFIG_PATH.parent\n",
    "system_template_path = (\n",
    "    config_dir / variant[\"system_template\"] if \"system_template\" in variant else None\n",
    ")\n",
    "user_template_path = (\n",
    "    config_dir / variant[\"user_template\"] if \"user_template\" in variant else None\n",
    ")\n",
    "assistant_template_path = (\n",
    "    config_dir / variant[\"assistant_template\"]\n",
    "    if \"assistant_template\" in variant\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "if system_template_path:\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "if user_template_path:\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "if assistant_template_path:\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_client = Client.from_url(os.environ[\"CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table_name = {\n",
    "    \"float\": \"FloatMetricFeedback\",\n",
    "    \"boolean\": \"BooleanMetricFeedback\",\n",
    "}.get(metric[\"type\"])\n",
    "\n",
    "if metric_table_name is None:\n",
    "    raise ValueError(f\"Unsupported metric type: {metric['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference id\n",
    "# We do a bit of conditional logic to handle float and boolean metrics\n",
    "threshold = FLOAT_METRIC_THRESHOLD if metric[\"type\"] == \"float\" else 0.5\n",
    "optimize_direction = metric[\"optimize\"]\n",
    "filtered_df = clickhouse_client.query_dataframe(\n",
    "    \"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    f.value\n",
    "FROM \n",
    "    Inference i\n",
    "JOIN \n",
    "    %(metric_table_name)s f ON i.id = f.target_id\n",
    "WHERE \n",
    "    i.function_name = %(function_name)s\n",
    "    AND (\n",
    "        (%(optimize_direction)s = 'max' AND f.value > %(threshold)s)\n",
    "        OR (%(optimize_direction)s = 'min' AND f.value < %(threshold)s)\n",
    "    )\n",
    "LIMIT %(max_samples)s\"\"\",\n",
    "    {\n",
    "        \"metric_table_name\": metric_table_name,\n",
    "        \"function_name\": FUNCTION_NAME,\n",
    "        \"optimize_direction\": optimize_direction,\n",
    "        \"threshold\": threshold,\n",
    "        \"max_samples\": MAX_SAMPLES,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(content: List[Dict[str, Any]], role: str) -> str:\n",
    "    assert role in [\"user\", \"assistant\"]\n",
    "    if len(content) != 1:\n",
    "        raise ValueError(f\"Message must have exactly one content block: {content}\")\n",
    "    if content[0][\"type\"] != \"text\":\n",
    "        raise ValueError(f\"Content block must be of type text: {content}\")\n",
    "    content = content[0][\"value\"]\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return env.render_template(role, **content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_openai_messages(example) -> List[Dict[str, str]]:\n",
    "    function_input = json.loads(example[\"input\"])\n",
    "    system = function_input.get(\"system\", {})\n",
    "    rendered_messages = []\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        else:\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message[\"content\"], message[\"role\"])\n",
    "        rendered_messages.append({\"role\": message[\"role\"], \"content\": rendered_message})\n",
    "    # Add the output to the messages\n",
    "    output = json.loads(example[\"output\"])\n",
    "    if len(output) != 1:\n",
    "        raise ValueError(f\"Output {output} has a not-one number of content blocks.\")\n",
    "    if output[0][\"type\"] != \"text\":\n",
    "        raise ValueError(f\"Output {output} has a not-text content block.\")\n",
    "    rendered_messages.append({\"role\": \"assistant\", \"content\": output[0][\"text\"]})\n",
    "    return dict(messages=rendered_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"openai_messages\"] = filtered_df.apply(example_to_openai_messages, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets for fine-tuning\n",
    "# Shuffle the DataFrame\n",
    "shuffled_df = filtered_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(len(shuffled_df) * (1 - VAL_FRACTION))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df = shuffled_df.iloc[:split_index]\n",
    "val_df = shuffled_df.iloc[split_index:]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_openai_training_data(df: pd.DataFrame, openai_client: openai.OpenAI) -> str:\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        mode=\"w\", suffix=\".jsonl\", delete=False\n",
    "    ) as temp_file:\n",
    "        # Write the openai_messages to the temporary file\n",
    "        for item in df[\"openai_messages\"]:\n",
    "            json.dump(item, temp_file)\n",
    "            temp_file.write(\"\\n\")\n",
    "        temp_file.flush()\n",
    "        temp_file_path = temp_file.name\n",
    "\n",
    "        # Upload the file to OpenAI\n",
    "        with open(temp_file_path, \"rb\") as file:\n",
    "            file_object = openai_client.files.create(file=file, purpose=\"fine-tune\")\n",
    "\n",
    "        return file_object.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = openai.OpenAI()\n",
    "train_file_object_id = upload_openai_training_data(train_df, openai_client)\n",
    "val_file_object_id = upload_openai_training_data(val_df, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_job = openai_client.fine_tuning.jobs.create(\n",
    "    training_file=train_file_object_id,\n",
    "    validation_file=val_file_object_id,\n",
    "    model=MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep an eye on the job status (this will take a while)\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    job_status = openai_client.fine_tuning.jobs.retrieve(ft_job.id)\n",
    "    print(f\"job_status: {job_status}\")\n",
    "    if job_status.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the following block to your config file to include the fine-tuned model in your gateway:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = job_status.fine_tuned_model\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"openai\"],\n",
    "            \"providers\": {\"openai\": {\"type\": \"openai\", \"model_name\": fine_tuned_model}},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "model_instructions = f\"```toml\\n{toml.dumps(model_config)}\\n```\"\n",
    "\n",
    "print(model_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the following block to your config file to include the fine-tuned model in your function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_name = f\"haiku_{fine_tuned_model}\"\n",
    "variant_config = {\n",
    "    \"functions\": {\n",
    "        FUNCTION_NAME: {\n",
    "            \"variants\": {\n",
    "                variant_name: {\n",
    "                    \"type\": \"chat_completion\",\n",
    "                    \"weight\": 0,\n",
    "                    \"model\": fine_tuned_model,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"functions\"][FUNCTION_NAME][\"variants\"][variant_name][\n",
    "        \"system_template\"\n",
    "    ] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"functions\"][FUNCTION_NAME][\"variants\"][variant_name][\n",
    "        \"user_template\"\n",
    "    ] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"functions\"][FUNCTION_NAME][\"variants\"][variant_name][\n",
    "        \"assistant_template\"\n",
    "    ] = assistant_template\n",
    "\n",
    "variant_instructions = f\"```toml\\n{toml.dumps(variant_config)}\\n```\"\n",
    "print(variant_instructions)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the weight to enable a gradual rollout of the new model.\n",
    "You might also add other parameters (max_tokens, temperature, etc.) to the variant TOML block.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
