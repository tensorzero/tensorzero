{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Vertex Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune Gemini models using their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Set the `TENSORZERO_CLICKHOUSE_URL` environment variable. For example: `TENSORZERO_CLICKHOUSE_URL=\"http://chuser:chpassword@localhost:8123/tensorzero\"`\n",
    "- Create local authentication credentials `gcloud auth application-default login`\n",
    "- You may need to [Create a Bucket](https://cloud.google.com/storage/docs/creating-buckets) on GCP, if you do not already have one.\n",
    "- Update the following parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "METRIC_NAME = \"exact_match\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning)\n",
    "MODEL_NAME = \"gemini-2.0-flash-lite-001\"\n",
    "\n",
    "# Google Cloud Variables\n",
    "PROJECT_ID = \"alpine-realm-415615\"\n",
    "LOCATION = \"us-central1\"\n",
    "BUCKET_NAME = \"tensorzero-fine-tuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "import vertexai\n",
    "from google.cloud import storage\n",
    "from google.cloud.aiplatform_v1.types import JobState\n",
    "from IPython.display import clear_output\n",
    "from tensorzero import (\n",
    "    BooleanMetricNode,\n",
    "    FloatMetricNode,\n",
    "    RawText,\n",
    "    TensorZeroGateway,\n",
    "    Text,\n",
    "    Thought,\n",
    "    ToolCall,\n",
    "    ToolResult,\n",
    ")\n",
    "from tensorzero.util import uuid7\n",
    "from vertexai.tuning import sft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the TensorZero client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = TensorZeroGateway.build_embedded(\n",
    "    config_file=CONFIG_PATH,\n",
    "    clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    "    timeout=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valudate config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the metric configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"metrics\" in config, \"No `[metrics]` section found in config\"\n",
    "assert METRIC_NAME in config[\"metrics\"], (\n",
    "    f\"No metric named `{METRIC_NAME}` found in config\"\n",
    ")\n",
    "\n",
    "metric = config[\"metrics\"][METRIC_NAME]\n",
    "\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the metric filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"optimize\" in metric, \"Metric is missing the `optimize` field\"\n",
    "\n",
    "if metric.get(\"type\") == \"float\":\n",
    "    comparison_operator = \">=\" if metric[\"optimize\"] == \"max\" else \"<=\"\n",
    "    metric_node = FloatMetricNode(\n",
    "        metric_name=METRIC_NAME,\n",
    "        value=FLOAT_METRIC_THRESHOLD,\n",
    "        comparison_operator=comparison_operator,\n",
    "    )\n",
    "elif metric.get(\"type\") == \"boolean\":\n",
    "    metric_node = BooleanMetricNode(\n",
    "        metric_name=METRIC_NAME,\n",
    "        value=True if metric[\"optimize\"] == \"max\" else False,\n",
    "    )\n",
    "\n",
    "metric_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences and feedback from ClickHouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_inferences = tensorzero_client.experimental_list_inferences(\n",
    "    function_name=FUNCTION_NAME,\n",
    "    variant_name=None,\n",
    "    filters=metric_node,\n",
    "    limit=MAX_SAMPLES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the stored inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_inferences = tensorzero_client.experimental_render_inferences(\n",
    "    stored_inferences=stored_inferences,\n",
    "    variants={FUNCTION_NAME: TEMPLATE_VARIANT_NAME},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert inferences to vertex format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_map = {\n",
    "    \"user\": \"user\",\n",
    "    \"assistant\": \"model\",\n",
    "    \"system\": \"system\",\n",
    "}\n",
    "\n",
    "\n",
    "def merge_messages(messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Merge consecutive messages with the same role into a single message.\n",
    "    \"\"\"\n",
    "    merged: List[Dict[str, Any]] = []\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        parts = msg.get(\"parts\", [])\n",
    "        if merged and merged[-1][\"role\"] == role:\n",
    "            merged[-1][\"parts\"].extend(parts)\n",
    "        else:\n",
    "            merged.append({\"role\": role, \"parts\": list(parts)})\n",
    "    return merged\n",
    "\n",
    "\n",
    "def render_chat_message(\n",
    "    role: str,\n",
    "    content_blocks: List[\n",
    "        Any\n",
    "    ],  # instances of Text, RawText, Thought, ToolCall, ToolResult\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Render a single chat message into Google “parts” format.\n",
    "    \"\"\"\n",
    "    parts: List[Dict[str, Any]] = []\n",
    "    for blk in content_blocks:\n",
    "        # plain text\n",
    "        if isinstance(blk, Text):\n",
    "            parts.append({\"text\": blk.text})\n",
    "        elif isinstance(blk, RawText):  # Verify if needed\n",
    "            parts.append({\"text\": blk.value})\n",
    "        # internal “thoughts”\n",
    "        elif isinstance(blk, Thought):\n",
    "            parts.append({\"text\": f\"<think>{blk.text}</think>\"})\n",
    "        # function call (assistant only)\n",
    "        elif isinstance(blk, ToolCall) and role == \"assistant\":\n",
    "            args = blk.raw_arguments\n",
    "            # raw_arguments might already be a dict or JSON string\n",
    "            if isinstance(args, str):\n",
    "                args = json.loads(args)\n",
    "            parts.append(\n",
    "                {\n",
    "                    \"functionCall\": {\n",
    "                        \"name\": blk.name,\n",
    "                        \"args\": args,\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        # function result (user only)\n",
    "        elif isinstance(blk, ToolResult) and role == \"user\":\n",
    "            parts.append(\n",
    "                {\n",
    "                    \"functionResponse\": {\n",
    "                        \"name\": blk.name,\n",
    "                        \"response\": {\"result\": blk.result},\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Unsupported block type {type(blk)} in role={role}, skipping inference.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "    return {\"role\": role_map[role], \"parts\": parts}\n",
    "\n",
    "\n",
    "def inference_to_google(\n",
    "    inf,\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Convert a single rendered_inference into the Google Vertex format dict.\n",
    "    \"\"\"\n",
    "    model_input = inf.input\n",
    "    rendered_msgs: List[Dict[str, Any]] = []\n",
    "\n",
    "    # 1) systemInstruction\n",
    "    if model_input.system:\n",
    "        system_instruction = {\n",
    "            \"role\": role_map[\"system\"],\n",
    "            \"parts\": [{\"text\": model_input.system}],\n",
    "        }\n",
    "    else:\n",
    "        system_instruction = None\n",
    "\n",
    "    # 2) all user/assistant messages\n",
    "    for msg in model_input.messages:\n",
    "        rendered = render_chat_message(msg.role, msg.content)\n",
    "        if rendered is None:\n",
    "            return None\n",
    "        rendered_msgs.append(rendered)\n",
    "\n",
    "    # 3) the assistant’s output\n",
    "    #    (same logic as render_chat_message but without ToolResult)\n",
    "    out_parts: List[Dict[str, Any]] = []\n",
    "    for blk in inf.output:\n",
    "        if isinstance(blk, Text):\n",
    "            out_parts.append({\"text\": blk.text})\n",
    "        elif isinstance(blk, Thought):\n",
    "            out_parts.append({\"text\": f\"<think>{blk.text}</think>\"})\n",
    "        elif isinstance(blk, ToolCall):\n",
    "            args = blk.raw_arguments\n",
    "            if isinstance(args, str):\n",
    "                args = json.loads(args)\n",
    "            out_parts.append(\n",
    "                {\n",
    "                    \"functionCall\": {\n",
    "                        \"name\": blk.name,\n",
    "                        \"args\": args,\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Unsupported output block {type(blk)}, skipping inference.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "    rendered_msgs.append({\"role\": role_map[\"assistant\"], \"parts\": out_parts})\n",
    "\n",
    "    # 4) merge any consecutive roles and return\n",
    "    contents = merge_messages(rendered_msgs)\n",
    "    result = {\"google_messages\": {\"contents\": contents}}\n",
    "    if system_instruction:\n",
    "        result[\"google_messages\"][\"systemInstruction\"] = system_instruction\n",
    "    # optionally keep track of episode\n",
    "    if hasattr(inf, \"episode_id\"):\n",
    "        result[\"episode_id\"] = inf.episode_id\n",
    "    return result\n",
    "\n",
    "\n",
    "google_payloads = []\n",
    "for inf in rendered_inferences:\n",
    "    payload = inference_to_google(inf)\n",
    "    if payload is not None:\n",
    "        google_payloads.append(payload)\n",
    "\n",
    "df = pd.DataFrame(google_payloads)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = df[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - VAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = df[df[\"episode_id\"].isin(train_episode_ids)]\n",
    "val_df = df[df[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Actual validation fraction: {len(val_df) / len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training and validation datasets to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_gcp(\n",
    "    df: pd.DataFrame, dataset_name: str, gcp_client: storage.Client\n",
    ") -> str:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n",
    "        # Write the openai_messages to the temporary file\n",
    "        for item in df[\"google_messages\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        bucket = gcp_client.bucket(BUCKET_NAME)\n",
    "        if not bucket.exists():\n",
    "            bucket.storage_class = \"STANDARD\"\n",
    "            bucket = gcp_client.create_bucket(bucket, location=\"us\")\n",
    "            print(\n",
    "                \"Created bucket {} in {} with storage class {}\".format(\n",
    "                    bucket.name, bucket.location, bucket.storage_class\n",
    "                )\n",
    "            )\n",
    "        blob = bucket.blob(dataset_name)\n",
    "\n",
    "        generation_match_precondition = 0\n",
    "        blob.upload_from_filename(\n",
    "            f.name, if_generation_match=generation_match_precondition\n",
    "        )\n",
    "\n",
    "\n",
    "gcp_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "train_file_name = f\"train_{uuid7()}.jsonl\"\n",
    "val_file_name = f\"val_{uuid7()}.jsonl\"\n",
    "\n",
    "\n",
    "upload_dataset_to_gcp(train_df, train_file_name, gcp_client)\n",
    "upload_dataset_to_gcp(val_df, val_file_name, gcp_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the fine-tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_tuning_job = sft.train(\n",
    "    source_model=MODEL_NAME,\n",
    "    train_dataset=f\"gs://{BUCKET_NAME}/{train_file_name}\",\n",
    "    validation_dataset=f\"gs://{BUCKET_NAME}/{val_file_name}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for the fine-tuning job to complete.\n",
    "\n",
    "This cell will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sft.SupervisedTuningJob(sft_tuning_job.resource_name)\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        job_state = response.state\n",
    "        print(job_state)\n",
    "        if job_state in (\n",
    "            JobState.JOB_STATE_SUCCEEDED.value,\n",
    "            JobState.JOB_STATE_FAILED.value,\n",
    "            JobState.JOB_STATE_CANCELLED.value,\n",
    "        ):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    response.refresh()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, you can add the fine-tuned model to your config file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = response.tuned_model_endpoint_name.split(\"/\")[-1]\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"gcp_vertex_gemini\"],\n",
    "            \"providers\": {\n",
    "                \"gcp_vertex_gemini\": {\n",
    "                    \"type\": \"gcp_vertex_gemini\",\n",
    "                    \"endpoint_id\": fine_tuned_model,\n",
    "                    \"location\": LOCATION,\n",
    "                    \"project_id\": PROJECT_ID,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"model\": fine_tuned_model,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {fine_tuned_model: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
