{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchtune Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune models using [torchtune](https://docs.pytorch.org/torchtune/main/) and their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "\n",
    "We demonstrate how to deploy a LoRA fine-tuned model for serverless inference using [Fireworks](https://fireworks.ai). Full instructions to deploy LoRA or full fine-tuned models are provided by [Fireworks](https://docs.fireworks.ai/fine-tuning/fine-tuning-models), [Together](https://docs.together.ai/docs/deploying-a-fine-tuned-model), and other inference providers. You can also use [vLLM](https://docs.vllm.ai/en/latest/examples/online_serving/api_client.html) to serve your fine-tuned model locally. The TensorZero client seemlessly integrates inference using your fine-tuned model for any of these approaches.\n",
    "\n",
    "To get started:\n",
    "\n",
    "- Set your `TENSORZERO_CLICKHOUSE_URL` enironment variable to point to the database containing the historical inferences you'd like to train on.\n",
    "- Set your `HF_TOKEN` to use Llama or Gemma models downloaded through huggingface.\n",
    "- Set the environment variable `CHECKPOINT_HOME` to a path with sufficient storage to save the base LLM checkpoints.\n",
    "- You'll also need to [install](https://docs.fireworks.ai/tools-sdks/firectl/firectl) the CLI tool `firectl` on your machine and sign in with `firectl signin`. You can test that this all worked with `firectl whoami`.\n",
    "- Update the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "METRIC_NAME = \"exact_match\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"  # It's OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# Random seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a model to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name of the model to fine-tune (supported models: https://docs.pytorch.org/torchtune/main/api_ref_models.html)\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Whether to use LoRA or not. Set to False for full model fine-tuning\n",
    "# If set to False, SEVERLESS must also be False as you will need to create your own deployment\n",
    "USE_LORA = True\n",
    "\n",
    "# Whether to use a serverless deployment.\n",
    "# Set to False is full model fine tuning or using LoRA for a model without serverless support\n",
    "SERVERLESS = True\n",
    "\n",
    "# Set to true if you want to include system and user messages in loss calculation\n",
    "TRAIN_ON_INPUT = False\n",
    "\n",
    "# Number of server nodes to use\n",
    "NNODES = 1\n",
    "\n",
    "# Number of devices (e.g., GPUs) to use per node\n",
    "NPROC_PER_NODE = 1\n",
    "\n",
    "# Set the directory where you would like to save the fine-tuned model\n",
    "OUTPUT_DIR = \"fine-tuned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import MODELS, list_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"CHECKPOINT_HOME\" in os.environ, \"CHECKPOINT_HOME environment variable not set\"\n",
    "assert \"HF_TOKEN\" in os.environ, \"HF_TOKEN environment variable not set\"\n",
    "\n",
    "checkpoint_home = Path(os.environ[\"CHECKPOINT_HOME\"])\n",
    "checkpoint_dir = checkpoint_home / MODEL_NAME\n",
    "\n",
    "command = [\n",
    "    \"tune\",\n",
    "    \"download\",\n",
    "    MODEL_NAME,\n",
    "    \"--output-dir\",\n",
    "    f\"{checkpoint_dir}\",\n",
    "]\n",
    "print(\" \".join(command))\n",
    "try:\n",
    "    subprocess.run(command, check=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, use Low Rank Adaptation.\n",
    "\n",
    "Some [Fireworks Models]() support [serverless LoRA deployment](https://docs.fireworks.ai/fine-tuning/fine-tuning-models), but full fine-tuning usually needs some form of reserved capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    MODEL_CONFIG = {\n",
    "        \"_component_\": MODELS[MODEL_NAME][\"modules\"][\"lora\"],\n",
    "        \"lora_attn_modules\": [\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"output_proj\",\n",
    "        ],\n",
    "        \"apply_lora_to_mlp\": True,\n",
    "        \"apply_lora_to_output\": False,\n",
    "        \"lora_rank\": 8,  # higher increases accuracy and memory\n",
    "        \"lora_alpha\": 16,  # usually alpha=2*rank\n",
    "        \"lora_dropout\": 0.0,\n",
    "    }\n",
    "else:\n",
    "    MODEL_CONFIG = {\n",
    "        \"_component_\": MODELS[MODEL_NAME][\"modules\"][\"full\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_CONFIG = MODELS[MODEL_NAME][\"tokenizer\"]\n",
    "if TOKENIZER_CONFIG.get(\"path\"):\n",
    "    TOKENIZER_CONFIG[\"path\"] = str(checkpoint_dir / TOKENIZER_CONFIG[\"path\"])\n",
    "if TOKENIZER_CONFIG.get(\"merges_file\"):\n",
    "    TOKENIZER_CONFIG[\"merges_file\"] = str(\n",
    "        checkpoint_dir / TOKENIZER_CONFIG[\"merges_file\"]\n",
    "    )\n",
    "TOKENIZER_CONFIG[\"max_seq_len\"] = (\n",
    "    None  # Can set to an integer value to reduce your memory footprint\n",
    ")\n",
    "\n",
    "TUNING_CONFIG = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    # Tokenizer\n",
    "    \"tokenizer\": TOKENIZER_CONFIG,\n",
    "    # Model Arguments\n",
    "    \"model\": MODEL_CONFIG,\n",
    "    \"checkpointer\": {\n",
    "        \"_component_\": \"torchtune.training.FullModelHFCheckpointer\",\n",
    "        \"checkpoint_dir\": str(checkpoint_dir),\n",
    "        \"checkpoint_files\": list_checkpoints(checkpoint_dir),\n",
    "        \"recipe_checkpoint\": None,\n",
    "        \"output_dir\": OUTPUT_DIR,\n",
    "        \"model_type\": MODELS[MODEL_NAME][\"checkpointer\"][\"model_type\"],\n",
    "    },\n",
    "    \"resume_from_checkpoint\": False,\n",
    "    \"save_adapter_weights_only\": USE_LORA,\n",
    "    # Optimizer and Scheduler\n",
    "    \"optimizer\": {\n",
    "        \"_component_\": \"torch.optim.AdamW\",\n",
    "        \"fused\": True,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lr\": 1e-4,\n",
    "    },\n",
    "    \"lr_scheduler\": {\n",
    "        \"_component_\": \"torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\",\n",
    "        \"num_warmup_steps\": 100,\n",
    "    },\n",
    "    \"loss\": {\n",
    "        \"_component_\": \"torchtune.modules.loss.LinearCrossEntropyLoss\",\n",
    "    },\n",
    "    # Training\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 2,\n",
    "    \"batch_size_val\": 2,\n",
    "    \"max_steps_per_epoch\": None,\n",
    "    \"gradient_accumulation_steps\": 8,  # Use to increase effective batch size\n",
    "    \"clip_grad_norm\": None,\n",
    "    \"compile\": False,  # torch.compile the model + loss: True increases speed + decreases memory\n",
    "    \"run_val_every_n_steps\": 10,\n",
    "    \"seed\": SEED,\n",
    "    \"shuffle\": True,\n",
    "    # Logging\n",
    "    \"log_every_n_steps\": 1,\n",
    "    \"log_peak_memory_stats\": True,\n",
    "    \"log_level\": \"INFO\",  # DEBUG, WARN, etc.\n",
    "    # Environment\n",
    "    \"device\": \"cuda\",\n",
    "    \"dtype\": \"bf16\",\n",
    "    \"enable_activation_checkpointing\": True,  # True reduces memory\n",
    "    \"enable_activation_offloading\": True,  # True reduces memory\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "import yaml\n",
    "from tensorzero import (\n",
    "    BooleanMetricNode,\n",
    "    ContentBlock,\n",
    "    FloatMetricNode,\n",
    "    RawText,\n",
    "    TensorZeroGateway,\n",
    "    Text,\n",
    "    Thought,\n",
    "    ToolCall,\n",
    "    ToolResult,\n",
    ")\n",
    "from tensorzero.internal import OutputMessage\n",
    "from tensorzero.util import uuid7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the metric configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"metrics\" in config, \"No `[metrics]` section found in config\"\n",
    "assert METRIC_NAME in config[\"metrics\"], (\n",
    "    f\"No metric named `{METRIC_NAME}` found in config\"\n",
    ")\n",
    "\n",
    "metric = config[\"metrics\"][METRIC_NAME]\n",
    "\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and render the stored inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = TensorZeroGateway.build_embedded(\n",
    "    config_file=CONFIG_PATH,\n",
    "    clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    "    timeout=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the metric filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"optimize\" in metric, \"Metric is missing the `optimize` field\"\n",
    "\n",
    "if metric.get(\"type\") == \"float\":\n",
    "    comparison_operator = \">=\" if metric[\"optimize\"] == \"max\" else \"<=\"\n",
    "    metric_node = FloatMetricNode(\n",
    "        metric_name=METRIC_NAME,\n",
    "        value=FLOAT_METRIC_THRESHOLD,\n",
    "        comparison_operator=comparison_operator,\n",
    "    )\n",
    "elif metric.get(\"type\") == \"boolean\":\n",
    "    metric_node = BooleanMetricNode(\n",
    "        metric_name=METRIC_NAME,\n",
    "        value=True if metric[\"optimize\"] == \"max\" else False,\n",
    "    )\n",
    "\n",
    "metric_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences and feedback from ClickHouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_inferences = tensorzero_client.experimental_list_inferences(\n",
    "    function_name=FUNCTION_NAME,\n",
    "    variant_name=None,\n",
    "    filters=metric_node,\n",
    "    limit=MAX_SAMPLES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the stored inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_inferences = tensorzero_client.experimental_render_inferences(\n",
    "    stored_inferences=stored_inferences,\n",
    "    variants={FUNCTION_NAME: TEMPLATE_VARIANT_NAME},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the rendered inferences to ChatML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_to_chatml(message: OutputMessage) -> Optional[List[Dict[str, Any]]]:\n",
    "    chatml_messages: List[Dict[str, Any]] = []\n",
    "    assert message.role in [\"user\", \"assistant\"], f\"Invalid role: {message.role}\"\n",
    "    content: List[str] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    for content_block in message.content:\n",
    "        if isinstance(content_block, Text):\n",
    "            assert content_block.arguments is None, \"Arguments should be None\"\n",
    "            content.append(content_block.text)\n",
    "        elif isinstance(content_block, RawText):\n",
    "            content.append(content_block.value)\n",
    "        elif isinstance(content_block, Thought):\n",
    "            content.append(f\"<think>{content_block['text']}</think>\")\n",
    "        elif isinstance(content_block, ToolCall):\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": content_block.raw_arguments,\n",
    "                        \"name\": content_block.name,\n",
    "                    },\n",
    "                    \"id\": content_block.id,\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif isinstance(content_block, ToolResult):\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            chatml_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block.id,\n",
    "                    \"content\": content_block.result,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {type(content_block)}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "    if content or tool_calls:\n",
    "        chatml_message: Dict[str, Any] = {\"role\": message.role}\n",
    "        if content:\n",
    "            chatml_message[\"content\"] = \"\\n\".join(content)\n",
    "        if tool_calls:\n",
    "            chatml_message[\"tool_calls\"] = tool_calls\n",
    "            if len(content) == 0:\n",
    "                chatml_message[\"content\"] = \"\"\n",
    "        chatml_messages.append(chatml_message)\n",
    "\n",
    "    return chatml_messages\n",
    "\n",
    "\n",
    "def output_to_chatml(output: List[ContentBlock]) -> Dict[str, Any]:\n",
    "    content: List[str] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in output:\n",
    "        if isinstance(content_block, Text):\n",
    "            assert content_block.arguments is None, \"Arguments should be None\"\n",
    "            content.append(content_block.text)\n",
    "        elif isinstance(content_block, Thought):\n",
    "            content.append(f\"<think>{content_block['text']}</think>\")\n",
    "        elif isinstance(content_block, ToolCall):\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": content_block.raw_arguments,\n",
    "                        \"name\": content_block.name,\n",
    "                    },\n",
    "                    \"id\": content_block.id,\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {type(content_block)}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = \"\\n\".join(content)\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "        if len(content) == 0:\n",
    "            output_message[\"content\"] = \"\"\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "conversations = []\n",
    "for rendered_inference in rendered_inferences:\n",
    "    messages = []\n",
    "    model_input = rendered_inference.input\n",
    "    if model_input.system is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": model_input.system})\n",
    "    for message in model_input.messages:\n",
    "        messages.extend(message_to_chatml(message))\n",
    "    messages.append(output_to_chatml(rendered_inference.output))\n",
    "\n",
    "    # Drop conversations that have unknown content\n",
    "    if all(msg is not None for msg in messages):\n",
    "        conversations.append(\n",
    "            {\n",
    "                \"conversation\": {\"messages\": messages},\n",
    "                \"episode_id\": rendered_inference.episode_id,\n",
    "            }\n",
    "        )\n",
    "\n",
    "conversations = pd.DataFrame(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = conversations[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - VAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = conversations[conversations[\"episode_id\"].isin(train_episode_ids)]\n",
    "val_df = conversations[conversations[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Actual validation fraction: {len(val_df) / len(conversations):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_dir = Path(temp_dir)\n",
    "\n",
    "    # Write training JSONL\n",
    "    train_json_path = temp_dir / \"train.json\"\n",
    "    with train_json_path.open(\"w\") as f:\n",
    "        for item in train_df[\"conversation\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Write evaluation JSONL\n",
    "    val_json_path = temp_dir / \"eval.json\"\n",
    "    with val_json_path.open(\"w\") as f:\n",
    "        for item in val_df[\"conversation\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Write YAML config\n",
    "    config_path = temp_dir / \"custom_8B_lora_single_device.yaml\"\n",
    "    TUNING_CONFIG[\"dataset\"] = {\n",
    "        \"_component_\": \"torchtune.datasets.chat_dataset\",\n",
    "        \"source\": \"json\",\n",
    "        \"packed\": False,  # True increases speed\n",
    "        \"data_files\": str(train_json_path),\n",
    "        \"conversation_column\": \"messages\",\n",
    "        \"conversation_style\": \"openai\",\n",
    "        \"train_on_input\": TRAIN_ON_INPUT,\n",
    "    }\n",
    "    TUNING_CONFIG[\"dataset_val\"] = {\n",
    "        \"_component_\": \"torchtune.datasets.chat_dataset\",\n",
    "        \"source\": \"json\",\n",
    "        \"packed\": False,  # True increases speed\n",
    "        \"data_files\": str(val_json_path),\n",
    "        \"conversation_column\": \"messages\",\n",
    "        \"conversation_style\": \"openai\",\n",
    "        \"train_on_input\": TRAIN_ON_INPUT,\n",
    "    }\n",
    "    TUNING_CONFIG[\"metric_logger\"] = {\n",
    "        \"_component_\": \"torchtune.training.metric_logging.DiskLogger\",\n",
    "        \"log_dir\": str(temp_dir / \"logs\"),\n",
    "    }\n",
    "    TUNING_CONFIG[\"profiler\"] = {  # Disabled\n",
    "        \"_component_\": \"torchtune.training.setup_torch_profiler\",\n",
    "        \"enabled\": False,\n",
    "        \"output_dir\": str(temp_dir / \"profiling_outputs\"),\n",
    "        \"cpu\": True,\n",
    "        \"cuda\": True,\n",
    "        \"profile_memory\": False,\n",
    "        \"with_stack\": False,\n",
    "        \"record_shapes\": True,\n",
    "        \"with_flops\": False,\n",
    "        \"wait_steps\": 5,\n",
    "        \"warmup_steps\": 3,\n",
    "        \"active_steps\": 2,\n",
    "        \"num_cycles\": 1,\n",
    "    }\n",
    "    with open(config_path, \"w\") as fp:\n",
    "        yaml.safe_dump(\n",
    "            TUNING_CONFIG,\n",
    "            fp,\n",
    "            sort_keys=False,\n",
    "            default_flow_style=False,  # expand lists/dicts in block style\n",
    "        )\n",
    "    print(f\"Config written to {config_path}\")\n",
    "    command = [\n",
    "        \"tune\",\n",
    "        \"run\",\n",
    "        \"--nnodes\",\n",
    "        str(NNODES),\n",
    "        \"--nproc_per_node\",\n",
    "        str(NPROC_PER_NODE),\n",
    "        \"lora_finetune_distributed\" if USE_LORA else \"full_finetune_distributed\",\n",
    "        \"--config\",\n",
    "        str(config_path),\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is done training, we need to [deploy](https://docs.fireworks.ai/fine-tuning/fine-tuning-models#deploying-and-using-a-model) it to Fireworks serverless inference. If you need high or guaranteed throughput you can also deploy the model to [reserved capacity](https://docs.fireworks.ai/deployments/reservations) or an on-demand [deployment](https://docs.fireworks.ai/guides/ondemand-deployments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"llama-v3p1-8b-instruct\"\n",
    "base_model_path = f\"accounts/fireworks/models/{base_model_id}\"\n",
    "\n",
    "fine_tuned_model_id = f\"{MODEL_NAME.lower().replace('/', '-').replace('.', 'p')}-{str(uuid7()).split('-')[-1]}\"\n",
    "\n",
    "checkpoint_dir = Path(OUTPUT_DIR) / f\"epoch_{TUNING_CONFIG['epochs'] - 1}\"\n",
    "\n",
    "command = [\n",
    "    \"firectl\",\n",
    "    \"create\",\n",
    "    \"model\",\n",
    "    fine_tuned_model_id,\n",
    "    str(checkpoint_dir),\n",
    "    \"--base-model\",\n",
    "    base_model_path,\n",
    "]\n",
    "try:\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    print(\"Command output:\", stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id(stdout: str) -> str:\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Name:\"):\n",
    "            return line.split(\":\")[1].strip()\n",
    "    raise ValueError(\"Model ID not found in output\")\n",
    "\n",
    "\n",
    "model_identifier = get_model_id(stdout)\n",
    "\n",
    "model_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deployment if not using a model with serverless support, if it does not support serveless addons, or if you are doing full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SERVERLESS:\n",
    "    command = [\"firectl\", \"create\", \"deployment\", model_identifier]\n",
    "    print(\" \".join(command))\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr.decode(\"utf-8\"))\n",
    "    else:\n",
    "        stdout = result.stdout.decode(\"utf-8\")\n",
    "        print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LoRA addon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    command = [\"firectl\", \"load-lora\", model_identifier]\n",
    "    print(\" \".join(command))\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr.decode(\"utf-8\"))\n",
    "    else:\n",
    "        stdout = result.stdout.decode(\"utf-8\")\n",
    "        print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, you can add the fine-tuned model to your config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"models\": {\n",
    "        model_identifier: {\n",
    "            \"routing\": [\"fireworks\"],\n",
    "            \"providers\": {\n",
    "                \"fireworks\": {\"type\": \"fireworks\", \"model_name\": model_identifier}\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"weight\": 0,\n",
    "    \"model\": model_identifier,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {model_identifier: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
