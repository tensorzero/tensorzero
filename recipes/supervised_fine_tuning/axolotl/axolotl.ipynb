{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Axolotl Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune models using [Axolotl](https://docs.axolotl.ai) and their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "\n",
    "We demonstrate how to deploy a LoRA fine-tuned model for serverless inference using [Fireworks](https://fireworks.ai). Full instructions to deploy LoRA or full fine-tuned models are provided by [Fireworks](https://docs.fireworks.ai/fine-tuning/fine-tuning-models), [Together](https://docs.together.ai/docs/deploying-a-fine-tuned-model), and other inference providers. You can also use [vLLM](https://docs.vllm.ai/en/latest/examples/online_serving/api_client.html) to serve your fine-tuned model locally. The TensorZero client seemlessly integrates inference using your fine-tuned model for any of these approaches.\n",
    "\n",
    "To get started:\n",
    "\n",
    "- Set your `TENSORZERO_CLICKHOUSE_URL` enironment variable to point to the database containing the historical inferences you'd like to train on.\n",
    "- Set your `HF_TOKEN` to use Llama or Gemma models downloaded through huggingface.\n",
    "- You'll also need to [install](https://docs.fireworks.ai/tools-sdks/firectl/firectl) the CLI tool `firectl` on your machine and sign in with `firectl signin`. You can test that this all worked with `firectl whoami`.\n",
    "- Update the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "METRIC_NAME = \"exact_match\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"  # It's OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# Random seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a model to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The huggingface name of the model to fine-tune (Axolotl supports various models like LLaMA, Mistral, Mixtral, Pythia, and more)\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# The name of the chat template to use\n",
    "# - tokenizer_default: Uses the chat template that is available in the tokenizer_config.json. If the chat template is not available in the tokenizer, it will raise an error.\n",
    "# - alpaca/inst/chatml/gemma/cohere/llama3/phi_3/deepseek_v2/jamba: These chat templates are available in the axolotl codebase at src/axolotl/utils/chat_templates.py\n",
    "CHAT_TEMPLATE = \"llama3\"\n",
    "\n",
    "# Whether to use LoRA or not. Set to False for full model fine-tuning\n",
    "# If set to False, SEVERLESS must also be False as you will need to create your own deployment\n",
    "USE_LORA = True\n",
    "\n",
    "# Whether to use a serverless deployment.\n",
    "# Set to False is full model fine tuning or using LoRA for a model without serverless support\n",
    "SERVERLESS = True\n",
    "\n",
    "# Can add \"user\" to the list to fine-tune on user messages also\n",
    "ROLES_TO_TRAIN = [\"assistant\"]\n",
    "\n",
    "# Number of server nodes to use\n",
    "DISTRIBUTED = False  # Only set to True if multiple GPUs are available. DeepSpeed will throw an error if only one GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the tuning parameters. A complete list of all [configuration options](https://docs.axolotl.ai/docs/config.html) is provided by Axolotl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorzero.util import uuid7\n",
    "\n",
    "TUNE_CONFIG = {\n",
    "    \"output_dir\": f\"./outputs/{MODEL_NAME}/{uuid7()}\",\n",
    "    # Model\n",
    "    \"base_model\": MODEL_NAME,  # This can also be a relative path to a model on disk\n",
    "    \"tokenizer_type\": \"AutoTokenizer\",\n",
    "    \"load_in_8bit\": True,  # Set to false for full fine-tuning\n",
    "    \"load_in_4bit\": False,\n",
    "    \"sequence_len\": 8192,\n",
    "    \"sample_packing\": True,\n",
    "    \"eval_sample_packing\": False,\n",
    "    \"pad_to_sequence_len\": True,\n",
    "    # Optimization\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"micro_batch_size\": 2,\n",
    "    \"num_epochs\": 4,\n",
    "    \"optimizer\": \"adamw_bnb_8bit\",\n",
    "    \"lr_scheduler\": \"cosine\",\n",
    "    \"learning_rate\": 0.0002,  # May want to set lower for full fine-tuning. e.g., 2e-5\n",
    "    \"warmup_steps\": 10,  # May want to increase for full fine-tuning. e.g., 100\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"bf16\": \"auto\",\n",
    "    \"tf32\": False,\n",
    "    # Logging\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"resume_from_checkpoint\": None,\n",
    "    \"logging_steps\": 1,\n",
    "    \"flash_attention\": True,\n",
    "    \"evals_per_epoch\": 2,\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"special_tokens\": {\"pad_token\": \"<|end_of_text|>\"},\n",
    "    # WandB configuration\n",
    "    \"wandb_project\": None,\n",
    "    \"wandb_entity\": None,\n",
    "    \"wandb_watch\": None,\n",
    "    \"wandb_name\": None,\n",
    "    \"wandb_log_model\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, use Low Rank Adaptation.\n",
    "\n",
    "Some [Fireworks Models]() support [serverless LoRA deployment](https://docs.fireworks.ai/fine-tuning/fine-tuning-models), but full fine-tuning usually needs some form of reserved capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    TUNE_CONFIG.update(\n",
    "        {\n",
    "            \"adapter\": \"lora\",\n",
    "            \"lora_model_dir\": None,\n",
    "            \"lora_r\": 8,\n",
    "            \"lora_alpha\": 16,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"lora_target_modules\": [\n",
    "                \"q_proj\",\n",
    "                \"v_proj\",\n",
    "                \"k_proj\",\n",
    "                \"o_proj\",\n",
    "            ],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "import yaml\n",
    "from clickhouse_connect import get_client\n",
    "from tensorzero import (\n",
    "    ContentBlock,\n",
    "    RawText,\n",
    "    StoredChatInference,\n",
    "    StoredJsonInference,\n",
    "    TensorZeroGateway,\n",
    "    Text,\n",
    "    Thought,\n",
    "    ToolCall,\n",
    "    ToolResult,\n",
    ")\n",
    "from tensorzero.internal import OutputMessage\n",
    "from tensorzero.util import uuid7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORZERO_GATEWAY_URL = \"http://localhost:3000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the metric configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"metrics\" in config, \"No `[metrics]` section found in config\"\n",
    "assert METRIC_NAME in config[\"metrics\"], (\n",
    "    f\"No metric named `{METRIC_NAME}` found in config\"\n",
    ")\n",
    "\n",
    "metric = config[\"metrics\"][METRIC_NAME]\n",
    "\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    function_type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_table_name = {\n",
    "    \"float\": \"FloatMetricFeedback\",\n",
    "    \"boolean\": \"BooleanMetricFeedback\",\n",
    "}.get(metric[\"type\"])\n",
    "\n",
    "if feedback_table_name is None:\n",
    "    raise ValueError(f\"Unsupported metric type: {metric['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the correct join key to use for the metric on the inference table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_join_key = {\n",
    "    \"episode\": \"episode_id\",\n",
    "    \"inference\": \"id\",\n",
    "}.get(metric[\"level\"])\n",
    "\n",
    "if inference_join_key is None:\n",
    "    raise ValueError(f\"Unsupported metric level: {metric['level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences and feedback from ClickHouse.\n",
    "\n",
    "If the metric is a float metric, we need to filter the data based on the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"optimize\" in metric, \"Metric is missing the `optimize` field\"\n",
    "\n",
    "threshold = FLOAT_METRIC_THRESHOLD if metric[\"type\"] == \"float\" else 0.5\n",
    "comparison_operator = \">=\" if metric[\"optimize\"] == \"max\" else \"<=\"\n",
    "\n",
    "inference_col = \"tool_params\" if function_type == \"chat\" else \"output_schema\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    i.variant_name,\n",
    "    i.input,\n",
    "    i.output,\n",
    "    f.value,\n",
    "    i.episode_id,\n",
    "    i.id,\n",
    "    i.{inference_col},\n",
    "FROM\n",
    "    {inference_table_name} i\n",
    "JOIN\n",
    "    (SELECT\n",
    "        target_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY target_id ORDER BY timestamp DESC) as rn\n",
    "    FROM\n",
    "        {feedback_table_name}\n",
    "    WHERE\n",
    "        metric_name = %(metric_name)s\n",
    "        AND value {comparison_operator} %(threshold)s\n",
    "    ) f ON i.{inference_join_key} = f.target_id and f.rn = 1\n",
    "WHERE\n",
    "    i.function_name = %(function_name)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"metric_name\": METRIC_NAME,\n",
    "    \"comparison_operator\": comparison_operator,\n",
    "    \"threshold\": threshold,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and render the stored inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = TensorZeroGateway.build_embedded(\n",
    "    config_file=CONFIG_PATH,\n",
    "    clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    "    timeout=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_parse_arguments(example_input):\n",
    "    for message in example_input[\"messages\"]:\n",
    "        for block in message[\"content\"]:\n",
    "            if block[\"type\"] == \"tool_call\":\n",
    "                block[\"arguments\"] = json.loads(block[\"arguments\"])\n",
    "\n",
    "\n",
    "stored_inferences = []\n",
    "for _, row in df.iterrows():\n",
    "    input_data = json.loads(row[\"input\"])\n",
    "    double_parse_arguments(input_data)\n",
    "    output_data = json.loads(row[\"output\"])\n",
    "    if function_type == \"chat\":\n",
    "        stored_inferences.append(\n",
    "            StoredChatInference(\n",
    "                function_name=FUNCTION_NAME,\n",
    "                variant_name=row[\"variant_name\"],\n",
    "                input=input_data,\n",
    "                output=output_data,\n",
    "                episode_id=row[\"episode_id\"],\n",
    "                inference_id=row[\"id\"],\n",
    "                tool_params=json.loads(row[\"tool_params\"]),\n",
    "            )\n",
    "        )\n",
    "    elif function_type == \"json\":\n",
    "        stored_inferences.append(\n",
    "            StoredJsonInference(\n",
    "                function_name=FUNCTION_NAME,\n",
    "                variant_name=row[\"variant_name\"],\n",
    "                input=input_data,\n",
    "                output=output_data,\n",
    "                episode_id=row[\"episode_id\"],\n",
    "                inference_id=row[\"id\"],\n",
    "                output_schema=json.loads(row[\"output_schema\"]),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_inferences = tensorzero_client.experimental_render_inferences(\n",
    "    stored_inferences=stored_inferences,\n",
    "    variants={FUNCTION_NAME: TEMPLATE_VARIANT_NAME},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the rendered inferences to ChatML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_to_chatml(message: OutputMessage) -> Optional[List[Dict[str, Any]]]:\n",
    "    chatml_messages: List[Dict[str, Any]] = []\n",
    "    assert message.role in [\"user\", \"assistant\"], f\"Invalid role: {message.role}\"\n",
    "    content: List[str] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    for content_block in message.content:\n",
    "        if isinstance(content_block, Text):\n",
    "            assert content_block.arguments is None, \"Arguments should be None\"\n",
    "            content.append(content_block.text)\n",
    "        elif isinstance(content_block, RawText):\n",
    "            content.append(content_block.value)\n",
    "        elif isinstance(content_block, Thought):\n",
    "            content.append(f\"<think>{content_block['text']}</think>\")\n",
    "        elif isinstance(content_block, ToolCall):\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": content_block.raw_arguments,\n",
    "                        \"name\": content_block.name,\n",
    "                    },\n",
    "                    \"id\": content_block.id,\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif isinstance(content_block, ToolResult):\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            chatml_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block.id,\n",
    "                    \"content\": content_block.result,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {type(content_block)}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return [None]\n",
    "    if len(tool_calls) > 1:\n",
    "        return [None]\n",
    "    if content or tool_calls:\n",
    "        chatml_message: Dict[str, Any] = {\"role\": message.role}\n",
    "        if content:\n",
    "            chatml_message[\"content\"] = \"\\n\".join(content)\n",
    "        if tool_calls:\n",
    "            chatml_message[\"tool_calls\"] = tool_calls\n",
    "        chatml_messages.append(chatml_message)\n",
    "\n",
    "    return chatml_messages\n",
    "\n",
    "\n",
    "def output_to_chatml(output: List[ContentBlock]) -> Dict[str, Any]:\n",
    "    content: List[str] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in output:\n",
    "        if isinstance(content_block, Text):\n",
    "            assert content_block.arguments is None, \"Arguments should be None\"\n",
    "            content.append(content_block.text)\n",
    "        elif isinstance(content_block, Thought):\n",
    "            content.append(f\"<think>{content_block['text']}</think>\")\n",
    "        elif isinstance(content_block, ToolCall):\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": content_block.raw_arguments,\n",
    "                        \"name\": content_block.name,\n",
    "                    },\n",
    "                    \"id\": content_block.id,\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {type(content_block)}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if len(tool_calls) > 1:\n",
    "        return None\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = \"\\n\".join(content)\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "conversations = []\n",
    "for rendered_inference in rendered_inferences:\n",
    "    messages = []\n",
    "    model_input = rendered_inference.input\n",
    "    if model_input.system is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": model_input.system})\n",
    "    for message in model_input.messages:\n",
    "        messages.extend(message_to_chatml(message))\n",
    "    messages.append(output_to_chatml(rendered_inference.output))\n",
    "\n",
    "    # Drop conversations that have unknown content\n",
    "    if all(msg is not None for msg in messages):\n",
    "        conversations.append(\n",
    "            {\n",
    "                \"conversation\": {\"messages\": messages},\n",
    "                \"episode_id\": rendered_inference.episode_id,\n",
    "            }\n",
    "        )\n",
    "\n",
    "conversations = pd.DataFrame(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = conversations[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - VAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = conversations[conversations[\"episode_id\"].isin(train_episode_ids)]\n",
    "val_df = conversations[conversations[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Actual validation fraction: {len(val_df) / len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up distributed computing using [DeepSpeed](https://www.deepspeed.ai) if specified. See Axolotl for [distributed computing guidance](https://docs.axolotl.ai/docs/multi-gpu.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISTRIBUTED:\n",
    "    command = [\n",
    "        \"axolotl\",\n",
    "        \"fetch\",\n",
    "        \"deepspeed_configs\",\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        TUNE_CONFIG[\"deepspeed\"] = \"deepspeed_configs/zero1.json\"\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_dir = Path(temp_dir)\n",
    "\n",
    "    # Write training JSONL\n",
    "    train_json_path = temp_dir / \"train.jsonl\"\n",
    "    with train_json_path.open(\"w\") as f:\n",
    "        for item in train_df[\"conversation\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Write evaluation JSONL\n",
    "    val_json_path = temp_dir / \"eval.jsonl\"\n",
    "    with val_json_path.open(\"w\") as f:\n",
    "        for item in val_df[\"conversation\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Write YAML config\n",
    "    config_path = temp_dir / \"config.yaml\"\n",
    "    TUNE_CONFIG[\"datasets\"] = [\n",
    "        {\n",
    "            \"path\": str(train_json_path),\n",
    "            \"type\": \"chat_template\",\n",
    "            \"chat_template\": CHAT_TEMPLATE,\n",
    "            \"field_messages\": \"messages\",\n",
    "            \"field_system\": \"system\",\n",
    "            \"roles_to_train\": ROLES_TO_TRAIN,\n",
    "        }\n",
    "    ]\n",
    "    TUNE_CONFIG[\"test_datasets\"] = [\n",
    "        {\n",
    "            \"path\": str(val_json_path),\n",
    "            \"ds_type\": \"json\",\n",
    "            \"split\": \"train\",\n",
    "            \"type\": \"chat_template\",\n",
    "            \"chat_template\": CHAT_TEMPLATE,\n",
    "            \"data_files\": [str(val_json_path)],\n",
    "        }\n",
    "    ]\n",
    "    with open(config_path, \"w\") as fp:\n",
    "        yaml.safe_dump(\n",
    "            TUNE_CONFIG,\n",
    "            fp,\n",
    "            sort_keys=False,\n",
    "            default_flow_style=False,  # expand lists/dicts in block style\n",
    "        )\n",
    "    print(f\"Config written to {config_path}\")\n",
    "    command = [\n",
    "        \"axolotl\",\n",
    "        \"train\",\n",
    "        str(config_path),\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is done training, we need to [deploy](https://docs.fireworks.ai/fine-tuning/fine-tuning-models#deploying-and-using-a-model) it to Fireworks serverless inference. If you need high or guaranteed throughput you can also deploy the model to [reserved capacity](https://docs.fireworks.ai/deployments/reservations) or an on-demand [deployment](https://docs.fireworks.ai/guides/ondemand-deployments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"llama-v3p1-8b-instruct\"\n",
    "base_model_path = f\"accounts/fireworks/models/{base_model_id}\"\n",
    "\n",
    "fine_tuned_model_id = f\"{MODEL_NAME.lower().replace('/', '-').replace('.', 'p')}-{str(uuid7()).split('-')[-1]}\"\n",
    "\n",
    "command = [\n",
    "    \"firectl\",\n",
    "    \"create\",\n",
    "    \"model\",\n",
    "    fine_tuned_model_id,\n",
    "    TUNE_CONFIG[\"output_dir\"],\n",
    "    \"--base-model\",\n",
    "    base_model_path,\n",
    "]\n",
    "try:\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    print(\"Command output:\", stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id(stdout: str) -> str:\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Name:\"):\n",
    "            return line.split(\":\")[1].strip()\n",
    "    raise ValueError(\"Model ID not found in output\")\n",
    "\n",
    "\n",
    "model_identifier = get_model_id(stdout)\n",
    "\n",
    "model_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deployment if not using a model with serverless support, if it does not support serveless addons, or if you are doing full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SERVERLESS:\n",
    "    command = [\"firectl\", \"create\", \"deployment\", model_identifier]\n",
    "    print(\" \".join(command))\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr.decode(\"utf-8\"))\n",
    "    else:\n",
    "        stdout = result.stdout.decode(\"utf-8\")\n",
    "        print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LoRA addon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    command = [\"firectl\", \"load-lora\", model_identifier]\n",
    "    print(\" \".join(command))\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr.decode(\"utf-8\"))\n",
    "    else:\n",
    "        stdout = result.stdout.decode(\"utf-8\")\n",
    "        print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, you can add the fine-tuned model to your config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"models\": {\n",
    "        model_identifier: {\n",
    "            \"routing\": [\"fireworks\"],\n",
    "            \"providers\": {\n",
    "                \"fireworks\": {\"type\": \"fireworks\", \"model_name\": model_identifier}\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"weight\": 0,\n",
    "    \"model\": model_identifier,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {model_identifier: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
