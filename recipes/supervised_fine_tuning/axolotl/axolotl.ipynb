{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Axolotl Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune models using [Axolotl](https://docs.axolotl.ai) and their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "\n",
    "We demonstrate how to deploy a LoRA fine-tuned model for serverless inference using [Fireworks](https://fireworks.ai). Full instructions to deploy LoRA or full fine-tuned models are provided by [Fireworks](https://docs.fireworks.ai/fine-tuning/fine-tuning-models), [Together](https://docs.together.ai/docs/deploying-a-fine-tuned-model), and other inference providers. You can also use [vLLM](https://docs.vllm.ai/en/latest/examples/online_serving/api_client.html) to serve your fine-tuned model locally. The TensorZero client seemlessly integrates inference using your fine-tuned model for any of these approaches.\n",
    "\n",
    "To get started:\n",
    "\n",
    "- Set your `TENSORZERO_CLICKHOUSE_URL` enironment variable to point to the database containing the historical inferences you'd like to train on.\n",
    "- Set your `HF_TOKEN` to use Llama or Gemma models downloaded through huggingface.\n",
    "- [Install](https://docs.fireworks.ai/tools-sdks/firectl/firectl) the CLI tool `firectl` on your machine and sign in with `firectl signin`. You can test that this all worked with `firectl whoami`. We use `firectl` for deployment to Fireworks in this example but you can serve the model however you prefer.\n",
    "- Update the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "METRIC_NAME = \"jaccard_similarity\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"  # It's OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# Random seed\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a model to fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The huggingface name of the model to fine-tune (Axolotl supports various models like LLaMA, Mistral, Mixtral, Pythia, and more)\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# The name of the chat template to use\n",
    "# - tokenizer_default: Uses the chat template that is available in the tokenizer_config.json. If the chat template is not available in the tokenizer, it will raise an error.\n",
    "# - alpaca/inst/chatml/gemma/cohere/llama3/phi_3/deepseek_v2/jamba: These chat templates are available in the axolotl codebase at src/axolotl/utils/chat_templates.py\n",
    "CHAT_TEMPLATE = \"llama3\"\n",
    "\n",
    "# Whether to use LoRA or not. Set to False for full model fine-tuning\n",
    "# If set to False, SEVERLESS must also be False as you will need to create your own deployment\n",
    "USE_LORA = True\n",
    "\n",
    "# Whether to use a serverless deployment.\n",
    "# Set to False is full model fine tuning or using LoRA for a model without serverless support\n",
    "SERVERLESS = True\n",
    "\n",
    "# Can add \"user\" to the list to fine-tune on user messages also\n",
    "ROLES_TO_TRAIN = [\"assistant\"]\n",
    "\n",
    "# Number of server nodes to use\n",
    "DISTRIBUTED = False  # Only set to True if multiple GPUs are available. DeepSpeed will throw an error if only one GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the tuning parameters. A complete list of all [configuration options](https://docs.axolotl.ai/docs/config.html) is provided by Axolotl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorzero.util import uuid7\n",
    "\n",
    "TUNE_CONFIG = {\n",
    "    \"output_dir\": f\"./outputs/{MODEL_NAME}/{uuid7()}\",\n",
    "    # Model\n",
    "    \"base_model\": MODEL_NAME,  # This can also be a relative path to a model on disk\n",
    "    \"tokenizer_type\": \"AutoTokenizer\",\n",
    "    \"load_in_8bit\": True,  # Set to false for full fine-tuning\n",
    "    \"load_in_4bit\": False,\n",
    "    \"sequence_len\": 8192,\n",
    "    \"sample_packing\": True,\n",
    "    \"eval_sample_packing\": False,\n",
    "    \"pad_to_sequence_len\": True,\n",
    "    # Optimization\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"micro_batch_size\": 2,\n",
    "    \"num_epochs\": 4,\n",
    "    \"optimizer\": \"adamw_bnb_8bit\",\n",
    "    \"lr_scheduler\": \"cosine\",\n",
    "    \"learning_rate\": 0.0002,  # May want to set lower for full fine-tuning. e.g., 2e-5\n",
    "    \"warmup_steps\": 10,  # May want to increase for full fine-tuning. e.g., 100\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"bf16\": \"auto\",\n",
    "    \"tf32\": False,\n",
    "    # Logging\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"resume_from_checkpoint\": None,\n",
    "    \"logging_steps\": 1,\n",
    "    \"flash_attention\": True,\n",
    "    \"evals_per_epoch\": 2,\n",
    "    \"save_strategy\": \"no\",\n",
    "    \"special_tokens\": {\"pad_token\": \"<|end_of_text|>\"},\n",
    "    # WandB configuration\n",
    "    \"wandb_project\": None,\n",
    "    \"wandb_entity\": None,\n",
    "    \"wandb_watch\": None,\n",
    "    \"wandb_name\": None,\n",
    "    \"wandb_log_model\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, use Low Rank Adaptation.\n",
    "\n",
    "Some [Fireworks Models]() support [serverless LoRA deployment](https://docs.fireworks.ai/fine-tuning/fine-tuning-models), but full fine-tuning usually needs some form of reserved capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    TUNE_CONFIG.update(\n",
    "        {\n",
    "            \"adapter\": \"lora\",\n",
    "            \"lora_model_dir\": None,\n",
    "            \"lora_r\": 8,\n",
    "            \"lora_alpha\": 16,\n",
    "            \"lora_dropout\": 0.05,\n",
    "            \"lora_target_modules\": [\n",
    "                \"q_proj\",\n",
    "                \"v_proj\",\n",
    "                \"k_proj\",\n",
    "                \"o_proj\",\n",
    "            ],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "tensorzero_path = os.path.abspath(os.path.join(os.getcwd(), \"../../../\"))\n",
    "if tensorzero_path not in sys.path:\n",
    "    sys.path.append(tensorzero_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import toml\n",
    "import yaml\n",
    "from tensorzero import (\n",
    "    FloatMetricFilter,\n",
    "    TensorZeroGateway,\n",
    ")\n",
    "from tensorzero.util import uuid7\n",
    "\n",
    "from recipes.util import tensorzero_rendered_samples_to_conversations, train_val_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the TensorZero client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = TensorZeroGateway.build_embedded(\n",
    "    config_file=CONFIG_PATH,\n",
    "    clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    "    timeout=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the metric filter as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_operator = \">=\"\n",
    "metric_node = FloatMetricFilter(\n",
    "    metric_name=METRIC_NAME,\n",
    "    value=FLOAT_METRIC_THRESHOLD,\n",
    "    comparison_operator=comparison_operator,\n",
    ")\n",
    "# from tensorzero import BooleanMetricFilter\n",
    "# metric_node = BooleanMetricFilter(\n",
    "#     metric_name=METRIC_NAME,\n",
    "#     value=True  # or False\n",
    "# )\n",
    "\n",
    "metric_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences from ClickHouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_samples = tensorzero_client.experimental_list_inferences(\n",
    "    function_name=FUNCTION_NAME,\n",
    "    variant_name=None,\n",
    "    output_source=\"inference\",  # could also be \"demonstration\"\n",
    "    filters=metric_node,\n",
    "    limit=MAX_SAMPLES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the inputs using the templates in the template variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_samples = tensorzero_client.experimental_render_samples(\n",
    "    stored_samples=stored_samples,\n",
    "    variants={FUNCTION_NAME: TEMPLATE_VARIANT_NAME},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples, eval_samples = train_val_split(\n",
    "    rendered_samples,\n",
    "    val_size=VAL_FRACTION,\n",
    "    last_inference_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the rendered samples to openai format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conversations = tensorzero_rendered_samples_to_conversations(train_samples, conversation_key=\"messages\")\n",
    "eval_conversations = tensorzero_rendered_samples_to_conversations(eval_samples, conversation_key=\"messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up distributed computing using [DeepSpeed](https://www.deepspeed.ai) if specified. See Axolotl for [distributed computing guidance](https://docs.axolotl.ai/docs/multi-gpu.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DISTRIBUTED:\n",
    "    command = [\n",
    "        \"axolotl\",\n",
    "        \"fetch\",\n",
    "        \"deepspeed_configs\",\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        TUNE_CONFIG[\"deepspeed\"] = \"deepspeed_configs/zero1.json\"\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    temp_dir = Path(temp_dir)\n",
    "\n",
    "    # Write training JSONL\n",
    "    train_json_path = temp_dir / \"train.jsonl\"\n",
    "    with train_json_path.open(\"w\") as f:\n",
    "        for item in train_conversations:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Write evaluation JSONL\n",
    "    val_json_path = temp_dir / \"eval.jsonl\"\n",
    "    with val_json_path.open(\"w\") as f:\n",
    "        for item in eval_conversations:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    # Write YAML config\n",
    "    config_path = temp_dir / \"config.yaml\"\n",
    "    TUNE_CONFIG[\"datasets\"] = [\n",
    "        {\n",
    "            \"path\": str(train_json_path),\n",
    "            \"type\": \"chat_template\",\n",
    "            \"chat_template\": CHAT_TEMPLATE,\n",
    "            \"field_messages\": \"messages\",\n",
    "            \"field_system\": \"system\",\n",
    "            \"roles_to_train\": ROLES_TO_TRAIN,\n",
    "        }\n",
    "    ]\n",
    "    TUNE_CONFIG[\"test_datasets\"] = [\n",
    "        {\n",
    "            \"path\": str(val_json_path),\n",
    "            \"ds_type\": \"json\",\n",
    "            \"split\": \"train\",\n",
    "            \"type\": \"chat_template\",\n",
    "            \"chat_template\": CHAT_TEMPLATE,\n",
    "            \"data_files\": [str(val_json_path)],\n",
    "        }\n",
    "    ]\n",
    "    TUNE_CONFIG[\"dataset_prepared_path\"] = str(temp_dir / \"prepared\")\n",
    "    with open(config_path, \"w\") as fp:\n",
    "        yaml.safe_dump(\n",
    "            TUNE_CONFIG,\n",
    "            fp,\n",
    "            sort_keys=False,\n",
    "            default_flow_style=False,  # expand lists/dicts in block style\n",
    "        )\n",
    "    print(f\"Config written to {config_path}\")\n",
    "    # preprocess dataset\n",
    "    command = [\n",
    "        \"axolotl\",\n",
    "        \"preprocess\",\n",
    "        str(config_path),\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error occurred:\", e.stderr)\n",
    "    # train\n",
    "    command = [\n",
    "        \"axolotl\",\n",
    "        \"train\",\n",
    "        str(config_path),\n",
    "    ]\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is done training, we need to [deploy](https://docs.fireworks.ai/fine-tuning/fine-tuning-models#deploying-and-using-a-model) it to Fireworks serverless inference. If you need high or guaranteed throughput you can also deploy the model to [reserved capacity](https://docs.fireworks.ai/deployments/reservations) or an on-demand [deployment](https://docs.fireworks.ai/guides/ondemand-deployments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"llama-v3p3-70b-instruct\"\n",
    "base_model_path = f\"accounts/fireworks/models/{base_model_id}\"\n",
    "\n",
    "fine_tuned_model_id = f\"{MODEL_NAME.lower().replace('/', '-').replace('.', 'p')}-{str(uuid7()).split('-')[-1]}\"\n",
    "\n",
    "command = [\n",
    "    \"firectl\",\n",
    "    \"create\",\n",
    "    \"model\",\n",
    "    fine_tuned_model_id,\n",
    "    TUNE_CONFIG[\"output_dir\"],\n",
    "    \"--base-model\",\n",
    "    base_model_path,\n",
    "]\n",
    "try:\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    stdout = result.stdout.decode(\"utf-8\")\n",
    "    print(\"Command output:\", stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error occurred:\", e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_id(stdout: str) -> str:\n",
    "    for line in stdout.splitlines():\n",
    "        if line.strip().startswith(\"Name:\"):\n",
    "            return line.split(\":\")[1].strip()\n",
    "    raise ValueError(\"Model ID not found in output\")\n",
    "\n",
    "\n",
    "model_identifier = get_model_id(stdout)\n",
    "\n",
    "model_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a deployment if not using a model with serverless support, if it does not support serveless addons, or if you are doing full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SERVERLESS:\n",
    "    command = [\"firectl\", \"create\", \"deployment\", model_identifier]\n",
    "    print(\" \".join(command))\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr.decode(\"utf-8\"))\n",
    "    else:\n",
    "        stdout = result.stdout.decode(\"utf-8\")\n",
    "        print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the LoRA addon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    command = [\"firectl\", \"load-lora\", model_identifier]\n",
    "    print(\" \".join(command))\n",
    "    result = subprocess.run(command, capture_output=True)\n",
    "    if result.returncode != 0:\n",
    "        print(result.stderr.decode(\"utf-8\"))\n",
    "    else:\n",
    "        stdout = result.stdout.decode(\"utf-8\")\n",
    "        print(stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is deployed, you can add the fine-tuned model to your config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"models\": {\n",
    "        model_identifier: {\n",
    "            \"routing\": [\"fireworks\"],\n",
    "            \"providers\": {\"fireworks\": {\"type\": \"fireworks\", \"model_name\": model_identifier}},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
