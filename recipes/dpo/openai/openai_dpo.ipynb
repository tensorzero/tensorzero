{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ee840c-d247-4c7e-ab3f-d31cc7dd1fef",
   "metadata": {},
   "source": [
    "# OpenAI Supervised Fine-Tuning using Direct Preference Optimization (DPO)\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune OpenAI models using Direct Preference Optimization (DPO) and their own data. Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a899c588-7265-4589-a636-c595d818c81b",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    " - Set the `CLICKHOUSE_URL` environment variable. For example: `CLICKHOUSE_URL`=`\"http://localhost:8123/tensorzero\"`\n",
    " - Set the `OPENAI_API_KEY` environment variable.\n",
    " - Update the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26cfcec-a2dd-4f89-b3df-5f8623002e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../examples/ner-fine-tuning-demonstrations/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt4o_initial_prompt\"\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://platform.openai.com/docs/guides/fine-tuning)\n",
    "MODEL_NAME = \"gpt-4o-2024-08-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb9147-8dfa-4a1a-82b7-6fa994e1f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import openai\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fe2dbd-edb8-4630-a91e-5a9aa754f237",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69892de5-ec2a-401d-a674-ac5dcd11f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91982a3b-cdcf-4b35-a3b3-683bce76b16f",
   "metadata": {},
   "source": [
    "Ensure that the function and variant being fine-tuned are present in the provided config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0a41e-871d-45c1-a05e-02a02665d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e11f8-f24f-430e-97a3-78edd815c9aa",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we will use for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f71686-f62c-4df2-99f0-fb2b48269926",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab21e02-6584-4422-abb0-ba7400d26c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95bd3a-3fdc-484c-8f1d-3a3776f2c6d0",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f03fce-dd4a-42bc-8e0b-47d12f767f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"CLICKHOUSE_URL\" in os.environ, \"CLICKHOUSE_URL environment variable not set\"\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd8383-bedf-4564-8db4-1581e110b201",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca296a-1ee8-4ace-97e4-d7c9dd33e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"json\": \"JsonInference\"}.get(function_type)\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29099c61-d84c-4008-abc1-1d31615c83d2",
   "metadata": {},
   "source": [
    "Query ClickHouse for inference, feedback, and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53976e1b-ee2a-4308-9372-a57965698121",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "    i.variant_name AS variant,\n",
    "    i.input AS input,\n",
    "    i.output AS non_preferred_output,\n",
    "    d.value AS preferred_output\n",
    "FROM \n",
    "    {inference_table_name} AS i\n",
    "INNER \n",
    "    JOIN DemonstrationFeedback AS d ON i.id = d.inference_id\n",
    "WHERE \n",
    "    (i.function_name = %(function_name)s) AND\n",
    "    (i.variant_name = %(variant_name)s)\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"variant_name\": TEMPLATE_VARIANT_NAME,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c1620-e850-4fd7-a3d1-d005623f4f51",
   "metadata": {},
   "source": [
    "OpenAI requires the fine-tuning data (for DPO) to be structured in this [format](https://platform.openai.com/docs/guides/fine-tuning#preference)  \n",
    "\n",
    "``` \n",
    "{\n",
    "  \"input\": {\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"<string>\"\n",
    "      }\n",
    "    ],\n",
    "    \"tools\": [],\n",
    "    \"parallel_tool_calls\": true\n",
    "  },\n",
    "  \"preferred_output\": [\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"<string>\"\n",
    "    }\n",
    "  ],\n",
    "  \"non_preferred_output\": [\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"<string>\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6621131-bd25-42a1-8190-713415346c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(content: List[Dict[str, Any]], role: str) -> str:\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "\n",
    "    if len(content) != 1:\n",
    "        raise ValueError(f\"Message must have exactly one content block: {content}\")\n",
    "\n",
    "    if content[0][\"type\"] != \"text\":\n",
    "        raise ValueError(f\"Content block must be of the type text: {content}\")\n",
    "\n",
    "    content = content[0][\"value\"]\n",
    "\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return env.render_template(role, **content)\n",
    "\n",
    "\n",
    "def render_output_message(output):\n",
    "    if function_type == \"chat\":\n",
    "        if len(output) != 1:\n",
    "            raise ValueError(f\"Output {output} must have exactly one content block\")\n",
    "        if output[0][\"type\"] != \"text\":\n",
    "            raise ValueError(f\"Output {output} must be a text block\")\n",
    "        return {\"role\": \"assistant\", \"content\": output[0][\"text\"]}\n",
    "    elif function_type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample) -> List[Dict[str, str]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    result = {\n",
    "        \"input\": {\"messages\": [], \"tools\": [], \"parallel_tool_calls\": True},\n",
    "        \"preferred_output\": [],\n",
    "        \"non_preferred_output\": [],\n",
    "    }\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "        else:\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message[\"content\"], message[\"role\"])\n",
    "        result[\"input\"][\"messages\"].append(\n",
    "            {\"role\": message[\"role\"], \"content\": rendered_message}\n",
    "        )\n",
    "\n",
    "    # Add the demonstration (preferred output)\n",
    "    preferred_output = json.loads(sample[\"preferred_output\"])\n",
    "    result[\"preferred_output\"].append(render_output_message(preferred_output))\n",
    "\n",
    "    # Add the inference output (non-preferred output)\n",
    "    non_preferred_output = json.loads(sample[\"non_preferred_output\"])\n",
    "    result[\"non_preferred_output\"].append(render_output_message(non_preferred_output))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "df[\"openai_messages\"] = df.apply(sample_to_openai_messages, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5db4d-420d-498c-99d2-83e3002fbece",
   "metadata": {},
   "source": [
    "Upload the prepared datasets to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b8ce4-4578-4f86-87d0-e955640b067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_openai(df, openai_client) -> str:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n",
    "        for item in df[\"openai_messages\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        print(f\"File persisted on path [{f.name}]\")\n",
    "\n",
    "        with open(f.name, \"rb\") as file:\n",
    "            file_object = openai_client.files.create(file=file, purpose=\"fine-tune\")\n",
    "\n",
    "        return file_object.id\n",
    "\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "dpo_fine_tuning_object_id = upload_dataset_to_openai(df, openai_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e272d04d-f417-458f-acad-cff04a5b0b38",
   "metadata": {},
   "source": [
    "Launch the fine-tuning job and wait for it to complete.\n",
    "\n",
    "NOTE : This step takes a while and you can monitor the progress and estimated completion time using OpenAI's fine-tuning [dashboard](https://platform.openai.com/finetune/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a188ee-be0b-4403-86b1-fb56d951740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
    "    training_file=dpo_fine_tuning_object_id,\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.2},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "        pprint(job_status.to_dict())\n",
    "        if job_status.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"The fine-tuning job has compeleted with result {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c5a943-987f-4458-970a-7bd3d6907e53",
   "metadata": {},
   "source": [
    "Once the fine-tuning job is complete, you can add the fine-tuned model to your config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad4a87-8adb-4b59-8451-e5eda3942f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = job_status.fine_tuned_model\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"openai\"],\n",
    "            \"providers\": {\"openai\": {\"type\": \"openai\", \"model_name\": fine_tuned_model}},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb6d0a-8160-4a3f-83c9-72dde3e1bec9",
   "metadata": {},
   "source": [
    "Finally, add a new variant to your function to use the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01bfe4c-8cd4-41b8-8e1e-f5ded5d2210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"model\": fine_tuned_model,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {fine_tuned_model: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907c28e-772e-48da-a453-8b4ef99e0175",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "You can change the weight to enable a gradual rollout of the new model.\n",
    "\n",
    "You might also add other parameters (e.g. max_tokens, temperature) to the variant section in the config file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
