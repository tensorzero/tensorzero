{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Prompt Engineering with DSPy\n",
    "\n",
    "This notebook shows how we can pull data from the TensorZero data model in ClickHouse and use it to optimize a prompt for a function using DSPy.\n",
    "Given that there are many strategies for prompt optimization in DSPy, we can use the same code skeleton to try a lot of different strategies.\n",
    "At a high level the notebook below does the following:\n",
    "\n",
    "1. Read the TensorZero configuration file and convert the function schemas of interest into a DSPy signature.\n",
    "2. Pull data from ClickHouse and convert it into a DSPy dataset.\n",
    "3. Run a prompt optimization loop using one of the teleprompting classes supported by DSPy.\n",
    "4. Parse the optimized prompt from the history and write it to a minijinja file.\n",
    "\n",
    "**Note:** DSPy does not model the chat completion interface commonly used by language models. So, we only support functions that have inputs into the user prompt, that only use text output, that are single-turn functions, and that have a flat JSON schema for input, i.e. functions that take a list of primitive types as input into the user schema and output text or a flat JSON object.\n",
    "\n",
    "To get started:\n",
    "\n",
    "- Set the `CLICKHOUSE_NATIVE_URL` environment variable. It should correspond to the native protocol port (i.e. not the HTTP port) of your ClickHouse instance. This is typically port 9000.\n",
    "- Set the `OPENAI_API_KEY` environment variable.\n",
    "- Update the following parameters to those that apply to your use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../examples/haiku_hidden_preferences/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"write_haiku\"\n",
    "BASE_VARIANT_NAME = \"initial_prompt_gpt4o_mini\"\n",
    "DEV_FRACTION = 0.1\n",
    "TEST_FRACTION = 0.1\n",
    "\n",
    "\n",
    "METRIC_NAME = \"haiku_score\"\n",
    "MAX_SAMPLES = 1000\n",
    "FLOAT_METRIC_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import dsp\n",
    "import dspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "from clickhouse_driver import Client\n",
    "from dspy.datasets.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can swap the client below for any of the ones supported [here](https://dspy-docs.vercel.app/api/category/language-model-api-clients) in case you want DSPy to use a different language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o-mini-2024-07-18\"\n",
    "lm_client = dspy.OpenAI(model=model_name)\n",
    "dspy.configure(lm=lm_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the function configuration for the function we are optimizing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_config = config[\"functions\"][FUNCTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_flat_schema(schema: dict):\n",
    "    \"\"\"Check if a JSON schema (given as a dict) is flat.\"\"\"\n",
    "    if not isinstance(schema, dict):\n",
    "        return False\n",
    "\n",
    "    if \"type\" not in schema or schema[\"type\"] != \"object\":\n",
    "        return False\n",
    "\n",
    "    if \"properties\" not in schema:\n",
    "        return True\n",
    "\n",
    "    for prop in schema[\"properties\"].values():\n",
    "        if prop.get(\"type\") in [\"object\", \"array\"]:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_json_schema(schema: dict) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Preprocess a flat JSON schema to create a mapping of field names to their types.\n",
    "\n",
    "    Args:\n",
    "    user_schema (dict): A flat JSON schema.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, str]: A dictionary mapping field names to their types (number, string, bool, or integer).\n",
    "    \"\"\"\n",
    "    assert is_flat_schema(schema), f\"JSON schema is not flat: {schema}\"\n",
    "    result = {}\n",
    "    properties = schema.get(\"properties\", {})\n",
    "\n",
    "    for field_name, field_info in properties.items():\n",
    "        field_type = field_info.get(\"type\", \"\")\n",
    "        if field_type == \"number\":\n",
    "            result[field_name] = \"number\"\n",
    "        elif field_type == \"string\":\n",
    "            result[field_name] = \"string\"\n",
    "        elif field_type == \"boolean\":\n",
    "            result[field_name] = \"bool\"\n",
    "        elif field_type == \"integer\":\n",
    "            result[field_name] = \"integer\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonschema_type_to_python_type(field_type: str) -> str:\n",
    "    if field_type == \"number\":\n",
    "        return \"float\"\n",
    "    elif field_type == \"string\":\n",
    "        return \"str\"\n",
    "    elif field_type == \"boolean\":\n",
    "        return \"bool\"\n",
    "    elif field_type == \"integer\":\n",
    "        return \"int\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported field type: {field_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_config_to_dspy_signature(function_name: str, function_config: dict):\n",
    "    assert (\n",
    "        \"system_schema\" not in function_config\n",
    "    ), \"System schema not supported by DSPy recipe\"\n",
    "    assert (\n",
    "        \"assistants_schema\" not in function_config\n",
    "    ), \"Assistant schema not supported by DSPy recipe\"\n",
    "    assert \"user_schema\" in function_config, \"User schema not found in function config\"\n",
    "    user_schema_path = config_path.parent / function_config[\"user_schema\"]\n",
    "    with user_schema_path.open(\"r\") as f:\n",
    "        user_schema = preprocess_json_schema(json.load(f))\n",
    "    output_schema_path = function_config.get(\"output_schema\", None)\n",
    "    if output_schema_path:\n",
    "        output_schema_path = config_path.parent / output_schema_path\n",
    "        with output_schema_path.open(\"r\") as f:\n",
    "            output_schema = preprocess_json_schema(json.load(f))\n",
    "    else:\n",
    "        output_schema = None\n",
    "    input_signature = \"\"\n",
    "    for field_name, field_type in user_schema.items():\n",
    "        input_signature += f\"{field_name}:{jsonschema_type_to_python_type(field_type)},\"\n",
    "    input_signature = input_signature[:-1]\n",
    "    ## we don't need to trim the trailing comma because DSPy will handle it correctly (ignore it)\n",
    "    if output_schema:\n",
    "        output_signature = \"\"\n",
    "        for field_name, field_type in output_schema.items():\n",
    "            output_signature += (\n",
    "                f\"{field_name}:{jsonschema_type_to_python_type(field_type)},\"\n",
    "            )\n",
    "        # we don't need to trim the trailing comma because DSPy will handle it correctly (ignore it)\n",
    "    else:\n",
    "        output_signature = \"output\"\n",
    "    string_signature = f\"{input_signature} -> {output_signature}\"\n",
    "    return dspy.make_signature(string_signature, signature_name=function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_signature = function_config_to_dspy_signature(FUNCTION_NAME, function_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the database name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"clickhouse\" in config and \"database\" in config[\"clickhouse\"]:\n",
    "    database_name = config[\"clickhouse\"][\"database\"]\n",
    "else:\n",
    "    database_name = \"tensorzero\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    \"CLICKHOUSE_NATIVE_URL\" in os.environ\n",
    "), \"CLICKHOUSE_NATIVE_URL environment variable not set\"\n",
    "\n",
    "url_with_database = urljoin(os.environ[\"CLICKHOUSE_NATIVE_URL\"], database_name)\n",
    "\n",
    "clickhouse_client = Client.from_url(url_with_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the metric configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"metrics\" in config, \"No `[metrics]` section found in config\"\n",
    "assert (\n",
    "    METRIC_NAME in config[\"metrics\"]\n",
    "), f\"No metric named `{METRIC_NAME}` found in config\"\n",
    "\n",
    "metric = config[\"metrics\"][METRIC_NAME]\n",
    "\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_table_name = {\n",
    "    \"float\": \"FloatMetricFeedback\",\n",
    "    \"boolean\": \"BooleanMetricFeedback\",\n",
    "}.get(metric[\"type\"])\n",
    "\n",
    "if feedback_table_name is None:\n",
    "    raise ValueError(f\"Unsupported metric type: {metric['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the dataset of example which were successful according to the metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"type\" in metric, \"Metric is missing the `type` field\"\n",
    "assert \"optimize\" in metric, \"Metric is missing the `optimize` field\"\n",
    "\n",
    "threshold = FLOAT_METRIC_THRESHOLD if metric[\"type\"] == \"float\" else 0.5\n",
    "comparison_operator = \">=\" if metric[\"optimize\"] == \"max\" else \"<=\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    i.episode_id,\n",
    "    f.value\n",
    "FROM \n",
    "    Inference i\n",
    "JOIN \n",
    "    BooleanMetricFeedback f ON i.id = f.target_id\n",
    "WHERE \n",
    "    i.function_name = %(function_name)s\n",
    "    AND f.value {comparison_operator} %(threshold)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"database_name\": database_name,\n",
    "    \"feedback_table_name\": feedback_table_name,\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"comparison_operator\": comparison_operator,\n",
    "    \"threshold\": threshold,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_dataframe(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dspy_compatible_inputs(input_raw: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Checks that the input of this Inference is in the correct format for DSPy.\n",
    "    Then returns the dictionary of inputs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_input = json.loads(input_raw)\n",
    "    except json.JSONDecodeError:\n",
    "        logger.warning(f\"Input is not valid JSON: {input_raw}\")\n",
    "        return None\n",
    "    messages = parsed_input.get(\"messages\", None)\n",
    "    if messages is None:\n",
    "        logger.warning(f\"Input contains no messages: {input_raw}\")\n",
    "        return None\n",
    "    if len(messages) != 1:\n",
    "        logger.warning(f\"Input contains more than one message: {input_raw}\")\n",
    "        return None\n",
    "    message = messages[0]\n",
    "    content = message.get(\"content\", None)\n",
    "    if content is None:\n",
    "        logger.warning(f\"Input contains no content: {input_raw}\")\n",
    "        return None\n",
    "    if len(content) != 1:\n",
    "        logger.warning(f\"Input must contain exactly one content item: {input_raw}\")\n",
    "        return None\n",
    "    content = content[0]\n",
    "    if content[\"type\"] != \"text\":\n",
    "        logger.warning(f\"Input contains non-text content: {input_raw}\")\n",
    "        return None\n",
    "    value = content.get(\"value\", None)\n",
    "    if value is None:\n",
    "        logger.warning(f\"Input contains no value: {input_raw}\")\n",
    "        return None\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the input column into a list of dicts and create a new DataFrame with parsed content\n",
    "parsed_inputs = df[\"input\"].apply(parse_dspy_compatible_inputs)\n",
    "\n",
    "# Filter out None values and create a list of dictionaries\n",
    "valid_inputs = [input_dict for input_dict in parsed_inputs if input_dict is not None]\n",
    "\n",
    "if valid_inputs:\n",
    "    # Create a new DataFrame from the list of dictionaries\n",
    "    parsed_df = pd.DataFrame(valid_inputs)\n",
    "\n",
    "    # Check for duplicate column names between df and parsed_df\n",
    "    df_columns = set(df.columns)\n",
    "    parsed_df_columns = set(parsed_df.columns)\n",
    "    duplicate_columns = df_columns.intersection(parsed_df_columns)\n",
    "    assert (\n",
    "        len(duplicate_columns) == 0\n",
    "    ), f\"Duplicate columns found in parsed_df: {duplicate_columns}\"\n",
    "\n",
    "    # Combine the original DataFrame with the new parsed DataFrame\n",
    "    result_df = pd.concat([df.reset_index(drop=True), parsed_df], axis=1)\n",
    "else:\n",
    "    print(\"No valid inputs were found after parsing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dspy_compatible_outputs_chat(output_raw: str) -> Optional[str]:\n",
    "    try:\n",
    "        parsed_output = json.loads(output_raw)\n",
    "    except json.JSONDecodeError:\n",
    "        logger.warning(f\"Output is not valid JSON: {output_raw}\")\n",
    "        return None\n",
    "    if len(parsed_output) != 1:\n",
    "        logger.warning(f\"Output contains more than one message: {output_raw}\")\n",
    "        return None\n",
    "    message = parsed_output[0]\n",
    "    if message[\"type\"] != \"text\":\n",
    "        logger.warning(f\"Output contains non-text content: {output_raw}\")\n",
    "        return None\n",
    "    value = message.get(\"text\", None)\n",
    "    if value is None:\n",
    "        logger.warning(f\"Output contains no value: {output_raw}\")\n",
    "        return None\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dspy_compatible_outputs_json(output_raw: str) -> Optional[Dict[str, str]]:\n",
    "    try:\n",
    "        parsed_output = json.loads(output_raw)\n",
    "    except json.JSONDecodeError:\n",
    "        logger.warning(f\"Output is not valid JSON: {output_raw}\")\n",
    "        return None\n",
    "    parsed_output = parsed_output.get(\"parsed\", None)\n",
    "    if parsed_output is None:\n",
    "        logger.warning(f\"Output contains no parsed content: {output_raw}\")\n",
    "        return None\n",
    "    return parsed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dspy_compatible_outputs(\n",
    "    output_raw: str, function_config: dict\n",
    ") -> Optional[Dict[str, str]]:\n",
    "    if function_config[\"type\"] == \"chat\":\n",
    "        chat_output = parse_dspy_compatible_outputs_chat(output_raw)\n",
    "        if chat_output is None:\n",
    "            return None\n",
    "        return {\"output\": chat_output}\n",
    "    elif function_config[\"type\"] == \"json\":\n",
    "        return parse_dspy_compatible_outputs_json(output_raw)\n",
    "    else:\n",
    "        logger.warning(f\"Unknown function type: {function_config['type']}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "parse_outputs = partial(parse_dspy_compatible_outputs, function_config=function_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the output column and create a new DataFrame with parsed content\n",
    "parsed_outputs = df[\"output\"].apply(parse_outputs)\n",
    "\n",
    "# Filter out None values and create a list of dictionaries\n",
    "valid_outputs = [\n",
    "    output_dict for output_dict in parsed_outputs if output_dict is not None\n",
    "]\n",
    "\n",
    "if valid_outputs:\n",
    "    # Create a new DataFrame from the list of dictionaries\n",
    "    parsed_output_df = pd.DataFrame(valid_outputs)\n",
    "\n",
    "    # Combine the result_df (which already has parsed inputs) with the new parsed outputs\n",
    "    # Overwrite columns in result_df with the same names from parsed_output_df\n",
    "    result_df.update(parsed_output_df)\n",
    "    result_df = pd.concat(\n",
    "        [\n",
    "            result_df,\n",
    "            parsed_output_df[\n",
    "                [\n",
    "                    col\n",
    "                    for col in parsed_output_df.columns\n",
    "                    if col not in result_df.columns\n",
    "                ]\n",
    "            ],\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    print(\"Resulting DataFrame:\")\n",
    "    print(result_df.head())\n",
    "else:\n",
    "    print(\"No valid inputs were found after parsing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorZeroDSPyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        dev_fraction: float = DEV_FRACTION,\n",
    "        test_fraction: float = TEST_FRACTION,\n",
    "    ):\n",
    "        # Ensure unique episode_ids in each set\n",
    "        unique_episodes = df[\"episode_id\"].unique()\n",
    "        np.random.shuffle(unique_episodes)\n",
    "\n",
    "        # Calculate the number of episodes for each set\n",
    "        total_episodes = len(unique_episodes)\n",
    "        dev_size = int(total_episodes * dev_fraction)\n",
    "        test_size = int(total_episodes * test_fraction)\n",
    "        train_size = total_episodes - dev_size - test_size\n",
    "\n",
    "        # Split episode_ids\n",
    "        train_episodes = unique_episodes[:train_size]\n",
    "        dev_episodes = unique_episodes[train_size : train_size + dev_size]\n",
    "        test_episodes = unique_episodes[train_size + dev_size :]\n",
    "\n",
    "        # Create masks for each set\n",
    "        train_mask = df[\"episode_id\"].isin(train_episodes)\n",
    "        dev_mask = df[\"episode_id\"].isin(dev_episodes)\n",
    "        test_mask = df[\"episode_id\"].isin(test_episodes)\n",
    "\n",
    "        # Split the DataFrame\n",
    "        self._train = df[train_mask].to_dict(orient=\"records\")\n",
    "        self._dev = df[dev_mask].to_dict(orient=\"records\")\n",
    "        self._test = df[test_mask].to_dict(orient=\"records\")\n",
    "        self.train_size = len(self._train)\n",
    "        self.dev_size = len(self._dev)\n",
    "        self.test_size = len(self._test)\n",
    "        super().__init__(\n",
    "            train_seed=0,\n",
    "            train_size=self.train_size,\n",
    "            eval_seed=0,\n",
    "            dev_size=self.dev_size,\n",
    "            test_size=self.test_size,\n",
    "        )\n",
    "\n",
    "        print(f\"Train set: {len(self._train)} samples\")\n",
    "        print(f\"Dev set: {len(self._dev)} samples\")\n",
    "        print(f\"Test set: {len(self._test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorZeroDSPyDataset(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy_function = dspy.Predict(function_signature)\n",
    "\n",
    "\n",
    "class Predictor(dspy.Module):\n",
    "    def __init__(self, signature: dspy.Signature):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.Predict(signature)\n",
    "\n",
    "    def forward(self, **inputs):\n",
    "        return self.prog(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can swap the teleprompter with any of the teleprompting classes supported by DSPy [here](https://dspy-docs.vercel.app/docs/building-blocks/optimizers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import LabeledFewShot\n",
    "\n",
    "teleprompter = LabeledFewShot(k=5)\n",
    "optimized_function = teleprompter.compile(\n",
    "    Predictor(function_signature), trainset=dataset.train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run an example inference to get the prompt from the history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_function(**parsed_inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse out the prompt from the history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = lm_client.history[-1][\"prompt\"]\n",
    "prompt_no_args = \"\\n\\n\".join(prompt.split(\"\\n\\n\")[:-1]) + \"\\n\\n\"\n",
    "for element, element_info in function_signature.model_json_schema()[\n",
    "    \"properties\"\n",
    "].items():\n",
    "    if element_info[\"__dspy_field_type\"] == \"input\":\n",
    "        # Should render to \"topic: {{ topic }}\", for example (i.e. a minijinja template)\n",
    "        prompt_no_args += f\"{element_info['prefix']} {{{{{element}}}}}\"\n",
    "user_prompt = prompt_no_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the optimized user prompt to a minijinja file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"optimized_user_prompt.minijinja\", \"w\") as f:\n",
    "    f.write(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now produced a DSPy user prompt printed above. You can now add the minijnja file to your config tree and use it in a TensorZero variant.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
