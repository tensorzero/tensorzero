{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Prompt Engineering using MIPRO\n",
    "\n",
    "This notebook provides an automated approach to optimizing prompt engineering using the [Multi-prompt Instruction PRoposal Optimizer (MIPRO)](https://arxiv.org/abs/2406.11695v1).\n",
    "It is designed for TensorZero users who want to optimize their system prompts based on collected inference and feedback data. We currently only support demonstration feeback here, but we will add support for other feedback types in the future.\n",
    "\n",
    "By following this guide, you can systematically refine your prompts to improve model performance in specific tasks.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Generate candidate instructions and candidate demonstrations. \n",
    "    - Candidate instructions are generated using OpenAI's o1 model based on a system template and a schema (if provided). \n",
    "    - Each candidate demonstration is a set of few-shot examples sampled from the training set.\n",
    "2. For each optimization step, sample an instruction and demonstration pair and score it using an LLM judge.\n",
    "    - The judge is a TensorZero function that uses OpenAI's gpt-4o-mini model to score the quality of the instruction and demonstration pair.\n",
    "    - The judge provides a score on each prediction from your function on the evaluation set.\n",
    "    - The scores are aggregated to produce a single score for the instruction and demonstration pair.\n",
    "3. The optimizer uses a random search or tree-structured parzen estimator (TPE) to sample the next instruction and demonstration pair to evaluate.\n",
    "4. The process repeats for a fixed number of iterations.\n",
    "5. The highest scoring instruction and demonstration pair are formatted to produce an optimized system template.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### 1. Environment Setup\n",
    "\n",
    "Before running the optimization, ensure that:\n",
    "\n",
    "- The OPENAI_API_KEY environment variable is set.\n",
    "\n",
    "- The necessary dependencies for TensorZero and MIPRO are installed.\n",
    "\n",
    "- [You have spun up the docker container for the function you want to optimize the prompt for.](https://www.tensorzero.com/docs/gateway)\n",
    "\n",
    "### 2. Update Configuration Parameters\n",
    "\n",
    "To customize the optimization for your specific function, modify the following parameters.\n",
    "\n",
    "## Step 1: Define Function Configuration Parameters\n",
    "\n",
    "These parameters specify the TensorZero function you aim to optimize. \n",
    "In this example, we optimize the system prompt for the Named Entity Recognition (NER) task. Adjust these values based on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuation arguments for the function you want to optimize the prompt for\n",
    "CONFIG_DIR = \"../../examples/data-extraction-ner/config\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"\n",
    "\n",
    "# The name of the variant for the search template\n",
    "GENERATE_ANSWER_VARIANT_NAME = \"gpt_4o_mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candidate instructions and demonstrations are passed as arguments to the `generate_answer` system template.\n",
    "If your function uses a different model than GPT-4o-mini, you can create a new variant with your desired model in `config/functions/tensorzero.toml` and update the `GENERATE_ANSWER_VARIANT_NAME` appropriately.\n",
    "We currently have support for \"chat\" and \"json\" function types.\n",
    "If you are optimizing a chat function using Anthropic's Claude 3 Haiku, you can add a new variant with the following configuration:\n",
    "\n",
    "```toml\n",
    "[functions.generate_answer_chat.variants.claude_3_haiku]\n",
    "type = \"chat_completion\"\n",
    "weight = 1.0\n",
    "model = \"anthropic::claude-3-haiku-20240307\"\n",
    "retries = { num_retries = 3, max_delay_s = 10 }\n",
    "system_template = \"functions/generate_answer_chat/search_template/system_template.minijinja\"\n",
    "user_template = \"functions/generate_answer_chat/search_template/user_template.minijinja\"\n",
    "```\n",
    "\n",
    "and change the `GENERATE_ANSWER_VARIANT_NAME` to `claude_3_haiku`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure the LLM Judge for Metric Optimization\n",
    "\n",
    "To guide the optimizer in evaluating prompt effectiveness, specify the task description and optimization metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the task you are optimizing the prompt for to be used by the optimizer judge\n",
    "TASK_DESCRIPTION = \"The task is to extract named entities from the input text.\"\n",
    "\n",
    "# Metric definition for scoring generated prompts\n",
    "METRIC_PROPERTIES = \"The metric is the Jaccard similarity between the predicted and ground truth entities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Optimization Parameters\n",
    "\n",
    "These settings control how the optimizer selects and evaluates candidate prompts and demonstrations.\n",
    "\n",
    "You may want to experiment with different values of the following key parameters to find the best configuration for your use case.\n",
    "Increasing their values may improve the quality of the optimized prompt, but will also increase the cost of the optimization.\n",
    "- The `NUM_CANDIDATE_INSTRUCTIONS` and `NUM_CANDIDATE_DEMONSTRATIONS` parameters control the size of the search space.\n",
    "- The `MAX_ITERATIONS` parameter controls the number of search steps taken by the optimization algorithm for evaluating instruction-demonstration pairs.\n",
    "- The `MAX_DEMONSTRATIONS` parameter controls the number of few-shot examples included in each candidate demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of candidate instructions to generate and search over\n",
    "NUM_CANDIDATE_INSTRUCTIONS = 10\n",
    "\n",
    "# Number of candidate demonstrations to sample and search over\n",
    "NUM_CANDIDATE_DEMONSTRATIONS = 10\n",
    "\n",
    "# Maximum number of demonstrations in each example\n",
    "MAX_DEMONSTRATIONS = 10\n",
    "\n",
    "# Maximum number of search steps taken by the optimization algorithm for evaluating instruction-demonstration pairs\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "# Set optimization direction ('maximize' or 'minimize') based on the metric properties you described above.\n",
    "OPTIMIZER_DIRECTION = \"maximize\"\n",
    "\n",
    "# Fraction of the dataset used by the judge to score the quality of the generated prompt\n",
    "EVAL_FRACTION = 0.2\n",
    "\n",
    "# Limit on the number of samples for demonstration selection\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from minijinja import Environment\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorzero import (\n",
    "    AsyncTensorZeroGateway,\n",
    "    ChatInferenceResponse,\n",
    "    InferenceResponse,\n",
    "    JsonInferenceResponse,\n",
    "    RawText,\n",
    "    Text,\n",
    ")\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from utils import generate_answer, get_instructions, judge_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORZERO_GATEWAY_URL = \"http://localhost:3001\"\n",
    "MAX_CONCURRENT_REQUESTS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = AsyncTensorZeroGateway(TENSORZERO_GATEWAY_URL, timeout=30)\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Load the TensorZero configuration file for the function you want to optimize the prompt for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_DIR) / \"tensorzero.toml\"\n",
    "\n",
    "assert config_path.exists(), f\"{config_path} does not exist\"\n",
    "assert config_path.is_file(), f\"{config_path} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for prompt optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the output schema if the function is a JSON function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if function_type == \"json\":\n",
    "    user_output_schema = json.load(\n",
    "        open(\n",
    "            Path(CONFIG_DIR) / config[\"functions\"][FUNCTION_NAME][\"output_schema\"], \"r\"\n",
    "        )\n",
    "    )\n",
    "    GENERATE_ANSWER_FUNCTION_NAME = \"generate_answer_json\"\n",
    "else:\n",
    "    GENERATE_ANSWER_FUNCTION_NAME = \"generate_answer_chat\"\n",
    "    user_output_schema = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the inference table name based on the function type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    function_type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences and demonstration feedback from ClickHouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    f.value,\n",
    "    i.episode_id\n",
    "FROM \n",
    "    {inference_table_name} i\n",
    "JOIN \n",
    "    (SELECT\n",
    "        inference_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY inference_id ORDER BY timestamp DESC) as rn\n",
    "    FROM \n",
    "        DemonstrationFeedback\n",
    "    ) f ON i.id = f.inference_id AND f.rn = 1\n",
    "WHERE \n",
    "    i.function_name = %(function_name)s\n",
    "    AND i.variant_name = %(variant_name)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "    \"variant_name\": TEMPLATE_VARIANT_NAME,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the messages in the input and demonstration columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(content: List[Dict[str, Any]], role: str) -> str:\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "\n",
    "    if len(content) != 1:\n",
    "        raise ValueError(f\"Message must have exactly one content block: {content}\")\n",
    "\n",
    "    if content[0][\"type\"] != \"text\":\n",
    "        raise ValueError(f\"Content block must be of type text: {content}\")\n",
    "\n",
    "    content = content[0][\"value\"]\n",
    "\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    else:\n",
    "        return env.render_template(role, **content)\n",
    "\n",
    "\n",
    "def format_input(sample):\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "    rendered_message = \"\"\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message += render_message(message[\"content\"], message[\"role\"])\n",
    "    return rendered_message\n",
    "\n",
    "\n",
    "def format_output(sample):\n",
    "    output = json.loads(sample[\"value\"])\n",
    "    if function_type == \"chat\":\n",
    "        if len(output) != 1:\n",
    "            raise ValueError(f\"Output {output} must have exactly one content block.\")\n",
    "\n",
    "        if output[0][\"type\"] != \"text\":\n",
    "            raise ValueError(f\"Output {output} must be a text block.\")\n",
    "        return output[0][\"text\"]\n",
    "    elif function_type == \"json\":\n",
    "        return output[\"raw\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "\n",
    "def format_system_args(sample):\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "    if \"system_args\" in function_input:\n",
    "        return function_input[\"system_args\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "df[\"input_str\"] = df.apply(format_input, axis=1)\n",
    "df[\"value_str\"] = df.apply(format_output, axis=1)\n",
    "df[\"system_args\"] = df.apply(format_system_args, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and evaluation sets.\n",
    "The training set is used to generate candidate demonstrations.\n",
    "The evaluation set is used by the judge to score the quality of the generated prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = df[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - EVAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = df[df[\"episode_id\"].isin(train_episode_ids)]\n",
    "eval_df = df[df[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Evaluation set size: {len(eval_df)}\")\n",
    "print(f\"Actual evaluation fraction: {len(eval_df) / len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidate Instructions\n",
    "Given the function's system template as an example, generate a set of candidate instructions to optimize the prompt over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(CONFIG_DIR) / variant[\"system_template\"], \"r\", encoding=\"utf-8\") as file:\n",
    "    example_instructions = file.read()\n",
    "\n",
    "if \"system_schema\" in config[\"functions\"][FUNCTION_NAME]:\n",
    "    system_schema_path = (\n",
    "        Path(CONFIG_DIR) / config[\"functions\"][FUNCTION_NAME][\"system_schema\"]\n",
    "    )\n",
    "    with open(system_schema_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        example_schema = f.read()\n",
    "else:\n",
    "    example_schema = None\n",
    "\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        get_instructions(\n",
    "            client=tensorzero_client,\n",
    "            example_instructions=example_instructions,\n",
    "            example_schema=example_schema,\n",
    "            semaphore=semaphore,\n",
    "        )\n",
    "        for _ in range(NUM_CANDIDATE_INSTRUCTIONS)\n",
    "    ]\n",
    ")\n",
    "\n",
    "candidate_instructions = [example_instructions]\n",
    "for response in responses:\n",
    "    if response is None:\n",
    "        continue\n",
    "    candidate_instructions.append(response.output.parsed[\"instructions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Candidate Demonstrations\n",
    "Given the training set, generate a set of candidate demonstrations to optimize the prompt over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demonstrations(\n",
    "    df: pd.DataFrame, input_col: str, output_col: str, seed: int = 42\n",
    ") -> str:\n",
    "    # Perform a bootstrap sample (with replacement) of the entire DataFrame.\n",
    "    sample = df.sample(n=MAX_DEMONSTRATIONS, replace=False, random_state=seed)\n",
    "    # Remove duplicate rows that may have been sampled multiple times.\n",
    "    # unique_sample = bootstrap_sample.drop_duplicates(subset=['episode_id'])[:MAX_DEMONSTRATIONS]\n",
    "    demonstrations = \"\"\n",
    "    for _, row in sample.iterrows():\n",
    "        demonstrations += f\"Input: {row[input_col]}\\nOutput: {row[output_col]}\\n\\n\"\n",
    "\n",
    "    return demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_demonstrations = [\n",
    "    generate_demonstrations(\n",
    "        df=train_df, input_col=\"input_str\", output_col=\"value_str\", seed=seed\n",
    "    )\n",
    "    for seed in range(NUM_CANDIDATE_DEMONSTRATIONS)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the Prompt\n",
    "\n",
    "### Define the optimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Online Statistics ---\n",
    "num_instructions = len(candidate_instructions)\n",
    "num_demonstrations = len(candidate_demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response: InferenceResponse) -> str:\n",
    "    if response is None:\n",
    "        return \"\"\n",
    "    if isinstance(response, JsonInferenceResponse):\n",
    "        return str(response.output.parsed)\n",
    "    elif isinstance(response, ChatInferenceResponse):\n",
    "        content = response.content\n",
    "        assert len(content) == 1  # TODO: Handle multiple content blocks\n",
    "        if isinstance(content[0], Text):\n",
    "            return content[0].text\n",
    "        elif isinstance(content[0], RawText):\n",
    "            return content[0].value\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {type(content[0])}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported response type: {type(response)}\")\n",
    "\n",
    "\n",
    "async def objective(trial: optuna.Trial):\n",
    "    # Sample an instruction and a demonstration set.\n",
    "    instruction_index = trial.suggest_categorical(\n",
    "        \"instruction_index\", range(num_instructions)\n",
    "    )\n",
    "    demonstration_index = trial.suggest_categorical(\n",
    "        \"demonstration_index\", range(num_demonstrations)\n",
    "    )\n",
    "    # Asynchronously generate answers for each query in the evaluation set.\n",
    "    responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            generate_answer(\n",
    "                client=tensorzero_client,\n",
    "                function_name=GENERATE_ANSWER_FUNCTION_NAME,\n",
    "                variant_name=GENERATE_ANSWER_VARIANT_NAME,\n",
    "                instruction=candidate_instructions[instruction_index],\n",
    "                demonstrations=candidate_demonstrations[demonstration_index],\n",
    "                query=query,\n",
    "                output_schema=user_output_schema,\n",
    "                system_args=system_args,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for query, system_args in zip(eval_df[\"input_str\"], eval_df[\"system_args\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Score the responses using the judge.\n",
    "    judge_responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            judge_answer(\n",
    "                client=tensorzero_client,\n",
    "                task_description=TASK_DESCRIPTION,\n",
    "                metric_properties=METRIC_PROPERTIES,\n",
    "                prediction=format_response(response) if response is not None else \"\",\n",
    "                truth=str(ground_truth),\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for response, ground_truth in zip(responses, eval_df[\"value_str\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Aggregate the scores.\n",
    "    scores = []\n",
    "    for response in judge_responses:\n",
    "        if response is not None:\n",
    "            scores.append(response.output.parsed[\"score\"])\n",
    "    # Return the mean score.\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "We start by sampling a random instruction and demonstration at each iteration in the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_random = optuna.create_study(\n",
    "    sampler=optuna.samplers.RandomSampler(seed=SEED), direction=OPTIMIZER_DIRECTION\n",
    ")\n",
    "\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    trial = study_random.ask()\n",
    "\n",
    "    value = await objective(trial)\n",
    "    print(f\"Iteration {iteration + 1}: {value}\")\n",
    "\n",
    "    frozen_trial = study_random.tell(trial, value)\n",
    "    study_random._log_completed_trial(frozen_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-structured Parzen Estimator\n",
    "Following the MIPRO paper, we use a tree-structured parzen estimator (TPE) to sample the next instruction and demonstration pair to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_tpe = optuna.create_study(\n",
    "    sampler=TPESampler(seed=SEED), direction=OPTIMIZER_DIRECTION\n",
    ")\n",
    "\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    trial = study_tpe.ask()\n",
    "\n",
    "    value = await objective(trial)\n",
    "    print(f\"Iteration {iteration + 1}: {value}\")\n",
    "\n",
    "    frozen_trial = study_tpe.tell(trial, value)\n",
    "    study_tpe._log_completed_trial(frozen_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an estimate of the best instruction and demonstration pair.\n",
    "We can now generate an optimized system template.\n",
    "You can save this template to a `system_template.minijinja` file and create a new variant of your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "templates_optimized = {}\n",
    "\n",
    "system_template_path = Path(\n",
    "    \"config/functions/generate_answer_json/search_template/system_template.minijinja\"\n",
    ")\n",
    "with system_template_path.open(\"r\") as f:\n",
    "    templates_optimized[\"system\"] = f.read()\n",
    "\n",
    "env_optimized = Environment(templates=templates_optimized)\n",
    "\n",
    "optimized_system_template = env_optimized.render_template(\n",
    "    \"system\",\n",
    "    instructions=candidate_instructions[study_tpe.best_params[\"instruction_index\"]],\n",
    "    demonstrations=candidate_demonstrations[\n",
    "        study_tpe.best_params[\"demonstration_index\"]\n",
    "    ],\n",
    ")\n",
    "pprint(optimized_system_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "For demonstration purposes, we evaluate the optimized prompts on a validation set using the Jaccard similarity metric rather than the judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VAL_DATAPOINTS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, List[str]]) -> List[str]:\n",
    "    res = []\n",
    "    for k, v in d.items():\n",
    "        assert isinstance(v, list)\n",
    "        for elt in v:\n",
    "            res.append(f\"__{k.upper()}__::{elt}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def compute_jaccard_similarity(\n",
    "    predicted: Dict[str, List[str]], ground_truth: Dict[str, List[str]]\n",
    ") -> float:\n",
    "    target_entities = flatten_dict(ground_truth)\n",
    "    pred_entities = flatten_dict(predicted)\n",
    "    target_count = Counter(target_entities)\n",
    "    pred_count = Counter(pred_entities)\n",
    "    num = 0\n",
    "    den = 0\n",
    "    all_keys = set(target_entities).union(set(pred_entities))\n",
    "    for key in all_keys:\n",
    "        num += min(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "        den += max(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "    if den == 0:\n",
    "        return 1\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def evaluate_response(\n",
    "    response: Optional[InferenceResponse], ground_truth_data: Dict[str, List[str]]\n",
    "):\n",
    "    predicted = response.output.parsed if response else None\n",
    "\n",
    "    # `predicted` is None if the model failed to return a valid JSON that complies with the output schema\n",
    "    valid_output = predicted is not None\n",
    "\n",
    "    jaccard_similarity = (\n",
    "        compute_jaccard_similarity(predicted, ground_truth_data) if predicted else 0\n",
    "    )\n",
    "\n",
    "    return valid_output, jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_val_dataset(path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(path)\n",
    "    df.output = df.output.apply(json.loads)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    val_df = df[df[\"split\"] == 1]\n",
    "\n",
    "    # Shuffle the splits\n",
    "    val_df = val_df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    val_df = val_df.iloc[:NUM_VAL_DATAPOINTS]\n",
    "\n",
    "    return val_df\n",
    "\n",
    "\n",
    "val_df = load_val_dataset(\"../../examples/data-extraction-ner/data/conllpp.csv\")\n",
    "\n",
    "print(f\"Validation data shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = {\"Random\": study_random, \"TPE\": study_tpe}\n",
    "scores = {}\n",
    "for study_name, study in studies.items():\n",
    "    responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            generate_answer(\n",
    "                client=tensorzero_client,\n",
    "                function_name=GENERATE_ANSWER_FUNCTION_NAME,\n",
    "                variant_name=GENERATE_ANSWER_VARIANT_NAME,\n",
    "                instruction=candidate_instructions[\n",
    "                    study.best_params[\"instruction_index\"]\n",
    "                ],\n",
    "                demonstrations=candidate_demonstrations[\n",
    "                    study.best_params[\"demonstration_index\"]\n",
    "                ],\n",
    "                output_schema=user_output_schema,\n",
    "                query=query,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for query in val_df[\"input\"]\n",
    "        ]\n",
    "    )\n",
    "    valid_output_scores = []\n",
    "    jaccard_similarity_scores = []\n",
    "    for response, ground_truth in zip(responses, val_df[\"output\"]):\n",
    "        valid_output, jaccard_similarity = evaluate_response(response, ground_truth)\n",
    "        valid_output_scores.append(valid_output)\n",
    "        jaccard_similarity_scores.append(jaccard_similarity)\n",
    "\n",
    "    # Compute the score for this iteration.\n",
    "    # (Be sure to handle the case where the denominator might be zero.)\n",
    "    score = np.sum(jaccard_similarity_scores) / (np.sum(valid_output_scores) + 1e-6)\n",
    "    scores[study_name] = score\n",
    "    print(f\"{study_name} score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
