{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Automated Prompt Engineering using MIPRO\n",
    "\n",
    "This notebook provides an automated approach to optimizing prompt engineering using the [Multi-prompt Instruction PRoposal Optimizer (MIPRO)](https://arxiv.org/abs/2406.11695v1).\n",
    "It is designed for TensorZero users who want to optimize their system prompts based on collected inference and feedback data. As such, we currently only support prompt optimization for applications with a single system prompt.\n",
    "\n",
    "Support for applications with multiple system prompts is in the pipeline. If this use case interests you, please see our our [LLM Gym Example](https://github.com/tensorzero/llmgym/tree/main/examples/mipro) for a full implementation.\n",
    "\n",
    "By following this guide, you can systematically refine your prompts to improve model performance in specific tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "The optimization process involves the following steps:\n",
    "\n",
    "1. **Generate candidate instructions and demonstrations**\n",
    "    - Candidate instructions are generated using OpenAI's o1 model based on a system template and an optional schema.\n",
    "        - This is configurable in the `config/tensorzero.toml` file if you want to use a different model.\n",
    "    - Candidate demonstrations are sets of few-shot examples sampled from the training dataset.\n",
    "2. **Evaluate Instruction-Demonstration Pairs**\n",
    "    - Sample an instruction and demonstration pair and score it using a Large Language Model (LLM) judge.\n",
    "    - The judge (a TensorZero function utilizing OpenAI's GPT-4o-mini model) scores the quality of the instruction-demonstration pair.\n",
    "    - Scores are aggregated over the evaluation set to produce a final evaluation score.\n",
    "3. **Optimization via Search Algorithms**\n",
    "    - Utilize a random search or a Tree-structured Parzen Estimator (TPE) to determine the next instruction and demonstration pair for evaluation.\n",
    "4. **Iterate the Optimization Process**\n",
    "    - Repeat the optimization process for a fixed number of iterations.\n",
    "5. **Select the Best Performing Prompts**\n",
    "    - The instruction and demonstration pairs corresponding to the highest-performing prompts are formatted to yield optimized system templates.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### 1. Environment Setup\n",
    "\n",
    "Before running the optimization, ensure that:\n",
    "\n",
    "- The `OPENAI_API_KEY` environment variable is set.\n",
    "\n",
    "- Required dependencies for TensorZero and MIPRO are installed.\n",
    "\n",
    "- The clickhouse client for the database containing the demonstration feeback is running and visible through `TENSORZERO_CLICKHOUSE_URL`\n",
    "    - For example, [by running the docker container for the function you want to optimize the prompt for.](https://www.tensorzero.com/docs/gateway)\n",
    "\n",
    "### 2. Configuration Parameters\n",
    "\n",
    "To tailor the optimization to your function, update the following parameters accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Define Function Configuration Parameters\n",
    "\n",
    "Specify the TensorZero function you want to optimize. The example below optimizes the system prompt for Named Entity Recognition (NER):\n",
    "\n",
    "- **Function Configuration Directory:** Location of the functionâ€™s configuration files.\n",
    "\n",
    "- **Function Name:** The TensorZero function being optimized.\n",
    "\n",
    "- **Model Variant:** The specific function variant to use as an example for the system template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuation arguments for the function you want to optimize the prompt for\n",
    "CONFIG_DIR = \"../../examples/data-extraction-ner/config\"\n",
    "\n",
    "# The name of the function you want to optimize the prompt for\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Step 2: Configure the LLM Judge for Metric Optimization\n",
    "\n",
    "The LLM judge guides the optimization process by evaluating prompt effectiveness. You must define:\n",
    "\n",
    "- **Task Description:** A summary of the task being optimized.\n",
    "\n",
    "- **Optimization Metric:** The metric used for evaluating prompt effectiveness (e.g., Jaccard similarity between predicted and ground truth entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the task you are optimizing the prompt for to be used by the optimizer judge\n",
    "TASK_DESCRIPTION = \"The task is to extract named entities from the input text.\"\n",
    "\n",
    "# Metric definition for scoring generated prompts\n",
    "METRIC_PROPERTIES = \"The metric is the Jaccard similarity between the predicted and ground truth entities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Step 3: Define Optimization Parameters\n",
    "\n",
    "The following parameters control the optimization process. Experimenting with different values can help refine results:\n",
    "\n",
    "- **Search Space**\n",
    "\n",
    "    - `NUM_CANDIDATE_INSTRUCTIONS`: Number of candidate instructions to generate.\n",
    "\n",
    "    - `NUM_CANDIDATE_DEMONSTRATIONS`: Number of candidate demonstrations to sample.\n",
    "\n",
    "- **Optimization Control**\n",
    "\n",
    "    - `MAX_ITERATIONS`: Number of optimization steps.\n",
    "\n",
    "    - `MAX_EXAMPLES_PER_DEMONSTRATION`: Maximum few-shot examples per demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of candidate instructions to generate and search over\n",
    "NUM_CANDIDATE_INSTRUCTIONS = 10\n",
    "\n",
    "# Number of candidate demonstrations to sample and search over\n",
    "NUM_CANDIDATE_DEMONSTRATIONS = 10\n",
    "\n",
    "# Maximum number of demonstrations in each candidate demonstration set\n",
    "MAX_EXAMPLES_PER_DEMONSTRATION = 10\n",
    "\n",
    "# Maximum number of search steps taken by the optimization algorithm for evaluating instruction-demonstration pairs\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "# Set optimization direction ('maximize' or 'minimize') based on the metric properties you described above.\n",
    "OPTIMIZER_DIRECTION = \"maximize\"\n",
    "\n",
    "# Fraction of the dataset used by the judge to score the quality of the generated prompt\n",
    "EVAL_FRACTION = 0.2\n",
    "\n",
    "# Limit on the number of samples for demonstration selection\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Evaluation Control**\n",
    "\n",
    "    - `EVAL_FRACTION`: Fraction of the dataset used for scoring generated prompts.\n",
    "\n",
    "    - `MAX_SAMPLES`: Limit on the number of demonstration samples.\n",
    "\n",
    "- **Reproducibility**\n",
    "\n",
    "    - `SEED`: Random seed for consistent results.\n",
    "\n",
    "---\n",
    "\n",
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from clickhouse_connect import get_client\n",
    "from minijinja import Environment\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorzero import (\n",
    "    AsyncTensorZeroGateway,\n",
    "    InferenceResponse,\n",
    "    JsonInferenceResponse,\n",
    "    RawText,\n",
    "    Text,\n",
    ")\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from utils.client_calls import candidate_inference, get_instructions, judge_answer\n",
    "from utils.configs.reader import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Initialize the MIPRO TensorZero Client\n",
    "\n",
    "This client is used to generate candidate instructions and score the quality of responses given the candidate instructions and demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_REQUESTS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mipro_client = await AsyncTensorZeroGateway.build_embedded(\n",
    "    config_file=\"config/tensorzero.toml\",\n",
    "    clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    ")\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Load Data\n",
    "Load the TensorZero configuration for the function you want to optimize the prompt for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = load_config(CONFIG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for prompt optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert FUNCTION_NAME in base_config.functions.keys(), (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in base_config.functions[FUNCTION_NAME].variants.keys(), (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "base_function = base_config.functions[FUNCTION_NAME]\n",
    "base_variant = deepcopy(base_function.variants[TEMPLATE_VARIANT_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the inference table name based on the function type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    base_function.type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {base_function.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences and demonstration feedback from ClickHouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use one of the metrics above, or choose `FILTER_METRIC_NAME = \"demonstration\"` to use ground truth demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_config.metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_METRIC_NAME = \"demonstration\"\n",
    "FILTER_METRIC_THRESHOLD = 0.9\n",
    "\n",
    "if (\n",
    "    FILTER_METRIC_NAME != \"demonstration\"\n",
    "):  # If no metric name is provided, use ground truth demonstrations\n",
    "    filter_metric = base_config.metrics[FILTER_METRIC_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    FILTER_METRIC_NAME == \"demonstration\"\n",
    "):  # Assume demonstration feedback is available and used.\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        i.input, \n",
    "        i.output, \n",
    "        f.value,\n",
    "        i.episode_id\n",
    "    FROM \n",
    "        {inference_table_name} i\n",
    "    JOIN \n",
    "        (SELECT\n",
    "            inference_id,\n",
    "            value,\n",
    "            ROW_NUMBER() OVER (PARTITION BY inference_id ORDER BY timestamp DESC) as rn\n",
    "        FROM \n",
    "            DemonstrationFeedback\n",
    "        ) f ON i.id = f.inference_id AND f.rn = 1\n",
    "    WHERE \n",
    "        i.function_name = %(function_name)s\n",
    "    LIMIT %(max_samples)s\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"function_name\": FUNCTION_NAME,\n",
    "        \"max_samples\": MAX_SAMPLES,\n",
    "    }\n",
    "else:\n",
    "    feedback_table_name = {\n",
    "        \"float\": \"FloatMetricFeedback\",\n",
    "        \"boolean\": \"BooleanMetricFeedback\",\n",
    "    }.get(filter_metric.type)\n",
    "\n",
    "    inference_join_key = {\n",
    "        \"episode\": \"episode_id\",\n",
    "        \"inference\": \"id\",\n",
    "    }.get(filter_metric.level)\n",
    "\n",
    "    if inference_join_key is None:\n",
    "        raise ValueError(f\"Unsupported metric level: {filter_metric.level}\")\n",
    "\n",
    "    threshold = FILTER_METRIC_THRESHOLD if filter_metric.type == \"float\" else 0.5\n",
    "    comparison_operator = \">=\" if filter_metric.optimize == \"maximize\" else \"<=\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        i.input, \n",
    "        i.output, \n",
    "        i.episode_id,\n",
    "        i.function_name,\n",
    "        f.value\n",
    "    FROM \n",
    "        {inference_table_name} i\n",
    "    JOIN \n",
    "        (SELECT\n",
    "            target_id,\n",
    "            value,\n",
    "            ROW_NUMBER() OVER (PARTITION BY target_id ORDER BY timestamp DESC) as rn\n",
    "        FROM \n",
    "            {feedback_table_name}\n",
    "        WHERE\n",
    "            metric_name = %(metric_name)s\n",
    "            AND value {comparison_operator} %(threshold)s\n",
    "        ) f ON i.{inference_join_key} = f.target_id and f.rn = 1\n",
    "    WHERE \n",
    "        i.function_name = %(function_name)s\n",
    "    LIMIT %(max_samples)s\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"function_name\": FUNCTION_NAME,\n",
    "        \"max_samples\": MAX_SAMPLES,\n",
    "        \"metric_name\": FILTER_METRIC_NAME,\n",
    "        \"threshold\": FILTER_METRIC_THRESHOLD,\n",
    "    }\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "if FILTER_METRIC_NAME != \"demonstration\":\n",
    "    df.value = df.output\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if base_variant.assistant_template is not None:\n",
    "    templates[\"assistant\"] = base_variant.assistant_template\n",
    "\n",
    "if base_variant.system_template is not None:\n",
    "    templates[\"system\"] = base_variant.system_template\n",
    "\n",
    "if base_variant.user_template is not None:\n",
    "    templates[\"user\"] = base_variant.user_template\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the messages in the input and demonstration columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(content: List[Dict[str, Any]], role: str) -> str:\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "\n",
    "    if len(content) != 1:\n",
    "        raise ValueError(f\"Message must have exactly one content block: {content}\")\n",
    "\n",
    "    if role == \"user\":\n",
    "        output = \"ENVIRONMENT:\\n\"\n",
    "    else:\n",
    "        output = \"AGENT:\\n\"\n",
    "\n",
    "    if content[0][\"type\"] == \"text\":\n",
    "        value = content[0][\"value\"]\n",
    "        if isinstance(value, str):\n",
    "            output += value\n",
    "        else:\n",
    "            value = env.render_template(role, **value)  # type: ignore\n",
    "            assert isinstance(value, str)\n",
    "            output += value\n",
    "    elif content[0][\"type\"] == \"tool_call\":\n",
    "        del content[0][\"id\"]\n",
    "        del content[0][\"type\"]\n",
    "        output += f\"Tool call: {json.dumps(content[0])}\"\n",
    "    elif content[0][\"type\"] == \"tool_result\":\n",
    "        output += f\"Tool result: {content[0]['result']}\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Content block must be of type text, tool_call, or tool_result: {content}\"\n",
    "        )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def format_input(sample):\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "    rendered_message = \"\"\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message += render_message(message[\"content\"], message[\"role\"])\n",
    "        rendered_message += \"\\n\"\n",
    "    return rendered_message\n",
    "\n",
    "\n",
    "def format_output(sample):\n",
    "    output = json.loads(sample[\"value\"])\n",
    "    if base_function.type == \"chat\":\n",
    "        if len(output) != 1:\n",
    "            raise ValueError(f\"Output {output} must have exactly one content block.\")\n",
    "        if output[0][\"type\"] == \"text\":\n",
    "            return output[0][\"text\"]\n",
    "        elif output[0][\"type\"] == \"tool_call\":\n",
    "            del output[0][\"raw_arguments\"]\n",
    "            del output[0][\"raw_name\"]\n",
    "            del output[0][\"type\"]\n",
    "            return f\"Tool call: {json.dumps(output[0])}\"\n",
    "        elif output[0][\"type\"] == \"tool_result\":\n",
    "            return json.dumps(output[0])\n",
    "        else:\n",
    "            raise ValueError(f\"Output {output} must be a text block.\")\n",
    "    elif base_function.type == \"json\":\n",
    "        return output[\"raw\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {base_function.type}\")\n",
    "\n",
    "\n",
    "def format_system_args(sample):\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "    if \"system\" in function_input:\n",
    "        return function_input[\"system\"]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "df[\"input_str\"] = df.apply(format_input, axis=1)\n",
    "df[\"value_str\"] = df.apply(format_output, axis=1)\n",
    "df[\"system_args\"] = df.apply(format_system_args, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and evaluation sets.\n",
    "The training set is used to generate candidate demonstrations.\n",
    "The evaluation set is used by the judge to score the quality of the generated prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = df[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - EVAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = df[df[\"episode_id\"].isin(train_episode_ids)]\n",
    "eval_df = df[df[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Evaluation set size: {len(eval_df)}\")\n",
    "print(f\"Actual evaluation fraction: {len(eval_df) / len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Generate Candidate Instructions\n",
    "Given the function's system template as an example, generate a set of candidate instructions to optimize the prompt over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_instructions = base_variant.system_template\n",
    "\n",
    "if base_function.system_schema is not None:\n",
    "    example_schema = base_function.system_schema.model_json_schema()\n",
    "else:\n",
    "    example_schema = None\n",
    "\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        get_instructions(\n",
    "            client=mipro_client,\n",
    "            example_instructions=example_instructions,\n",
    "            example_schema=example_schema,\n",
    "            semaphore=semaphore,\n",
    "        )\n",
    "        for _ in range(NUM_CANDIDATE_INSTRUCTIONS)\n",
    "    ]\n",
    ")\n",
    "\n",
    "candidate_instructions = [example_instructions]\n",
    "for response in responses:\n",
    "    if response is None:\n",
    "        continue\n",
    "    candidate_instructions.append(response.output.parsed[\"instructions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Generate Candidate Demonstrations\n",
    "Given the training set, generate a set of candidate demonstrations to optimize the prompt over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demonstrations(\n",
    "    df: pd.DataFrame,\n",
    "    max_examples_per_demonstration: int,\n",
    "    input_col: str,\n",
    "    output_col: str,\n",
    "    system_col: str,\n",
    "    seed: int = 42,\n",
    ") -> str:\n",
    "    sample = df.sample(\n",
    "        n=max_examples_per_demonstration, replace=False, random_state=seed\n",
    "    )\n",
    "    demonstrations = \"\"\n",
    "    demonstration_number = 1\n",
    "    for _, row in sample.iterrows():  # type: ignore\n",
    "        demonstrations += f\"DEMONSTRATION {demonstration_number}:\\n\"\n",
    "        demonstration_number += 1\n",
    "        if row[system_col] is not None and row[system_col] != \"\":\n",
    "            demonstrations += f\"SYSTEM:\\n{row[system_col]}\\n\"\n",
    "        demonstrations += f\"{row[input_col]}AGENT:\\n{row[output_col]}\\n\\n\"\n",
    "    return demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_demonstrations = [\n",
    "    generate_demonstrations(\n",
    "        df=train_df,\n",
    "        max_examples_per_demonstration=MAX_EXAMPLES_PER_DEMONSTRATION,\n",
    "        input_col=\"input_str\",\n",
    "        output_col=\"value_str\",\n",
    "        system_col=\"system_args\",\n",
    "        seed=seed,\n",
    "    )\n",
    "    for seed in range(NUM_CANDIDATE_DEMONSTRATIONS)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Optimize the Prompt\n",
    "\n",
    "### Define the optimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initialize Online Statistics ---\n",
    "num_instructions = len(candidate_instructions)\n",
    "num_demonstrations = len(candidate_demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_system_template(instructions: str, demonstrations: str) -> str:\n",
    "    return f\"Instructions:{instructions}\\n\\nDemonstrations:\\n\\n{demonstrations}\"\n",
    "\n",
    "\n",
    "def format_response(response: Optional[InferenceResponse]) -> str:\n",
    "    if response is None:\n",
    "        return \"\"\n",
    "    if isinstance(response, JsonInferenceResponse):\n",
    "        return str(response.output.parsed)\n",
    "    else:\n",
    "        content = response.content\n",
    "        assert len(content) == 1  # TODO: Handle multiple content blocks\n",
    "        if isinstance(content[0], Text):\n",
    "            return content[0].text\n",
    "        elif isinstance(content[0], RawText):\n",
    "            return content[0].value\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {type(content[0])}\")\n",
    "\n",
    "\n",
    "async def objective(trial: optuna.Trial):\n",
    "    # Sample an instruction and a demonstration set.\n",
    "    instruction_index = trial.suggest_categorical(\n",
    "        \"instruction_index\", range(num_instructions)\n",
    "    )\n",
    "    demonstration_index = trial.suggest_categorical(\n",
    "        \"demonstration_index\", range(num_demonstrations)\n",
    "    )\n",
    "    # format the candidate prompt\n",
    "    candidate_prompt = format_system_template(\n",
    "        candidate_instructions[instruction_index],\n",
    "        candidate_demonstrations[demonstration_index],\n",
    "    )\n",
    "    # create a new variant with the candidate prompt\n",
    "    candidate_variant_name = f\"{instruction_index}_{demonstration_index}\"\n",
    "    candidate_config = deepcopy(base_config)\n",
    "    candidate_config.functions[FUNCTION_NAME].variants[candidate_variant_name] = (\n",
    "        deepcopy(base_variant)\n",
    "    )\n",
    "    candidate_config.functions[FUNCTION_NAME].variants[\n",
    "        candidate_variant_name\n",
    "    ].system_template = candidate_prompt\n",
    "    candidate_config.functions[FUNCTION_NAME].variants[\n",
    "        candidate_variant_name\n",
    "    ].name = candidate_variant_name\n",
    "    # write the new config to a temporary directory\n",
    "    tmp_config_dir = candidate_config.write()\n",
    "    # build a new client with the new config\n",
    "    target_client = await AsyncTensorZeroGateway.build_embedded(\n",
    "        config_file=str(tmp_config_dir / \"tensorzero.toml\"),\n",
    "        clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    "    )\n",
    "    # Asynchronously generate answers for each query in the evaluation set.\n",
    "    responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            candidate_inference(\n",
    "                client=target_client,\n",
    "                function_name=FUNCTION_NAME,\n",
    "                input=json.loads(input_args),\n",
    "                variant_name=candidate_variant_name,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for input_args in eval_df[\"input\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Score the responses using the judge.\n",
    "    judge_responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            judge_answer(\n",
    "                client=mipro_client,\n",
    "                task_description=TASK_DESCRIPTION,\n",
    "                metric_properties=METRIC_PROPERTIES,\n",
    "                prediction=format_response(response) if response is not None else \"\",\n",
    "                ground_truth=str(ground_truth),\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for response, ground_truth in zip(responses, eval_df[\"value_str\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Aggregate the scores.\n",
    "    scores = []\n",
    "    for response in judge_responses:\n",
    "        if response is not None:\n",
    "            if response.output.parsed is not None:\n",
    "                scores.append(response.output.parsed[\"score\"])\n",
    "    # Return the mean score.\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "We start by sampling a random instruction and demonstration at each iteration in the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_random = optuna.create_study(\n",
    "    sampler=optuna.samplers.RandomSampler(seed=SEED), direction=OPTIMIZER_DIRECTION\n",
    ")\n",
    "\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    trial = study_random.ask()\n",
    "\n",
    "    value = await objective(trial)\n",
    "    print(f\"Iteration {iteration + 1}: {value}\")\n",
    "\n",
    "    frozen_trial = study_random.tell(trial, value)\n",
    "    study_random._log_completed_trial(frozen_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-structured Parzen Estimator\n",
    "Following the MIPRO paper, we use a tree-structured parzen estimator (TPE) to sample the next instruction and demonstration pair to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_tpe = optuna.create_study(\n",
    "    sampler=TPESampler(seed=SEED), direction=OPTIMIZER_DIRECTION\n",
    ")\n",
    "\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    trial = study_tpe.ask()\n",
    "\n",
    "    value = await objective(trial)\n",
    "    print(f\"Iteration {iteration + 1}: {value}\")\n",
    "\n",
    "    frozen_trial = study_tpe.tell(trial, value)\n",
    "    study_tpe._log_completed_trial(frozen_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Save the optimized candidate\n",
    "\n",
    "We now have an estimate of the best instruction and demonstration pair.\n",
    "We can now generate an optimized system template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_system_template = format_system_template(\n",
    "    instructions=candidate_instructions[study_tpe.best_params[\"instruction_index\"]],\n",
    "    demonstrations=candidate_demonstrations[\n",
    "        study_tpe.best_params[\"demonstration_index\"]\n",
    "    ],\n",
    ")\n",
    "print(optimized_system_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the optimized configuration file tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = None  # Set to a local path to save the optimized config\n",
    "\n",
    "optimized_variant_name = \"mipro_optimized\"\n",
    "optimized_config = deepcopy(base_config)\n",
    "optimized_config.functions[FUNCTION_NAME].variants[optimized_variant_name] = deepcopy(\n",
    "    base_variant\n",
    ")\n",
    "optimized_config.functions[FUNCTION_NAME].variants[\n",
    "    optimized_variant_name\n",
    "].system_template = optimized_system_template\n",
    "optimized_config.functions[FUNCTION_NAME].variants[\n",
    "    optimized_variant_name\n",
    "].name = optimized_variant_name\n",
    "# write the new config to a temporary directory\n",
    "optimized_config_dir = optimized_config.write(base_dir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Conclusion\n",
    "\n",
    "By following this notebook, you can systematically refine prompts for better performance. The optimized prompt can be saved and used in production by updating the function's system template configuration.\n",
    "\n",
    "Future updates will extend support to additional feedback types and we encourage you to explore different optimization strategies.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
