{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccdee8",
   "metadata": {},
   "source": [
    "# Automated Prompt Engineering using MIPRO\n",
    "\n",
    "This notebook provides an automated approach to optimizing prompt engineering using the [Multi-prompt Instruction PRoposal Optimizer (MIPRO)](https://arxiv.org/abs/2406.11695v1).\n",
    "It is designed for TensorZero users who want to optimize their system prompts based on collected inference and feedback data. As such, we currently only support prompt optimization for applications with a single system prompt.\n",
    "\n",
    "Support for applications with multiple system prompts is in the pipeline. If this use case interests you, please see our our [LLM Gym Example](https://github.com/tensorzero/llmgym/tree/main/examples/mipro) for a full implementation.\n",
    "\n",
    "By following this guide, you can systematically refine your prompts to improve model performance in specific tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28484198",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The optimization process involves the following steps:\n",
    "\n",
    "1. **Generate candidate instructions and demonstrations**\n",
    "    - Candidate instructions are generated using OpenAI's o1 model based on a system template and an optional schema.\n",
    "        - This is configurable in the `config/tensorzero.toml` file if you want to use a different model.\n",
    "    - Candidate demonstrations are sets of few-shot examples sampled from the training dataset.\n",
    "2. **Evaluate Instruction-Demonstration Pairs**\n",
    "    - Sample an instruction and demonstration pair and score it using a Large Language Model (LLM) judge.\n",
    "    - The judge (a TensorZero function utilizing OpenAI's GPT-4o-mini model) scores the quality of the instruction-demonstration pair.\n",
    "    - Scores are aggregated over the evaluation set to produce a final evaluation score.\n",
    "3. **Optimization via Search Algorithms**\n",
    "    - Utilize a random search or a Tree-structured Parzen Estimator (TPE) to determine the next instruction and demonstration pair for evaluation.\n",
    "4. **Iterate the Optimization Process**\n",
    "    - Repeat the optimization process for a fixed number of iterations.\n",
    "5. **Select the Best Performing Prompts**\n",
    "    - The instruction and demonstration pairs corresponding to the highest-performing prompts are formatted to yield optimized system templates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b00020",
   "metadata": {},
   "source": [
    "## Step 1: Define Function Configuration Parameters\n",
    "\n",
    "Specify the TensorZero function you want to optimize. The example below optimizes the system prompt for Named Entity Recognition (NER):\n",
    "\n",
    "- **Function Configuration Directory:** Location of the functionâ€™s configuration files.\n",
    "\n",
    "- **Function Name:** The TensorZero function being optimized.\n",
    "\n",
    "- **Model Variant:** The specific function variant to use as an example for the system template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12709b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuation arguments for the function you want to optimize the prompt for\n",
    "CONFIG_DIR = \"../../examples/data-extraction-ner/config\"\n",
    "\n",
    "# The name of the function you want to optimize the prompt for\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b29ee0",
   "metadata": {},
   "source": [
    "## Step 2: Configure the LLM Judge for Metric Optimization\n",
    "\n",
    "The LLM judge guides the optimization process by evaluating prompt effectiveness. You must define:\n",
    "\n",
    "- **Task Description:** A summary of the task being optimized.\n",
    "- **Optimization Metric:** The metric used for evaluating prompt effectiveness (e.g. Jaccard similarity between predicted and ground truth entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b52423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the task you are optimizing the prompt for to be used by the optimizer judge\n",
    "TASK_DESCRIPTION = \"The task is to extract named entities from the input text.\"\n",
    "\n",
    "# Metric definition for scoring generated prompts\n",
    "METRIC_PROPERTIES = \"The metric is the Jaccard similarity between the predicted and ground truth entities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c76ff6",
   "metadata": {},
   "source": [
    "## Step 3: Define Optimization Parameters\n",
    "\n",
    "The following parameters control the optimization process. Experimenting with different values can help refine results:\n",
    "\n",
    "- **Search Space**\n",
    "    - `NUM_CANDIDATE_INSTRUCTIONS`: Number of candidate instructions to generate.\n",
    "    - `NUM_CANDIDATE_DEMONSTRATIONS`: Number of candidate demonstrations to sample.\n",
    "- **Optimization Control**\n",
    "    - `MAX_ITERATIONS`: Number of optimization steps.\n",
    "    - `MAX_EXAMPLES_PER_DEMONSTRATION`: Maximum few-shot examples per demonstration.\n",
    "- **Evaluation Control**\n",
    "    - `EVAL_FRACTION`: Fraction of the dataset used for scoring generated prompts.\n",
    "    - `MAX_SAMPLES`: Limit on the number of demonstration samples.\n",
    "- **Reproducibility**\n",
    "    - `SEED`: Random seed for consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of candidate instructions to generate and search over\n",
    "NUM_CANDIDATE_INSTRUCTIONS = 10\n",
    "\n",
    "# Number of candidate demonstrations to sample and search over\n",
    "NUM_CANDIDATE_DEMONSTRATIONS = 10\n",
    "\n",
    "# Maximum number of demonstrations in each candidate demonstration set\n",
    "MAX_EXAMPLES_PER_DEMONSTRATION = 10\n",
    "\n",
    "# Maximum number of search steps taken by the optimization algorithm for evaluating instruction-demonstration pairs\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "# Set optimization direction ('maximize' or 'minimize') based on the metric properties you described above.\n",
    "OPTIMIZER_DIRECTION = \"maximize\"\n",
    "\n",
    "# Fraction of the dataset used by the judge to score the quality of the generated prompt\n",
    "EVAL_FRACTION = 0.2\n",
    "\n",
    "# Limit on the number of samples for demonstration selection\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845bc6a",
   "metadata": {},
   "source": [
    "\n",
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67219775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from clickhouse_connect import get_client\n",
    "from minijinja import Environment\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorzero import (\n",
    "    AsyncTensorZeroGateway,\n",
    "    InferenceResponse,\n",
    "    JsonInferenceResponse,\n",
    "    RawText,\n",
    "    Text,\n",
    ")\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from utils.client_calls import candidate_inference, get_instructions, judge_answer\n",
    "from utils.configs.reader import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a9e7f",
   "metadata": {},
   "source": [
    "## Initialize the MIPRO TensorZero Client\n",
    "\n",
    "This client is used to generate candidate instructions and score the quality of responses given the candidate instructions and demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_REQUESTS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mipro_client = await AsyncTensorZeroGateway.build_embedded(\n",
    "    config_file=\"config/tensorzero.toml\",\n",
    "    clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    ")\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c81169",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the TensorZero configuration for the function you want to optimize the prompt for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = load_config(CONFIG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78c9d7",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for prompt optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert FUNCTION_NAME in base_config.functions.keys(), (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in base_config.functions[FUNCTION_NAME].variants.keys(), (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "base_function = base_config.functions[FUNCTION_NAME]\n",
    "base_variant = deepcopy(base_function.variants[TEMPLATE_VARIANT_NAME])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb66b6",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812df9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c02cdc9",
   "metadata": {},
   "source": [
    "Determine the inference table name based on the function type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    base_function.type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {base_function.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa738a",
   "metadata": {},
   "source": [
    "Query the inferences and demonstration feedback from ClickHouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8639c784",
   "metadata": {},
   "source": [
    "You can use one of the metrics above, or choose `FILTER_METRIC_NAME = \"demonstration\"` to use ground truth demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd772162",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_config.metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493624fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_METRIC_NAME = \"demonstration\"\n",
    "FILTER_METRIC_THRESHOLD = 0.9\n",
    "\n",
    "if (\n",
    "    FILTER_METRIC_NAME != \"demonstration\"\n",
    "):  # If no metric name is provided, use ground truth demonstrations\n",
    "    filter_metric = base_config.metrics[FILTER_METRIC_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e519ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    FILTER_METRIC_NAME == \"demonstration\"\n",
    "):  # Assume demonstration feedback is available and used.\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        i.input,\n",
    "        f.value as output,\n",
    "        i.episode_id\n",
    "    FROM\n",
    "        {inference_table_name} i\n",
    "    JOIN\n",
    "        (SELECT\n",
    "            inference_id,\n",
    "            value,\n",
    "            ROW_NUMBER() OVER (PARTITION BY inference_id ORDER BY timestamp DESC) as rn\n",
    "        FROM\n",
    "            DemonstrationFeedback\n",
    "        ) f ON i.id = f.inference_id AND f.rn = 1\n",
    "    WHERE\n",
    "        i.function_name = %(function_name)s\n",
    "    LIMIT %(max_samples)s\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"function_name\": FUNCTION_NAME,\n",
    "        \"max_samples\": MAX_SAMPLES,\n",
    "    }\n",
    "else:\n",
    "    feedback_table_name = {\n",
    "        \"float\": \"FloatMetricFeedback\",\n",
    "        \"boolean\": \"BooleanMetricFeedback\",\n",
    "    }.get(filter_metric.type)\n",
    "\n",
    "    inference_join_key = {\n",
    "        \"episode\": \"episode_id\",\n",
    "        \"inference\": \"id\",\n",
    "    }.get(filter_metric.level)\n",
    "\n",
    "    if inference_join_key is None:\n",
    "        raise ValueError(f\"Unsupported metric level: {filter_metric.level}\")\n",
    "\n",
    "    threshold = FILTER_METRIC_THRESHOLD if filter_metric.type == \"float\" else 0.5\n",
    "    comparison_operator = \">=\" if filter_metric.optimize == \"max\" else \"<=\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        i.input,\n",
    "        i.output,\n",
    "        i.episode_id,\n",
    "        i.function_name,\n",
    "        f.value\n",
    "    FROM\n",
    "        {inference_table_name} i\n",
    "    JOIN\n",
    "        (SELECT\n",
    "            target_id,\n",
    "            value,\n",
    "            ROW_NUMBER() OVER (PARTITION BY target_id ORDER BY timestamp DESC) as rn\n",
    "        FROM\n",
    "            {feedback_table_name}\n",
    "        WHERE\n",
    "            metric_name = %(metric_name)s\n",
    "            AND value {comparison_operator} %(threshold)s\n",
    "        ) f ON i.{inference_join_key} = f.target_id and f.rn = 1\n",
    "    WHERE\n",
    "        i.function_name = %(function_name)s\n",
    "    LIMIT %(max_samples)s\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"function_name\": FUNCTION_NAME,\n",
    "        \"max_samples\": MAX_SAMPLES,\n",
    "        \"metric_name\": FILTER_METRIC_NAME,\n",
    "        \"threshold\": FILTER_METRIC_THRESHOLD,\n",
    "    }\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390140d9",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec091171",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if base_variant.assistant_template is not None:\n",
    "    templates[\"assistant\"] = base_variant.assistant_template\n",
    "\n",
    "if base_variant.system_template is not None:\n",
    "    templates[\"system\"] = base_variant.system_template\n",
    "\n",
    "if base_variant.user_template is not None:\n",
    "    templates[\"user\"] = base_variant.user_template\n",
    "\n",
    "candidate_template = \"\"\"\n",
    "{{ instructions }}\n",
    "{% for demo in demonstrations %}\n",
    "=== Demonstration {{ loop.index }} ===\n",
    "{% for msg in demo.messages %}{% if msg.role != 'system' %}\n",
    "**{{ msg.role | capitalize }}**\n",
    "{% if msg.content is defined %}{% if msg.content is string %}\n",
    "{{ msg.content }}\n",
    "{% else %}{% for block in msg.content %}\n",
    "{{ block.text }}\n",
    "{% endfor %}{% endif %}{% endif %}\n",
    "{% if msg.tool_calls is defined %}{% for call in msg.tool_calls %}\n",
    "> Tool Call: `{{ call.function.name }}` ({{ call.function.arguments }})\n",
    "{% endfor %}{% endif %}{% endif %}{% endfor %}{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "templates[\"candidate\"] = candidate_template\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba20802",
   "metadata": {},
   "source": [
    "Render the messages in the input and demonstration columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(message: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message[\"role\"]\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message[\"content\"]:\n",
    "        if content_block[\"type\"] == \"text\":\n",
    "            parsed_content = content_block[\"value\"]\n",
    "            if not isinstance(parsed_content, str):\n",
    "                parsed_content = env.render_template(role, **parsed_content)\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block[\"type\"] == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block[\"value\"]})\n",
    "        elif content_block[\"type\"] == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_call\" and role == \"assistant\":\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_result\" and role == \"user\":\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            role_message[\"content\"] = content\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output: List[Dict[str, Any]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[str] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    if base_function.type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    elif base_function.type == \"chat\":\n",
    "        for content_block in output:\n",
    "            if content_block[\"type\"] == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": content_block[\"text\"]})\n",
    "            elif content_block[\"type\"] == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "                )\n",
    "            elif content_block[\"type\"] == \"tool_call\":\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {base_function.type}\")\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = content\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample) -> List[Dict[str, Any]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    rendered_messages = []\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or base_variant.system_template:\n",
    "        if base_variant.system_template:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        else:\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        rendered_messages.extend(render_message(message))\n",
    "\n",
    "    # Add the output to the messages\n",
    "    output = json.loads(sample[\"output\"])\n",
    "    rendered_output = render_output(output)\n",
    "    if rendered_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    rendered_messages.append(rendered_output)\n",
    "\n",
    "    return {\"messages\": rendered_messages}\n",
    "\n",
    "\n",
    "df[\"conversational_messages\"] = df.apply(sample_to_openai_messages, axis=1)\n",
    "\n",
    "# Drop null rows\n",
    "df = df[df[\"conversational_messages\"].notna()]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db598f6",
   "metadata": {},
   "source": [
    "Split the data into training and evaluation sets.\n",
    "The training set is used to generate candidate demonstrations.\n",
    "The evaluation set is used by the judge to score the quality of the generated prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d26cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = df[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - EVAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = df[df[\"episode_id\"].isin(train_episode_ids)]\n",
    "eval_df = df[df[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Evaluation set size: {len(eval_df)}\")\n",
    "print(f\"Actual evaluation fraction: {len(eval_df) / len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d64a6",
   "metadata": {},
   "source": [
    "## Generate Candidate Instructions\n",
    "\n",
    "Given the function's system template as an example, generate a set of candidate instructions to optimize the prompt over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_instructions = base_variant.system_template\n",
    "\n",
    "if base_function.system_schema is not None:\n",
    "    example_schema = base_function.system_schema.model_json_schema()\n",
    "else:\n",
    "    example_schema = None\n",
    "\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        get_instructions(\n",
    "            client=mipro_client,\n",
    "            example_instructions=example_instructions,\n",
    "            example_schema=example_schema,\n",
    "            semaphore=semaphore,\n",
    "        )\n",
    "        for _ in range(NUM_CANDIDATE_INSTRUCTIONS)\n",
    "    ]\n",
    ")\n",
    "\n",
    "candidate_instructions = [example_instructions]\n",
    "for response in responses:\n",
    "    if response is None:\n",
    "        continue\n",
    "    candidate_instructions.append(response.output.parsed[\"instructions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd0c2c",
   "metadata": {},
   "source": [
    "## Generate Candidate Demonstrations\n",
    "\n",
    "Given the training set, generate a set of candidate demonstrations to optimize the prompt over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demonstrations(\n",
    "    df: pd.DataFrame,\n",
    "    max_examples_per_demonstration: int,\n",
    "    seed: int = 42,\n",
    ") -> str:\n",
    "    sample = df.sample(\n",
    "        n=max_examples_per_demonstration, replace=False, random_state=seed\n",
    "    )\n",
    "    demonstrations = []\n",
    "    for _, row in sample.iterrows():  # type: ignore\n",
    "        demonstrations.append(row[\"conversational_messages\"])\n",
    "    return demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_demonstrations = [\n",
    "    generate_demonstrations(\n",
    "        df=train_df,\n",
    "        max_examples_per_demonstration=MAX_EXAMPLES_PER_DEMONSTRATION,\n",
    "        seed=seed,\n",
    "    )\n",
    "    for seed in range(NUM_CANDIDATE_DEMONSTRATIONS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3eaa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    env.render_template(\n",
    "        \"candidate\",\n",
    "        demonstrations=candidate_demonstrations[0],\n",
    "        instructions=candidate_instructions[1],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37a556",
   "metadata": {},
   "source": [
    "## Optimize the Prompt\n",
    "\n",
    "### Define the optimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2be191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize online statistics\n",
    "num_instructions = len(candidate_instructions)\n",
    "num_demonstrations = len(candidate_demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ebff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response: Optional[InferenceResponse]) -> str:\n",
    "    if response is None:\n",
    "        return \"\"\n",
    "    if isinstance(response, JsonInferenceResponse):\n",
    "        return str(response.output.parsed)\n",
    "    else:\n",
    "        content = response.content\n",
    "        assert len(content) == 1  # TODO: Handle multiple content blocks\n",
    "        if isinstance(content[0], Text):\n",
    "            return content[0].text\n",
    "        elif isinstance(content[0], RawText):\n",
    "            return content[0].value\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {type(content[0])}\")\n",
    "\n",
    "\n",
    "async def objective(trial: optuna.Trial):\n",
    "    # Sample an instruction and a demonstration set\n",
    "    instruction_index = trial.suggest_categorical(\n",
    "        \"instruction_index\", range(num_instructions)\n",
    "    )\n",
    "    demonstration_index = trial.suggest_categorical(\n",
    "        \"demonstration_index\", range(num_demonstrations)\n",
    "    )\n",
    "    # Format the candidate prompt\n",
    "    candidate_prompt = env.render_template(\n",
    "        \"candidate\",\n",
    "        instructions=candidate_instructions[instruction_index],\n",
    "        demonstrations=candidate_demonstrations[demonstration_index],\n",
    "    )\n",
    "    # Create a new variant with the candidate prompt\n",
    "    candidate_variant_name = f\"{instruction_index}_{demonstration_index}\"\n",
    "    candidate_config = deepcopy(base_config)\n",
    "    candidate_config.functions[FUNCTION_NAME].variants[candidate_variant_name] = (\n",
    "        deepcopy(base_variant)\n",
    "    )\n",
    "    candidate_config.functions[FUNCTION_NAME].variants[\n",
    "        candidate_variant_name\n",
    "    ].system_template = candidate_prompt\n",
    "    candidate_config.functions[FUNCTION_NAME].variants[\n",
    "        candidate_variant_name\n",
    "    ].name = candidate_variant_name\n",
    "    # Write the new config to a temporary directory\n",
    "    tmp_config_dir = candidate_config.write()\n",
    "    # Build a new client with the new config\n",
    "    target_client = await AsyncTensorZeroGateway.build_embedded(\n",
    "        config_file=str(tmp_config_dir / \"tensorzero.toml\"),\n",
    "        clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    "    )\n",
    "    # Asynchronously generate answers for each query in the evaluation set\n",
    "    responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            candidate_inference(\n",
    "                client=target_client,\n",
    "                function_name=FUNCTION_NAME,\n",
    "                input=json.loads(input_args),\n",
    "                variant_name=candidate_variant_name,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for input_args in eval_df[\"input\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Score the responses using the judge\n",
    "    judge_responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            judge_answer(\n",
    "                client=mipro_client,\n",
    "                task_description=TASK_DESCRIPTION,\n",
    "                metric_properties=METRIC_PROPERTIES,\n",
    "                prediction=format_response(response) if response is not None else \"\",\n",
    "                ground_truth=str(ground_truth),\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for response, ground_truth in zip(responses, eval_df[\"output\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Aggregate the scores\n",
    "    scores = []\n",
    "    for response in judge_responses:\n",
    "        if response is not None:\n",
    "            if response.output.parsed is not None:\n",
    "                scores.append(response.output.parsed[\"score\"])\n",
    "\n",
    "    # Return the mean score\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0b9fb",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "We start by sampling a random instruction and demonstration at each iteration in the optimization loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_random = optuna.create_study(\n",
    "    sampler=optuna.samplers.RandomSampler(seed=SEED), direction=OPTIMIZER_DIRECTION\n",
    ")\n",
    "\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    trial = study_random.ask()\n",
    "\n",
    "    value = await objective(trial)\n",
    "    print(f\"Iteration {iteration + 1}: {value}\")\n",
    "\n",
    "    frozen_trial = study_random.tell(trial, value)\n",
    "    study_random._log_completed_trial(frozen_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e89ea",
   "metadata": {},
   "source": [
    "### Tree-structured Parzen Estimator\n",
    "Following the MIPRO paper, we use a tree-structured parzen estimator (TPE) to sample the next instruction and demonstration pair to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_tpe = optuna.create_study(\n",
    "    sampler=TPESampler(seed=SEED), direction=OPTIMIZER_DIRECTION\n",
    ")\n",
    "\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    trial = study_tpe.ask()\n",
    "\n",
    "    value = await objective(trial)\n",
    "    print(f\"Iteration {iteration + 1}: {value}\")\n",
    "\n",
    "    frozen_trial = study_tpe.tell(trial, value)\n",
    "    study_tpe._log_completed_trial(frozen_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55721e9b",
   "metadata": {},
   "source": [
    "## Save the Optimized Candidate\n",
    "\n",
    "We now have an estimate of the best instruction and demonstration pair.\n",
    "We can now generate an optimized system template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63391f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_system_template = env.render_template(\n",
    "    \"candidate\",\n",
    "    instructions=candidate_instructions[study_tpe.best_params[\"instruction_index\"]],\n",
    "    demonstrations=candidate_demonstrations[\n",
    "        study_tpe.best_params[\"demonstration_index\"]\n",
    "    ],\n",
    ")\n",
    "print(optimized_system_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e761cc",
   "metadata": {},
   "source": [
    "You can save the optimized configuration file tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"tmp\"  # Set to a local path to save the optimized config\n",
    "\n",
    "optimized_variant_name = \"mipro_optimized\"\n",
    "optimized_config = deepcopy(base_config)\n",
    "optimized_config.functions[FUNCTION_NAME].variants[optimized_variant_name] = deepcopy(\n",
    "    base_variant\n",
    ")\n",
    "optimized_config.functions[FUNCTION_NAME].variants[\n",
    "    optimized_variant_name\n",
    "].system_template = optimized_system_template\n",
    "optimized_config.functions[FUNCTION_NAME].variants[\n",
    "    optimized_variant_name\n",
    "].name = optimized_variant_name\n",
    "# write the new config to a temporary directory\n",
    "optimized_config_dir = optimized_config.write(base_dir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d138c3d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By following this notebook, you can systematically refine prompts for better performance.\n",
    "The optimized prompt can be saved and used in production by updating the function's system template configuration.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
