{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ad7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccdee8",
   "metadata": {},
   "source": [
    "# Automated Prompt Engineering using MIPRO\n",
    "\n",
    "This notebook provides an automated approach to optimizing prompt engineering using the [Multi-prompt Instruction PRoposal Optimizer (MIPRO)](https://arxiv.org/abs/2406.11695v1).\n",
    "It is designed for TensorZero users who want to optimize their system prompts based on collected inference and feedback data. As such, we currently only support prompt optimization for applications with a single system prompt.\n",
    "\n",
    "Support for applications with multiple system prompts is in the pipeline. If this use case interests you, please see our our [LLM Gym Example](https://github.com/tensorzero/llmgym/tree/main/examples/mipro) for a full implementation.\n",
    "\n",
    "By following this guide, you can systematically refine your prompts to improve model performance in specific tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28484198",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The optimization process involves the following steps:\n",
    "\n",
    "1. **Generate candidate instructions and demonstrations**\n",
    "   - Candidate instructions are generated using OpenAI's o1 model based on a system template and an optional schema.\n",
    "     - This is configurable in the `config/tensorzero.toml` file if you want to use a different model.\n",
    "   - Candidate demonstrations are sets of few-shot examples sampled from the training dataset.\n",
    "2. **Evaluate Instruction-Demonstration Pairs**\n",
    "   - Sample an instruction and demonstration pair and score it using a Large Language Model (LLM) judge.\n",
    "   - The judge (a TensorZero function utilizing OpenAI's GPT-4o-mini model) scores the quality of the instruction-demonstration pair.\n",
    "   - Scores are aggregated over the evaluation set to produce a final evaluation score.\n",
    "3. **Optimization via Search Algorithms**\n",
    "   - Utilize a random search or a Tree-structured Parzen Estimator (TPE) to determine the next instruction and demonstration pair for evaluation.\n",
    "4. **Iterate the Optimization Process**\n",
    "   - Repeat the optimization process for a fixed number of iterations.\n",
    "5. **Select the Best Performing Prompts**\n",
    "   - The instruction and demonstration pairs corresponding to the highest-performing prompts are formatted to yield optimized system templates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b00020",
   "metadata": {},
   "source": [
    "## Step 1: Define Function Configuration Parameters\n",
    "\n",
    "Specify the TensorZero function you want to optimize. The example below optimizes the system prompt for Named Entity Recognition (NER):\n",
    "\n",
    "- **Function Configuration Directory:** Location of the functionâ€™s configuration files.\n",
    "\n",
    "- **Function Name:** The TensorZero function being optimized.\n",
    "\n",
    "- **Model Variant:** The specific function variant to use as an example for the system template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12709b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuation arguments for the function you want to optimize the prompt for\n",
    "CONFIG_DIR = \"../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "# The name of the function you want to optimize the prompt for\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b29ee0",
   "metadata": {},
   "source": [
    "## Step 2: Configure the LLM Judge for Metric Optimization\n",
    "\n",
    "The LLM judge guides the optimization process by evaluating prompt effectiveness. You must define:\n",
    "\n",
    "- **Task Description:** A summary of the task being optimized.\n",
    "- **Optimization Metric:** The metric used for evaluating prompt effectiveness (e.g. Jaccard similarity between predicted and ground truth entities).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b52423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the task you are optimizing the prompt for to be used by the optimizer judge\n",
    "TASK_DESCRIPTION = \"The task is to extract named entities from the input text.\"\n",
    "\n",
    "# Metric definition for scoring generated prompts\n",
    "METRIC_PROPERTIES = \"The metric is the Jaccard similarity between the predicted and ground truth entities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c76ff6",
   "metadata": {},
   "source": [
    "## Step 3: Define Optimization Parameters\n",
    "\n",
    "The following parameters control the optimization process. Experimenting with different values can help refine results:\n",
    "\n",
    "- **Search Space**\n",
    "  - `NUM_CANDIDATE_INSTRUCTIONS`: Number of candidate instructions to generate.\n",
    "  - `NUM_CANDIDATE_DEMONSTRATIONS`: Number of candidate demonstrations to sample.\n",
    "- **Optimization Control**\n",
    "  - `MAX_ITERATIONS`: Number of optimization steps.\n",
    "  - `MAX_EXAMPLES_PER_DEMONSTRATION`: Maximum few-shot examples per demonstration.\n",
    "- **Evaluation Control**\n",
    "  - `EVAL_FRACTION`: Fraction of the dataset used for scoring generated prompts.\n",
    "  - `MAX_SAMPLES`: Limit on the number of demonstration samples.\n",
    "- **Reproducibility**\n",
    "  - `SEED`: Random seed for consistent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d67af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of candidate instructions to generate and search over\n",
    "NUM_CANDIDATE_INSTRUCTIONS = 10\n",
    "\n",
    "# Number of candidate demonstrations to sample and search over\n",
    "NUM_CANDIDATE_DEMONSTRATIONS = 10\n",
    "\n",
    "# Maximum number of demonstrations in each candidate demonstration set\n",
    "MAX_EXAMPLES_PER_DEMONSTRATION = 10\n",
    "\n",
    "# Maximum number of search steps taken by the optimization algorithm for evaluating instruction-demonstration pairs\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "# Set optimization direction ('maximize' or 'minimize') based on the metric properties you described above.\n",
    "OPTIMIZER_DIRECTION = \"maximize\"\n",
    "\n",
    "# Fraction of the dataset used by the judge to score the quality of the generated prompt\n",
    "EVAL_FRACTION = 0.2\n",
    "\n",
    "# Limit on the number of samples for demonstration selection\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845bc6a",
   "metadata": {},
   "source": [
    "## Import Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c29237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "tensorzero_path = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "if tensorzero_path not in sys.path:\n",
    "    sys.path.append(tensorzero_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67219775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from random import shuffle\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "from minijinja import Environment\n",
    "from optuna.samplers import TPESampler\n",
    "from tensorzero import (\n",
    "    AsyncTensorZeroGateway,\n",
    "    ChatCompletionConfig,\n",
    "    ChatInferenceOutput,\n",
    "    InferenceResponse,\n",
    "    JsonInferenceResponse,\n",
    "    RawText,\n",
    "    RenderedSample,\n",
    "    Text,\n",
    ")\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from utils.client_calls import candidate_inference, get_instructions, judge_answer\n",
    "\n",
    "from recipes.util import train_val_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a9e7f",
   "metadata": {},
   "source": [
    "## Initialize the MIPRO TensorZero Client\n",
    "\n",
    "This client is used to generate candidate instructions and score the quality of responses given the candidate instructions and demonstrations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720d957",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_REQUESTS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mipro_client = await AsyncTensorZeroGateway.build_embedded(\n",
    "    config_file=\"config/tensorzero.toml\",\n",
    ")\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c81169",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the TensorZero configuration for the function you want to optimize the prompt for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78c9d7",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we'll use for prompt optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_client = await AsyncTensorZeroGateway.build_embedded(\n",
    "    config_file=CONFIG_DIR,\n",
    "    clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = original_client.experimental_get_config()\n",
    "base_function = config.functions[FUNCTION_NAME]\n",
    "base_variant = base_function.variants[TEMPLATE_VARIANT_NAME]\n",
    "if not isinstance(base_variant, ChatCompletionConfig):\n",
    "    raise ValueError(\"Only chat completion variants are supported\")\n",
    "model_name = base_variant.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfa738a",
   "metadata": {},
   "source": [
    "Query the inferences and demonstration feedback from ClickHouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7615e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences = await original_client.experimental_list_inferences(\n",
    "    function_name=\"extract_entities\",\n",
    "    output_source=\"demonstration\",  # or \"inference\"\n",
    "    filters=None,\n",
    "    # You can also filter by the value of metrics here (e.g.\n",
    "    # FloatMetricFilter(\n",
    "    # metric_name=\"jaccard_similarity\",\n",
    "    # value=0.5,\n",
    "    # comparison_operator=\">\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fdde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_samples = await original_client.experimental_render_samples(\n",
    "    stored_samples=inferences,\n",
    "    variants={FUNCTION_NAME: TEMPLATE_VARIANT_NAME},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565f4fa",
   "metadata": {},
   "source": [
    "Split the data into training and validation sets for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples, val_samples = train_val_split(\n",
    "    rendered_samples,\n",
    "    val_size=EVAL_FRACTION,\n",
    "    last_inference_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390140d9",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec091171",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "candidate_template = \"\"\"\n",
    "{{ instructions }}\n",
    "{% for demo in demonstrations %}\n",
    "=== Demonstration {{ loop.index }} ===\n",
    "{% for msg in demo.messages %}{% if msg.role != 'system' %}\n",
    "**{{ msg.role | capitalize }}**\n",
    "{% if msg.content is defined %}{% if msg.content is string %}\n",
    "{{ msg.content }}\n",
    "{% else %}{% for block in msg.content %}\n",
    "{{ block.text }}\n",
    "{% endfor %}{% endif %}{% endif %}\n",
    "{% if msg.tool_calls is defined %}{% for call in msg.tool_calls %}\n",
    "> Tool Call: `{{ call.function.name }}` ({{ call.function.arguments }})\n",
    "{% endfor %}{% endif %}{% endif %}{% endfor %}{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "templates[\"candidate\"] = candidate_template\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba20802",
   "metadata": {},
   "source": [
    "Render the messages in the input and demonstration columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_output(output: ChatInferenceOutput) -> Dict[str, Any]:\n",
    "    content = []\n",
    "    tool_calls = []\n",
    "\n",
    "    for block in output:\n",
    "        if block.type == \"text\":\n",
    "            content.append({\"type\": \"text\", \"text\": block.text})\n",
    "        elif block.type == \"thought\":\n",
    "            content.append({\"type\": \"text\", \"text\": f\"<think>{block.text}</think>\"})\n",
    "        elif block.type == \"tool_call\":\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(block.arguments),\n",
    "                        \"name\": block.name,\n",
    "                    },\n",
    "                    \"id\": block.id,\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {block.type}\")\n",
    "\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = content\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample: RenderedSample) -> List[Dict[str, Any]]:\n",
    "    rendered_messages = []\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = sample.input.system\n",
    "    if system:\n",
    "        rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in sample.input.messages:\n",
    "        content = []\n",
    "        for part in message.content:\n",
    "            if part.type == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": part.text})\n",
    "            elif part.type == \"tool_call\":\n",
    "                content.append(\n",
    "                    {\n",
    "                        \"type\": \"tool_call\",\n",
    "                        \"name\": part.raw_name,\n",
    "                        \"arguments\": part.raw_arguments,\n",
    "                    }\n",
    "                )\n",
    "            elif part.type == \"tool_result\":\n",
    "                content.append(\n",
    "                    {\"type\": \"tool_result\", \"name\": part.name, \"result\": part.result}\n",
    "                )\n",
    "            elif part.type == \"thought\":\n",
    "                content.append({\"type\": \"text\", \"text\": f\"<think>{part.text}</think>\"})\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported content type: {part.type}\")\n",
    "        rendered_messages.append({\"role\": message.role, \"content\": content})\n",
    "\n",
    "    # Add the output to the messages\n",
    "    if sample.output:\n",
    "        rendered_messages.append(\n",
    "            {\"role\": \"assistant\", \"content\": prepare_output(sample.output)}\n",
    "        )\n",
    "\n",
    "    return rendered_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db598f6",
   "metadata": {},
   "source": [
    "Split the data into training and evaluation sets.\n",
    "The training set is used to generate candidate demonstrations.\n",
    "The evaluation set is used by the judge to score the quality of the generated prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d26cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_examples = [\n",
    "    (sample_to_openai_messages(example), example) for example in train_samples\n",
    "]\n",
    "val_examples = [\n",
    "    (sample_to_openai_messages(example), example) for example in val_samples\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d64a6",
   "metadata": {},
   "source": [
    "## Generate Candidate Instructions\n",
    "\n",
    "Given the function's system template as an example, generate a set of candidate instructions to optimize the prompt over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(base_variant, ChatCompletionConfig):\n",
    "    raise ValueError(\"Only chat completion variants are supported\")\n",
    "\n",
    "example_instructions = base_variant.system_template\n",
    "if example_instructions is None:\n",
    "    raise ValueError(\"System template is required\")\n",
    "\n",
    "if base_function.system_schema is not None:\n",
    "    example_schema = json.dumps(base_function.system_schema.model_json_schema())\n",
    "else:\n",
    "    example_schema = None\n",
    "\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        get_instructions(\n",
    "            client=mipro_client,\n",
    "            example_instructions=example_instructions,\n",
    "            example_schema=example_schema,\n",
    "            semaphore=semaphore,\n",
    "        )\n",
    "        for _ in range(NUM_CANDIDATE_INSTRUCTIONS)\n",
    "    ]\n",
    ")\n",
    "\n",
    "candidate_instructions = [example_instructions]\n",
    "for response in responses:\n",
    "    if response is None:\n",
    "        continue\n",
    "    candidate_instructions.append(response.output.parsed[\"instructions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd0c2c",
   "metadata": {},
   "source": [
    "## Generate Candidate Demonstrations\n",
    "\n",
    "Given the training set, generate a set of candidate demonstrations to optimize the prompt over.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demonstrations(\n",
    "    train_examples: List[Tuple[List[Dict[str, Any]], RenderedSample]],\n",
    "    max_examples_per_demonstration: int,\n",
    "    seed: int = 42,\n",
    ") -> str:\n",
    "    shuffle(train_examples)\n",
    "    demonstrations = []\n",
    "    for example in train_examples[:max_examples_per_demonstration]:\n",
    "        demonstrations.append({\"messages\": example[0]})\n",
    "    return demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d244ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_demonstrations = [\n",
    "    generate_demonstrations(\n",
    "        train_examples=train_examples,\n",
    "        max_examples_per_demonstration=MAX_EXAMPLES_PER_DEMONSTRATION,\n",
    "        seed=seed,\n",
    "    )\n",
    "    for seed in range(NUM_CANDIDATE_DEMONSTRATIONS)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_demonstrations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3eaa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    env.render_template(\n",
    "        \"candidate\",\n",
    "        demonstrations=candidate_demonstrations[0],\n",
    "        instructions=candidate_instructions[1],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37a556",
   "metadata": {},
   "source": [
    "## Optimize the Prompt\n",
    "\n",
    "### Define the optimization objective\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2be191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize online statistics\n",
    "num_instructions = len(candidate_instructions)\n",
    "num_demonstrations = len(candidate_demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ebff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response: Optional[InferenceResponse]) -> str:\n",
    "    if response is None:\n",
    "        return \"\"\n",
    "    if isinstance(response, JsonInferenceResponse):\n",
    "        return str(response.output.parsed)\n",
    "    else:\n",
    "        content = response.content\n",
    "        assert len(content) == 1  # TODO: Handle multiple content blocks\n",
    "        if isinstance(content[0], Text):\n",
    "            return content[0].text\n",
    "        elif isinstance(content[0], RawText):\n",
    "            return content[0].value\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {type(content[0])}\")\n",
    "\n",
    "\n",
    "async def objective(trial: optuna.Trial):\n",
    "    # Sample an instruction and a demonstration set\n",
    "    instruction_index = trial.suggest_categorical(\n",
    "        \"instruction_index\", range(num_instructions)\n",
    "    )\n",
    "    demonstration_index = trial.suggest_categorical(\n",
    "        \"demonstration_index\", range(num_demonstrations)\n",
    "    )\n",
    "    # Format the candidate prompt\n",
    "    candidate_system_prompt = env.render_template(\n",
    "        \"candidate\",\n",
    "        instructions=candidate_instructions[instruction_index],\n",
    "        demonstrations=candidate_demonstrations[demonstration_index],\n",
    "    )\n",
    "\n",
    "    # Asynchronously generate answers for each query in the evaluation set\n",
    "    responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            candidate_inference(\n",
    "                client=original_client,\n",
    "                input=example[1].input,\n",
    "                system_prompt=candidate_system_prompt,\n",
    "                model_name=model_name,\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for example in val_examples\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Score the responses using the judge\n",
    "    judge_responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            judge_answer(\n",
    "                client=mipro_client,\n",
    "                task_description=TASK_DESCRIPTION,\n",
    "                metric_properties=METRIC_PROPERTIES,\n",
    "                prediction=format_response(response) if response is not None else \"\",\n",
    "                ground_truth=str(example[1].output),\n",
    "                semaphore=semaphore,\n",
    "            )\n",
    "            for response, example in zip(responses, val_examples)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Aggregate the scores\n",
    "    scores = []\n",
    "    for response in judge_responses:\n",
    "        if response is not None:\n",
    "            if response.output.parsed is not None:\n",
    "                scores.append(response.output.parsed[\"score\"])\n",
    "\n",
    "    # Return the mean score\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0b9fb",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "We start by sampling a random instruction and demonstration at each iteration in the optimization loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4fd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_random = optuna.create_study(\n",
    "    sampler=optuna.samplers.RandomSampler(seed=SEED), direction=OPTIMIZER_DIRECTION\n",
    ")\n",
    "\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    trial = study_random.ask()\n",
    "\n",
    "    value = await objective(trial)\n",
    "    print(f\"Iteration {iteration + 1}: {value}\")\n",
    "\n",
    "    frozen_trial = study_random.tell(trial, value)\n",
    "    study_random._log_completed_trial(frozen_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e89ea",
   "metadata": {},
   "source": [
    "### Tree-structured Parzen Estimator\n",
    "\n",
    "Following the MIPRO paper, we use a tree-structured parzen estimator (TPE) to sample the next instruction and demonstration pair to evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee28616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_tpe = optuna.create_study(\n",
    "    sampler=TPESampler(seed=SEED), direction=OPTIMIZER_DIRECTION\n",
    ")\n",
    "\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    trial = study_tpe.ask()\n",
    "\n",
    "    value = await objective(trial)\n",
    "    print(f\"Iteration {iteration + 1}: {value}\")\n",
    "\n",
    "    frozen_trial = study_tpe.tell(trial, value)\n",
    "    study_tpe._log_completed_trial(frozen_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55721e9b",
   "metadata": {},
   "source": [
    "## Save the Optimized Candidate\n",
    "\n",
    "We now have an estimate of the best instruction and demonstration pair.\n",
    "We can now generate an optimized system template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63391f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_system_template = env.render_template(\n",
    "    \"candidate\",\n",
    "    instructions=candidate_instructions[study_tpe.best_params[\"instruction_index\"]],\n",
    "    demonstrations=candidate_demonstrations[\n",
    "        study_tpe.best_params[\"demonstration_index\"]\n",
    "    ],\n",
    ")\n",
    "print(optimized_system_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e761cc",
   "metadata": {},
   "source": [
    "You can make a new variant with this optimized system template.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d138c3d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "By following this notebook, you can systematically refine prompts for better performance.\n",
    "The optimized prompt can be saved and used in production by updating the function's system template configuration.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
