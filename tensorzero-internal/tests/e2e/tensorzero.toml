# NOTE: This is an example configuration file for TensorZero (used for E2E tests).
#       You can use this file as a reference for your own configuration by adding
#       your own models, functions, and metrics.

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                  GENERAL                                   │
# └────────────────────────────────────────────────────────────────────────────┘

[gateway]
bind_address = "0.0.0.0:3000"
debug = true
enable_template_filesystem_access = true

[object_storage]
type = "disabled"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                   MODELS                                   │
# └────────────────────────────────────────────────────────────────────────────┘

[models."gpt-4o-mini-2024-07-18"]
routing = ["openai"]

[models."gpt-4o-mini-2024-07-18".providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"

[models."gpt-4o-mini-2024-07-18-extra-body"]
routing = ["openai"]

[models."gpt-4o-mini-2024-07-18-extra-body".providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"
extra_body = [
    { pointer = "/temperature", value = 0.456 },
    { pointer = "/frequency_penalty", value = 1.42 },
]

[models."gpt-4o-mini-2024-07-18-dynamic"]
routing = ["openai"]

[models."gpt-4o-mini-2024-07-18-dynamic".providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"
api_key_location = "dynamic::openai_api_key"

[models."o1-2024-12-17"]
routing = ["openai"]

[models."o1-2024-12-17".providers.openai]
type = "openai"
model_name = "o1-2024-12-17"

[models."gpt-4o-mini-azure"]
routing = ["azure"]

[models."gpt-4o-mini-azure".providers.azure]
type = "azure"
deployment_id = "gpt4o-mini-20240718"
endpoint = "https://t0-azure-openai-east.openai.azure.com"


[models."gpt-4o-mini-azure-dynamic"]
routing = ["azure"]

[models."gpt-4o-mini-azure-dynamic".providers.azure]
type = "azure"
deployment_id = "gpt4o-mini-20240718"
endpoint = "https://t0-azure-openai-east.openai.azure.com"
api_key_location = "dynamic::azure_openai_api_key"

[models."claude-3-7-sonnet-20250219-thinking"]
routing = ["anthropic-extra-body"]

[models."claude-3-7-sonnet-20250219-thinking".providers.anthropic-extra-body]
type = "anthropic"
model_name = "claude-3-7-sonnet-20250219"
extra_body = [
    { pointer = "/thinking", value = { type = "enabled", budget_tokens = 1024 } },
]

[models."claude-3-7-sonnet-20250219-thinking-128k"]
routing = ["anthropic"]

[models."claude-3-7-sonnet-20250219-thinking-128k".providers.anthropic]
type = "anthropic"
model_name = "claude-3-7-sonnet-20250219"
extra_headers = [
    { name = "anthropic-beta", value = "output-128k-2025-02-19"}
]
extra_body = [
    # We use a budget tokens of 1024 to make sure that it doesn't think for too long,
    # since 'stop_sequences' does not seem to apply to thinking. We set 'max_tokens'
    # to 128k in 'test_thinking_128k'
    { pointer = "/thinking", value = { type = "enabled", budget_tokens = 1024 } },
    { pointer = "/stop_sequences", value = ["my_custom_stop"]}
]

[models.claude-3-haiku-20240307]
routing = ["anthropic", "aws-bedrock"]

[models.claude-3-haiku-20240307.providers.anthropic]
type = "anthropic"
model_name = "claude-3-haiku-20240307"

[models.claude-3-haiku-20240307.providers.aws-bedrock]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"
region = "us-east-1"

[models.claude-3-haiku-20240307-us-east-1]
routing = ["aws-bedrock-us-east-1"]

[models.claude-3-haiku-20240307-us-east-1.providers.aws-bedrock-us-east-1]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"
region = "us-east-1"

[models.claude-3-haiku-20240307-uk-hogwarts-1]
routing = ["aws-bedrock-uk-hogwarts-1"]

[models.claude-3-haiku-20240307-uk-hogwarts-1.providers.aws-bedrock-uk-hogwarts-1]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"
region = "uk-hogwarts-1"

# Duplicate so that we can test just Anthropic no fallbacks
[models.claude-3-haiku-20240307-anthropic]
routing = ["anthropic"]

[models.claude-3-haiku-20240307-anthropic.providers.anthropic]
type = "anthropic"
model_name = "claude-3-haiku-20240307"

[models."claude-3-haiku-20240307-anthropic-dynamic"]
routing = ["anthropic"]

[models."claude-3-haiku-20240307-anthropic-dynamic".providers.anthropic]
type = "anthropic"
model_name = "claude-3-haiku-20240307"
api_key_location = "dynamic::anthropic_api_key"

# Duplicate so that we can test just AWS Bedrock no fallbacks
[models.claude-3-haiku-20240307-aws-bedrock]
routing = ["aws-bedrock"]

[models.claude-3-haiku-20240307-aws-bedrock.providers.aws-bedrock]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"
region = "us-east-1"

[models.claude-3-haiku-20240307-gcp-vertex]
routing = ["gcp_vertex_anthropic"]

[models.claude-3-haiku-20240307-gcp-vertex.providers.gcp_vertex_anthropic]
type = "gcp_vertex_anthropic"
model_id = "claude-3-haiku@20240307"
location = "us-central1"
project_id = "tensorzero-public"

[models."gemini-2.0-flash-001"]
routing = ["gcp_vertex_gemini"]

[models."gemini-2.0-flash-001".providers.gcp_vertex_gemini]
type = "gcp_vertex_gemini"
model_id = "gemini-2.0-flash-001"
location = "us-central1"
project_id = "tensorzero-public"


[models."gemini-1.5-pro-001"]
routing = ["gcp_vertex_gemini"]

[models."gemini-1.5-pro-001".providers.gcp_vertex_gemini]
type = "gcp_vertex_gemini"
model_id = "gemini-1.5-pro-001"
location = "us-central1"
project_id = "tensorzero-public"


[models."gemini-2.0-flash-lite"]
routing = ["google_ai_studio_gemini"]

[models."gemini-2.0-flash-lite".providers.google_ai_studio_gemini]
type = "google_ai_studio_gemini"
model_name = "gemini-2.0-flash-lite"


[models."gemini-2.0-flash-lite-dynamic"]
routing = ["google_ai_studio_gemini"]

[models."gemini-2.0-flash-lite-dynamic".providers.google_ai_studio_gemini]
type = "google_ai_studio_gemini"
model_name = "gemini-2.0-flash-lite"
api_key_location = "dynamic::google_ai_studio_api_key"

[models."gemini-1.5-pro-002"]
routing = ["google_ai_studio_gemini"]

[models."gemini-1.5-pro-002".providers.google_ai_studio_gemini]
type = "google_ai_studio_gemini"
model_name = "gemini-1.5-pro-002"

[models."gemini-1.5-pro-002-dynamic"]
routing = ["google_ai_studio_gemini"]

[models."gemini-1.5-pro-002-dynamic".providers.google_ai_studio_gemini]
type = "google_ai_studio_gemini"
model_name = "gemini-1.5-pro-002"
api_key_location = "dynamic::google_ai_studio_api_key"

[models."llama3.3-70b-instruct-fireworks"]
routing = ["fireworks"]

[models."llama3.3-70b-instruct-fireworks".providers.fireworks]
type = "fireworks"
model_name = "accounts/fireworks/models/llama-v3p3-70b-instruct"

[models."llama3.3-70b-instruct-fireworks-dynamic"]
routing = ["fireworks"]

[models."llama3.3-70b-instruct-fireworks-dynamic".providers.fireworks]
type = "fireworks"
model_name = "accounts/fireworks/models/llama-v3p3-70b-instruct"
api_key_location = "dynamic::fireworks_api_key"

[models.qwen2p5-72b-instruct]
routing = ["fireworks"]

[models.qwen2p5-72b-instruct.providers.fireworks]
type = "fireworks"
model_name = "accounts/fireworks/models/qwen2p5-72b-instruct"

[models."llama3.1-8b-instruct-together"]
routing = ["together"]

[models."llama3.1-8b-instruct-together".providers.together]
type = "together"
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"

[models."phi-3.5-mini-instruct-tgi"]
routing = ["tgi"]

[models."phi-3.5-mini-instruct-tgi".providers.tgi]
type = "tgi"
api_base = "https://zr0gj152lrhnrr-80.proxy.runpod.net/v1/"

[models."llama3.1-8b-instruct-together-dynamic"]
routing = ["together"]

[models."llama3.1-8b-instruct-together-dynamic".providers.together]
type = "together"
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
api_key_location = "dynamic::together_api_key"

[models."llama3.1-405b-instruct-turbo-together"]
routing = ["together"]

[models."llama3.1-405b-instruct-turbo-together".providers.together]
type = "together"
model_name = "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo"

[models."HuggingFaceTB/SmolLM-1.7B-Instruct"]
routing = ["sglang"]

[models."HuggingFaceTB/SmolLM-1.7B-Instruct".providers.sglang]
type = "sglang"
model_name = "HuggingFaceTB/SmolLM-1.7B-Instruct"
api_base = "https://s6o7i4kpd3a1em-80.proxy.runpod.net/v1/"

[models."microsoft/Phi-3.5-mini-instruct"]
routing = ["vllm"]

[models."microsoft/Phi-3.5-mini-instruct".providers.vllm]
type = "vllm"
model_name = "microsoft/Phi-3.5-mini-instruct"
api_base = "https://pun1owldydhycl-8000.proxy.runpod.net/v1/"

[models."microsoft/Phi-3.5-mini-instruct-dynamic"]
routing = ["vllm"]

[models."microsoft/Phi-3.5-mini-instruct-dynamic".providers.vllm]
type = "vllm"
model_name = "microsoft/Phi-3.5-mini-instruct"
api_base = "https://pun1owldydhycl-8000.proxy.runpod.net/v1/"
api_key_location = "dynamic::vllm_api_key"

[models."open-mistral-nemo-2407"]
routing = ["mistral"]

[models."open-mistral-nemo-2407".providers.mistral]
type = "mistral"
model_name = "open-mistral-nemo-2407"

[models."open-mistral-nemo-2407-dynamic"]
routing = ["mistral"]

[models."open-mistral-nemo-2407-dynamic".providers.mistral]
type = "mistral"
model_name = "open-mistral-nemo-2407"
api_key_location = "dynamic::mistral_api_key"

[models.o1-mini]
routing = ["openai"]

[models.o1-mini.providers.openai]
type = "openai"
model_name = "o1-mini"

[models."grok_2_1212"]
routing = ["xai"]

[models."grok_2_1212".providers.xai]
type = "xai"
model_name = "grok-2-1212"

[models."grok_2_1212-dynamic"]
routing = ["xai"]

[models."grok_2_1212-dynamic".providers.xai]
type = "xai"
model_name = "grok-2-1212"
api_key_location = "dynamic::xai_api_key"

[models."meta-llama/Meta-Llama-3-70B-Instruct"]
routing = ["hyperbolic"]

[models."meta-llama/Meta-Llama-3-70B-Instruct".providers.hyperbolic]
type = "hyperbolic"
model_name = "meta-llama/Meta-Llama-3-70B-Instruct"

[models."meta-llama/Meta-Llama-3-70B-Instruct-dynamic"]
routing = ["hyperbolic"]

[models."meta-llama/Meta-Llama-3-70B-Instruct-dynamic".providers.hyperbolic]
type = "hyperbolic"
model_name = "meta-llama/Meta-Llama-3-70B-Instruct"
api_key_location = "dynamic::hyperbolic_api_key"

[models."deepseek-chat"]
routing = ["deepseek"]

[models."deepseek-chat".providers.deepseek]
type = "deepseek"
model_name = "deepseek-chat"

[models."deepseek-reasoner"]
routing = ["deepseek"]

[models."deepseek-reasoner".providers.deepseek]
type = "deepseek"
model_name = "deepseek-reasoner"

[models."deepseek-chat-dynamic"]
routing = ["deepseek"]

[models."deepseek-chat-dynamic".providers.deepseek]
type = "deepseek"
model_name = "deepseek-chat"

[models."together-deepseek-r1"]
routing = ["together"]

[models."together-deepseek-r1".providers.together]
type = "together"
model_name = "deepseek-ai/DeepSeek-R1"


[models.slow]
routing = ["slow"]

[models.slow.providers.slow]
type = "dummy"
model_name = "slow"

[models.test]
routing = ["good"]

[models.test.providers.good]
type = "dummy"
model_name = "good"

[models.error]
routing = ["error"]

[models.error.providers.error]
type = "dummy"
model_name = "error"

[models.test_fallback]
routing = ["error", "good"]

[models.test_fallback.providers.error]
type = "dummy"
model_name = "error"

[models.test_fallback.providers.good]
type = "dummy"
model_name = "good"

[models.json]
routing = ["json"]

[models.json.providers.json]
type = "dummy"
model_name = "json"

[models.json_goodbye]
routing = ["dummy"]

[models.json_goodbye.providers.dummy]
type = "dummy"
model_name = "json_goodbye"

[models.tool]
routing = ["tool"]

[models.tool.providers.tool]
type = "dummy"
model_name = "tool"

[models.bad_tool]
routing = ["bad_tool"]

[models.bad_tool.providers.bad_tool]
type = "dummy"
model_name = "bad_tool"

[models.best_of_n_evaluator]
routing = ["best_of_n_evaluator"]

[models.best_of_n_evaluator.providers.best_of_n_evaluator]
type = "dummy"
model_name = "best_of_n_0"

[models.test_key]
routing = ["dummy"]

[models.test_key.providers.dummy]
type = "dummy"
model_name = "test_key"
api_key_location = "dynamic::DUMMY_API_KEY"

[models.flaky_basic_test]
routing = ["dummy"]

[models.flaky_basic_test.providers.dummy]
type = "dummy"
model_name = "flaky_basic_test"

[models.err_in_stream]
routing = ["dummy"]

[models.err_in_stream.providers.dummy]
type = "dummy"
model_name = "err_in_stream"

[models.flaky_best_of_n_judge]
routing = ["dummy"]

[models.flaky_best_of_n_judge.providers.dummy]
type = "dummy"
model_name = "flaky_best_of_n_judge"

[models.alternate]
routing = ["dummy"]

[models.alternate.providers.dummy]
type = "dummy"
model_name = "alternate"

[models.json_beatles_1]
routing = ["dummy"]

[models.json_beatles_1.providers.dummy]
type = "dummy"
model_name = "json_beatles_1"

[models.json_beatles_2]
routing = ["dummy"]

[models.json_beatles_2.providers.dummy]
type = "dummy"
model_name = "json_beatles_2"

[models.json_reasoner]
routing = ["dummy"]

[models.json_reasoner.providers.dummy]
type = "dummy"
model_name = "json_reasoner"

[models.reasoner]
routing = ["dummy"]

[models.reasoner.providers.dummy]
type = "dummy"
model_name = "reasoner"

[embedding_models.text-embedding-3-small]
routing = ["openai"]

[embedding_models.text-embedding-3-small.providers.openai]
type = "openai"
model_name = "text-embedding-3-small"


# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                 FUNCTIONS                                  │
# └────────────────────────────────────────────────────────────────────────────┘

[functions.basic_test_no_system_schema]
type = "chat"

[functions.basic_test_no_system_schema.variants.test]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.basic_test]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"

[functions.basic_test.variants.test]
type = "chat_completion"
weight = 1
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
temperature = 1.0
max_tokens = 100
seed = 69

[functions.basic_test.variants.slow]
type = "chat_completion"
model = "slow"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"


[functions.basic_test.variants.test_dynamic_api_key]
type = "chat_completion"
model = "test_key"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
temperature = 1.0
max_tokens = 100
seed = 69

[functions.basic_test.variants.anthropic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.anthropic-extra-body]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.anthropic-extra-headers]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "x-api-key", value = "invalid_anthropic_auth" }]

[functions.basic_test.variants.anthropic-dynamic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.anthropic-shorthand]
type = "chat_completion"
model = "anthropic::claude-3-haiku-20240307"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock-extra-body]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/inferenceConfig/temperature", value = 0.123 }]

[functions.basic_test.variants.aws-bedrock-extra-headers]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Content-Length", value = "2" }]

[functions.basic_test.variants.aws-bedrock-us-east-1]
type = "chat_completion"
model = "claude-3-haiku-20240307-us-east-1"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock-uk-hogwarts-1]
type = "chat_completion"
model = "claude-3-haiku-20240307-uk-hogwarts-1"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.azure]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.azure-extra-body]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.azure-extra-headers]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "api-key", value = "invalid_azure_openai_auth" }]

[functions.basic_test.variants.azure-dynamic]
type = "chat_completion"
model = "gpt-4o-mini-azure-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.fireworks]
type = "chat_completion"
model = "qwen2p5-72b-instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.fireworks-extra-body]
type = "chat_completion"
model = "qwen2p5-72b-instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.fireworks-extra-headers]
type = "chat_completion"
model = "qwen2p5-72b-instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_fireworks_auth" }]

[functions.basic_test.variants.fireworks-dynamic]
type = "chat_completion"
model = "llama3.3-70b-instruct-fireworks-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.fireworks-shorthand]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-gemini-flash-extra-body]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/generationConfig/temperature", value = 0.123 }]

[functions.basic_test.variants.gcp-vertex-gemini-flash-extra-headers]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_gcp_vertex_auth" }]

[functions.basic_test.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b-extra-body]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/generationConfig/temperature", value = 0.123 }]

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b-extra-headers]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Content-Length", value = "2" }]

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b-dynamic]
type = "chat_completion"
model = "gemini-2.0-flash-lite-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b-shorthand]
type = "chat_completion"
model = "google_ai_studio_gemini::gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-pro-002]
type = "chat_completion"
model = "gemini-1.5-pro-002"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-pro-002-dynamic]
type = "chat_completion"
model = "gemini-1.5-pro-002-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-haiku]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-haiku-extra-body]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.gcp-vertex-haiku-extra-headers]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Content-Length", value = "2" }]

[functions.basic_test.variants.mistral]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.mistral-extra-body]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.mistral-extra-headers]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_mistral_auth" }]

[functions.basic_test.variants.mistral-dynamic]
type = "chat_completion"
model = "open-mistral-nemo-2407-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.mistral-shorthand]
type = "chat_completion"
model = "mistral::open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100


[functions.basic_test.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.openai-extra-body]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.openai-extra-body-provider-config]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18-extra-body"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [
    { pointer = "/temperature", value = 0.123 },
    { pointer = "/max_completion_tokens", value = 123 },
]

[functions.basic_test.variants.openai-extra-headers]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "Bearer: invalid_openai_auth" }]

[functions.basic_test.variants.openai-o1]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 1000

[functions.basic_test.variants.openai-dynamic]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.openai-shorthand]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.o1-mini]
type = "chat_completion"
model = "o1-mini"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.basic_test.variants.tgi]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.tgi-extra-body]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.tgi-extra-headers]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_tgi_auth" }]

[functions.basic_test.variants.together]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.together-extra-body]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.together-extra-headers]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "Bearer invalid_together_auth" }]

[functions.basic_test.variants.together-dynamic]
type = "chat_completion"
model = "llama3.1-8b-instruct-together-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.together-shorthand]
type = "chat_completion"
model = "together::meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.together-tool]
type = "chat_completion"
model = "llama3.1-405b-instruct-turbo-together"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.sglang]
type = "chat_completion"
model = "HuggingFaceTB/SmolLM-1.7B-Instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.sglang-extra-body]
type = "chat_completion"
model = "HuggingFaceTB/SmolLM-1.7B-Instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.sglang-extra-headers]
type = "chat_completion"
model = "HuggingFaceTB/SmolLM-1.7B-Instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "Bearer invalid_sglang_auth" }]

[functions.basic_test.variants.vllm]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.vllm-extra-body]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.vllm-extra-headers]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_vllm_auth" }]

[functions.basic_test.variants.vllm-dynamic]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.xai]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.xai-extra-body]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.xai-extra-headers]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "Bearer invalid_xai_auth" }]

[functions.basic_test.variants.xai-dynamic]
type = "chat_completion"
model = "grok_2_1212-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.xai-shorthand]
type = "chat_completion"
model = "xai::grok-2-1212"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.hyperbolic]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.hyperbolic-extra-body]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.hyperbolic-extra-headers]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "Bearer invalid_hyperbolic_auth" }]

[functions.basic_test.variants.hyperbolic-dynamic]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.hyperbolic-shorthand]
type = "chat_completion"
model = "hyperbolic::meta-llama/Meta-Llama-3-70B-Instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.deepseek-chat]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.deepseek-chat-extra-body]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.deepseek-chat-extra-headers]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "Bearer sk-bad-tensorzero" }]

[functions.basic_test.variants.deepseek-reasoner]
type = "chat_completion"
model = "deepseek-reasoner"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 800

[functions.basic_test.variants.together-deepseek-r1]
type = "chat_completion"
model = "together-deepseek-r1"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 800

[functions.basic_test.variants.reasoner]
type = "chat_completion"
model = "reasoner"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 800

[functions.basic_test.variants.deepseek-dynamic]
type = "chat_completion"
model = "deepseek-chat-dynamic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.deepseek-shorthand]
type = "chat_completion"
model = "deepseek::deepseek-chat"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
[functions.basic_test.variants.flaky]
type = "chat_completion"
model = "flaky_basic_test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
retries = { num_retries = 5, max_delay_s = 0.1 }

[functions.basic_test.variants.err_in_stream]
type = "chat_completion"
model = "err_in_stream"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.dicl]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
embedding_model = "text-embedding-3-small"
k = 3
max_tokens = 100

[functions.basic_test.variants.empty_dicl]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
embedding_model = "text-embedding-3-small"
k = 3
max_tokens = 100

[functions.basic_test.variants.empty_dicl_extra_body]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
embedding_model = "text-embedding-3-small"
k = 3
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.empty_dicl_shorthand]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
embedding_model = "openai::text-embedding-3-small"
k = 3
max_tokens = 100

[functions.model_fallback_test]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"

[functions.model_fallback_test.variants.test]
type = "chat_completion"
weight = 1
model = "test_fallback"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.json_fail]
type = "json"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../fixtures/config/functions/basic_test/output_schema.json"

[functions.json_fail.variants.test]
type = "chat_completion"
weight = 1
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success]
type = "json"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
user_schema = "../../fixtures/config/functions/json_success/user_schema.json"
output_schema = "../../fixtures/config/functions/basic_test/output_schema.json"

[functions.json_success.variants.test]
type = "chat_completion"
weight = 1
model = "json"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.anthropic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.anthropic-default]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.anthropic-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.aws-bedrock]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.aws-bedrock-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.aws-bedrock-default]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.azure]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.azure-default]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.azure-strict]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.json_success.variants.azure-implicit]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.fireworks]
type = "chat_completion"
model = "llama3.3-70b-instruct-fireworks"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.fireworks-implicit]
type = "chat_completion"
model = "qwen2p5-72b-instruct"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.deepseek-reasoner]
type = "chat_completion"
model = "deepseek-reasoner"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 800

[functions.json_success.variants.together-deepseek-r1]
type = "chat_completion"
model = "together-deepseek-r1"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 800

[functions.json_success.variants.fireworks-default]
type = "chat_completion"
model = "llama3.3-70b-instruct-fireworks"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-flash-default]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.gcp-vertex-gemini-flash-implicit]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-pro-implicit]
type = "chat_completion"
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-haiku]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-haiku-default]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.gcp-vertex-haiku-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.google-ai-studio-gemini-flash-8b]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.google-ai-studio-gemini-flash-8b-default]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"


[functions.json_success.variants.google-ai-studio-gemini-flash-8b-implicit]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.google-ai-studio-gemini-pro-002]
type = "chat_completion"
model = "gemini-1.5-pro-002"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.google-ai-studio-gemini-pro-002-implicit]
type = "chat_completion"
model = "gemini-1.5-pro-002"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.json_reasoner]
type = "chat_completion"
model = "json_reasoner"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"
[functions.json_success.variants.mistral]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.mistral-default]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"
[functions.json_success.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.openai-default]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.openai-o1]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 1000

[functions.json_success.variants.openai-implicit]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.openai-strict]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.json_success.variants.tgi]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.tgi-default]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.together]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.together-default]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.together-implicit]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.sglang]
type = "chat_completion"
model = "HuggingFaceTB/SmolLM-1.7B-Instruct"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.json_success.variants.sglang-default]
type = "chat_completion"
model = "HuggingFaceTB/SmolLM-1.7B-Instruct"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"


[functions.json_success.variants.vllm]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.vllm-default]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.vllm-implicit]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.dicl]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
system_instructions = "../../fixtures/config/functions/json_success/prompt/system_instructions.txt"
embedding_model = "text-embedding-3-small"
k = 3
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.deepseek-chat]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.deepseek-chat-default]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.xai]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.xai-default]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.xai-strict]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.dynamic_json]
type = "json"
system_schema = "../../fixtures/config/functions/dynamic_json/system_schema.json"
user_schema = "../../fixtures/config/functions/dynamic_json/user_schema.json"

[functions.dynamic_json.variants.test]
type = "chat_completion"
weight = 1
model = "json"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.anthropic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.anthropic-default]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.anthropic-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.aws-bedrock]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.aws-bedrock-default]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.aws-bedrock-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.azure]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.azure-default]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.azure-strict]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.dynamic_json.variants.azure-implicit]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.fireworks]
type = "chat_completion"
model = "llama3.3-70b-instruct-fireworks"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.fireworks-default]
type = "chat_completion"
model = "llama3.3-70b-instruct-fireworks"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.fireworks-implicit]
type = "chat_completion"
model = "qwen2p5-72b-instruct"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-gemini-flash-default]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"


[functions.dynamic_json.variants.gcp-vertex-gemini-flash-implicit]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-gemini-pro-implicit]
type = "chat_completion"
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.google-ai-studio-gemini-flash-8b]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.google-ai-studio-gemini-flash-8b-default]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.google-ai-studio-gemini-flash-8b-implicit]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.google-ai-studio-gemini-pro-002]
type = "chat_completion"
model = "gemini-1.5-pro-002"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.google-ai-studio-gemini-pro-002-implicit]
type = "chat_completion"
model = "gemini-1.5-pro-002"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-haiku]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-haiku-default]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.gcp-vertex-haiku-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.mistral]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.mistral-default]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.openai-default]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.openai-o1]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 1000


[functions.dynamic_json.variants.openai-implicit]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.openai-strict]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.dynamic_json.variants.sglang]
type = "chat_completion"
model = "HuggingFaceTB/SmolLM-1.7B-Instruct"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.dynamic_json.variants.sglang-default]
type = "chat_completion"
model = "HuggingFaceTB/SmolLM-1.7B-Instruct"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.tgi]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.tgi-default]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.together]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.together-default]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.together-implicit]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.vllm]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.vllm-default]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.vllm-implicit]
type = "chat_completion"
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.deepseek-chat]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.deepseek-chat-default]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.xai]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.xai-default]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.xai-strict]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.variant_failover]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
user_schema = "../../fixtures/config/functions/variant_failover/user_schema.json"

[functions.variant_failover.variants.good]
type = "chat_completion"
weight = 0.5
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover.variants.error]
type = "chat_completion"
weight = 0.5
model = "error"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover_zero_weight]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
user_schema = "../../fixtures/config/functions/variant_failover/user_schema.json"

[functions.variant_failover_zero_weight.variants.first_error]
type = "chat_completion"
weight = 0.5
model = "dummy::error_1"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover_zero_weight.variants.second_error]
type = "chat_completion"
weight = 0.5
model = "dummy::error_2"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover_zero_weight.variants.no_weight]
type = "chat_completion"
model = "dummy::error_no_weight"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover_zero_weight.variants.zero_weight]
type = "chat_completion"
weight = 0
model = "dummy::error_zero_weight"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.prometheus_test1]
type = "chat"

[functions.prometheus_test1.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test2]
type = "chat"

[functions.prometheus_test2.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test3]
type = "chat"

[functions.prometheus_test3.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test4]
type = "chat"

[functions.prometheus_test4.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.weather_helper]
type = "chat"
system_schema = "../../fixtures/config/functions/weather_helper/system_schema.json"
tools = ["get_temperature"]
tool_choice = "auto"

[functions.weather_helper.variants.anthropic-thinking]
type = "chat_completion"
weight = 0
model = "claude-3-7-sonnet-20250219-thinking"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
# This must be at least as large as `thinking.budget_tokens`
max_tokens = 1200

[functions.weather_helper.variants.variant]
type = "chat_completion"
weight = 1
model = "tool"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.bad_tool]
type = "chat_completion"
model = "bad_tool"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.aws-bedrock]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.anthropic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.azure]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.fireworks]
type = "chat_completion"
model = "qwen2p5-72b-instruct"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.google-ai-studio-gemini-flash-8b]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.google-ai-studio-gemini-pro-002]
type = "chat_completion"
model = "gemini-1.5-pro-002"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-haiku]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.mistral]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.openai-o1]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 1000

[functions.weather_helper.variants.together-tool]
type = "chat_completion"
model = "llama3.1-405b-instruct-turbo-together"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.xai]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper_parallel]
type = "chat"
system_schema = "../../fixtures/config/functions/weather_helper_parallel/system_schema.json"
tools = ["get_temperature", "get_humidity"]
tool_choice = "auto"
# We use an inference-time parameter to set `parallel_tool_calls = true` for the test

[functions.weather_helper_parallel.variants.openai]
type = "chat_completion"
weight = 1
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper_parallel.variants.openai-o1]
type = "chat_completion"
weight = 1
model = "o1-2024-12-17"
system_template = "../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"
max_tokens = 1000

[functions.weather_helper_parallel.variants.anthropic]
type = "chat_completion"
weight = 1
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"

[functions.weather_helper_parallel.variants.together-tool]
type = "chat_completion"
weight = 1
model = "llama3.1-405b-instruct-turbo-together"
system_template = "../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"

[functions.best_of_n]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"

[functions.best_of_n.variants.variant0]
type = "chat_completion"
weight = 0
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.best_of_n.variants.variant1]
type = "chat_completion"
weight = 0
model = "json"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.best_of_n.variants.best_of_n_variant]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant1"]

[functions.best_of_n.variants.best_of_n_variant.evaluator]
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.best_of_n.variants.best_of_n_variant_extra_body]
type = "experimental_best_of_n_sampling"
weight = 0
candidates = ["variant0", "variant1"]

[functions.best_of_n.variants.best_of_n_variant_extra_body.evaluator]
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
extra_body = [{ pointer = "/generationConfig/temperature", value = 0.123 }]

[functions.best_of_n.variants.flaky_best_of_n_variant]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant1"]

[functions.best_of_n.variants.flaky_best_of_n_variant.evaluator]
model = "flaky_best_of_n_judge"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
retries = { num_retries = 5, max_delay_s = 0.1 }

[functions.best_of_n_json]
type = "json"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../fixtures/config/functions/best_of_n_json/output_schema.json"

[functions.best_of_n_json.variants.variant0]
type = "chat_completion"
weight = 0
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json.variants.variant1]
type = "chat_completion"
weight = 0
model = "json"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json.variants.variant2]
type = "chat_completion"
weight = 0
model = "json_goodbye"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json.variants.best_of_n_variant]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant1", "variant2"]

[functions.best_of_n_json.variants.best_of_n_variant.evaluator]
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json.variants.best_of_n_variant_implicit_tool]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant1", "variant2"]

[functions.best_of_n_json.variants.best_of_n_variant_implicit_tool.evaluator]
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "implicit_tool"

[functions.best_of_n_json_repeated]
type = "json"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../fixtures/config/functions/best_of_n_json/output_schema.json"

[functions.best_of_n_json_repeated.variants.variant0]
type = "chat_completion"
weight = 0
model = "dummy::random_answer"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json_repeated.variants.variant1]
type = "chat_completion"
weight = 0
model = "json"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json_repeated.variants.best_of_n_variant]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant0", "variant1"]

[functions.best_of_n_json_repeated.variants.best_of_n_variant.evaluator]
model = "dummy::best_of_n_0"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"

[functions.mixture_of_n.variants.variant0]
type = "chat_completion"
weight = 0
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n.variants.variant1]
type = "chat_completion"
weight = 0
model = "alternate"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant1"]

[functions.mixture_of_n.variants.mixture_of_n_variant.fuser]
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"


[functions.mixture_of_n_extra_body]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"


[functions.mixture_of_n_extra_body.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant1"]

[functions.mixture_of_n_extra_body.variants.variant0]
type = "chat_completion"
model = "o1-2024-12-17"
weight = 0
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n_extra_body.variants.variant1]
type = "chat_completion"
model = "test"
weight = 0
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n_extra_body.variants.mixture_of_n_variant.fuser]
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.mixture_of_n_json]
type = "json"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../fixtures/config/functions/mixture_of_n_json/output_schema.json"

[functions.mixture_of_n_json.variants.variant0]
type = "chat_completion"
weight = 0
model = "json_beatles_1"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json.variants.variant1]
type = "chat_completion"
weight = 0
model = "json_beatles_2"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant1"]

[functions.mixture_of_n_json.variants.mixture_of_n_variant.fuser]
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.write_haiku]
type = "chat"
user_schema = "../../fixtures/config/functions/write_haiku/user_schema.json"

[functions.write_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/write_haiku/initial_prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/write_haiku/initial_prompt/user_template.minijinja"

[functions.extract_entities]
type = "json"
output_schema = "../../fixtures/config/functions/extract_entities/output_schema.json"

[functions.extract_entities.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/extract_entities/initial_prompt/system_template.minijinja"
json_mode = "strict"

[functions.extract_entities.variants.dummy_error]
type = "chat_completion"
model = "dummy::error"
system_template = "../../fixtures/config/functions/extract_entities/initial_prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json_repeated]
type = "json"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"

[functions.mixture_of_n_json_repeated.variants.variant0]
type = "chat_completion"
weight = 0
model = "dummy::random_answer"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json_repeated.variants.variant1]
type = "chat_completion"
weight = 0
model = "json"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json_repeated.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant0", "variant1"]

[functions.mixture_of_n_json_repeated.variants.mixture_of_n_variant.fuser]
model = "json"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                  METRICS                                   │
# └────────────────────────────────────────────────────────────────────────────┘

[metrics.task_success]
type = "boolean"
optimize = "max"
level = "inference"

[metrics.goal_achieved]
type = "boolean"
optimize = "max"
level = "episode"

[metrics.user_rating]
type = "float"
optimize = "max"
level = "episode"

[metrics.brevity_score]
type = "float"
optimize = "max"
level = "inference"

[metrics.prometheus_test_boolean1]
type = "boolean"
optimize = "max"
level = "inference"

[metrics.prometheus_test_boolean2]
type = "boolean"
optimize = "max"
level = "inference"

[metrics.prometheus_test_float1]
type = "float"
optimize = "max"
level = "inference"

[metrics.prometheus_test_float2]
type = "float"
optimize = "max"
level = "inference"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                  TOOLS                                     │
# └────────────────────────────────────────────────────────────────────────────┘

[tools.get_temperature]
description = "Get the current temperature in a given location"
parameters = "../../fixtures/config/tools/get_temperature.json"

[tools.get_humidity]
description = "Get the current humidity in a given location"
parameters = "../../fixtures/config/tools/get_humidity.json"


# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                EVALUATIONS                                 │
# └────────────────────────────────────────────────────────────────────────────┘

[evaluations.test_evaluation]
type = "static"
function_name = "basic_test"

[evaluations.test_evaluation.evaluators.happy_bool]
type = "llm_judge"
output_type = "boolean"
optimize = "max"

[evaluations.test_evaluation.evaluators.happy_bool.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::true"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"

[evaluations.test_evaluation.evaluators.sad_bool]
type = "llm_judge"
output_type = "boolean"
optimize = "max"

[evaluations.test_evaluation.evaluators.sad_bool.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::false"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"

[evaluations.test_evaluation.evaluators.zero]
type = "llm_judge"
output_type = "float"
optimize = "max"

[evaluations.test_evaluation.evaluators.zero.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::zero"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"

[evaluations.test_evaluation.evaluators.one]
type = "llm_judge"
output_type = "float"
optimize = "max"

[evaluations.test_evaluation.evaluators.one.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::one"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"

[evaluations.json_evaluation]
type = "static"
dataset_name = "test_dataset"
function_name = "json_success"

[evaluations.json_evaluation.evaluators.happy_bool]
type = "llm_judge"
output_type = "boolean"
optimize = "max"

[evaluations.json_evaluation.evaluators.happy_bool.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::true"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"

[evaluations.json_evaluation.evaluators.sad_bool]
type = "llm_judge"
output_type = "boolean"
optimize = "max"

[evaluations.json_evaluation.evaluators.sad_bool.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::false"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"

[evaluations.json_evaluation.evaluators.zero]
type = "llm_judge"
output_type = "float"
optimize = "max"

[evaluations.json_evaluation.evaluators.zero.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::zero"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"

[evaluations.json_evaluation.evaluators.one]
type = "llm_judge"
output_type = "float"
optimize = "max"

[evaluations.json_evaluation.evaluators.one.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::one"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"


[evaluations.entity_extraction]
type = "static"
function_name = "extract_entities"

[evaluations.entity_extraction.evaluators.exact_match]
type = "exact_match"
# expected to fail
cutoff = 0.6

[evaluations.entity_extraction.evaluators.count_sports]
type = "llm_judge"
output_type = "float"
optimize = "min"
include = { reference_output = false }
cutoff = 0.5

[evaluations.entity_extraction.evaluators.count_sports.variants.mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
active = true
system_instructions = "../../fixtures/config/evaluations/entity_extraction/count_sports/system_instructions.txt"
json_mode = "strict"
temperature = 0.1

[evaluations.entity_extraction.evaluators.error]
type = "llm_judge"
output_type = "float"
optimize = "max"

[evaluations.entity_extraction.evaluators.error.variants.dummy]
type = "chat_completion"
model = "dummy::llm_judge::error"
active = true
system_instructions = "../../fixtures/config/evaluations/test_evaluation/llm_judge_bool/system_instructions.txt"
json_mode = "on"

[evaluations.haiku_with_outputs]
type = "static"
function_name = "write_haiku"

[evaluations.haiku_with_outputs.evaluators.exact_match]
type = "exact_match"

[evaluations.haiku_without_outputs]
type = "static"
function_name = "write_haiku"

[evaluations.haiku_without_outputs.evaluators.exact_match]
type = "exact_match"

[evaluations.haiku_without_outputs.evaluators.topic_starts_with_f]
type = "llm_judge"
output_type = "boolean"
optimize = "min"
include = { reference_output = false }
cutoff = 0.5

[evaluations.haiku_without_outputs.evaluators.topic_starts_with_f.variants.mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
active = true
system_instructions = "../../fixtures/config/evaluations/haiku_without_outputs/topic_starts_with_f/system_instructions.txt"
json_mode = "strict"
temperature = 0
