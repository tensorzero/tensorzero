#![allow(clippy::print_stdout)]

use std::cell::Cell;
use std::future::Future;
use std::sync::Arc;

use paste::paste;
use reqwest::Client;
use secrecy::{ExposeSecret, SecretString};
use serde_json::json;
use tensorzero_internal::clickhouse::migration_manager::migration_trait::Migration;
use tracing_test::traced_test;
use uuid::Uuid;

use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0000::Migration0000;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0002::Migration0002;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0003::Migration0003;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0004::Migration0004;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0005::Migration0005;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0006::Migration0006;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0008::Migration0008;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0009::Migration0009;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0011::Migration0011;
use tensorzero_internal::clickhouse::migration_manager::migrations::migration_0013::Migration0013;

use tensorzero_internal::clickhouse::migration_manager::{self, make_all_migrations};
use tensorzero_internal::clickhouse::test_helpers::{get_clickhouse, CLICKHOUSE_URL};
use tensorzero_internal::clickhouse::ClickHouseConnectionInfo;

fn get_clean_clickhouse() -> ClickHouseConnectionInfo {
    let database = format!(
        "tensorzero_e2e_tests_migration_manager_{}",
        Uuid::now_v7().simple()
    );
    let mut clickhouse_url = url::Url::parse(&CLICKHOUSE_URL).unwrap();
    clickhouse_url.set_path("");
    clickhouse_url.set_query(Some(format!("database={database}").as_str()));

    ClickHouseConnectionInfo::Production {
        database_url: SecretString::from(clickhouse_url.to_string()),
        database: database.clone(),
        // This works around an issue with ClickHouse Cloud where long-lived connections
        // are abruptly closed by the server.
        client: Client::builder().pool_max_idle_per_host(0).build().unwrap(),
    }
}

/// A helper macro to work with `#[traced_test]`. We need to generate a new `#[traced_test]`
/// function for each each run of `run_migrations_up_to`, to avoid mixing up the logs.
macro_rules! invoke_all {
    ($target_fn:ident, $migrations:expr, [$($migration_num:literal),*]) => {
        // Verify that the literal array matches the migrations array
        let literal_array = [$($migration_num),*];
        assert_eq!($migrations.len(), literal_array.len(), "The migration indices array must be the same length as the migrations array");
        for i in 0..$migrations.len() {
            assert_eq!(literal_array[i], i, "The migration indices array should be a list of numbers");
        }

        // For each value in the literal array, generate a new `#[traced_test]` function
        // that calls the target function with that value, along with the `logs_contain`
        // helper generated by the `traced_test` macro.
        $(
            paste! {
                #[traced_test]
                async fn [<helper_ $migration_num>] <F: Future<Output = ()>> (mut target_fn: impl FnMut(usize, fn(&str) -> bool) -> F) {
                    target_fn($migration_num, logs_contain).await;
                }

                // Invoke the generated function
                [<helper_ $migration_num>]($target_fn).await;
            }

        )*
    }
}

const MANIFEST_PATH: &str = env!("CARGO_MANIFEST_DIR");

async fn count_table_rows(clickhouse: &ClickHouseConnectionInfo, table: &str) -> u64 {
    clickhouse
        .run_query_synchronous(format!("SELECT count(*) FROM {table}"), None)
        .await
        .unwrap()
        .trim()
        .parse()
        .unwrap()
}

async fn insert_large_fixtures(clickhouse: &ClickHouseConnectionInfo) {
    // Insert data so that we test the migration re-creates the tables properly.
    let s3_fixtures_path: String = format!("{MANIFEST_PATH}/../ui/fixtures/s3-fixtures");

    let ClickHouseConnectionInfo::Production {
        database_url,
        database,
        client: _,
    } = clickhouse
    else {
        panic!("ClickHouseConnectionInfo is not a Production connection");
    };

    let url = url::Url::parse(database_url.expose_secret()).unwrap();
    let mut host = url.host_str().unwrap();
    if host == "localhost" || host == "127.0.0.1" {
        host = "host.docker.internal";
    }
    let username = url.username();
    let password = url.password().unwrap_or("");

    // We use our latest fixtures - new columns will get ignored when inserting.
    for (file, table) in [
        ("large_chat_inference_v2.parquet", "ChatInference"),
        ("large_json_inference.parquet", "JsonInference"),
        (
            "large_chat_boolean_feedback.parquet",
            "BooleanMetricFeedback",
        ),
        ("large_chat_comment_feedback.parquet", "CommentFeedback"),
        (
            "large_chat_demonstration_feedback.parquet",
            "DemonstrationFeedback",
        ),
        ("large_chat_float_feedback.parquet", "FloatMetricFeedback"),
    ] {
        let mut command = tokio::process::Command::new("docker");
        command.args([
            "run",
            "--add-host=host.docker.internal:host-gateway",
            "-v",
            &format!("{s3_fixtures_path}:/s3-fixtures"),
            "clickhouse/clickhouse-server:25.4-alpine",
            "clickhouse-client",
            "--host",
            host,
            "--user",
            username,
            "--password",
            password,
            "--database",
            database,
            "--query",
            &format!(
                r#"
        INSERT INTO {table} FROM INFILE '/s3-fixtures/{file}' FORMAT Parquet
    "#
            ),
        ]);
        assert!(
            command.spawn().unwrap().wait().await.unwrap().success(),
            "Failed to insert {table}"
        );
    }
}

async fn run_migration_0009_with_data<R: Future<Output = bool>, F: FnOnce() -> R>(
    clickhouse: &ClickHouseConnectionInfo,
    run_migration: F,
) -> bool {
    let initial_boolean_feedback_count: u64 =
        count_table_rows(clickhouse, "BooleanMetricFeedback").await;
    let initial_comment_feedback_count: u64 = count_table_rows(clickhouse, "CommentFeedback").await;
    let initial_demonstration_feedback_count: u64 =
        count_table_rows(clickhouse, "DemonstrationFeedback").await;
    let initial_float_metric_feedback_count: u64 =
        count_table_rows(clickhouse, "FloatMetricFeedback").await;

    let clean_start = run_migration().await;

    let final_boolean_feedback_count: u64 =
        count_table_rows(clickhouse, "BooleanMetricFeedback").await;
    let final_comment_feedback_count: u64 = count_table_rows(clickhouse, "CommentFeedback").await;
    let final_demonstration_feedback_count: u64 =
        count_table_rows(clickhouse, "DemonstrationFeedback").await;
    let final_float_metric_feedback_count: u64 =
        count_table_rows(clickhouse, "FloatMetricFeedback").await;

    assert_eq!(initial_boolean_feedback_count, final_boolean_feedback_count);
    assert_eq!(initial_comment_feedback_count, final_comment_feedback_count);
    assert_eq!(
        initial_demonstration_feedback_count,
        final_demonstration_feedback_count
    );
    assert_eq!(
        initial_float_metric_feedback_count,
        final_float_metric_feedback_count
    );

    clean_start
}

async fn run_migration_0020_with_data<R: Future<Output = bool>, F: FnOnce() -> R>(
    clickhouse: &ClickHouseConnectionInfo,
    run_migration: F,
) -> bool {
    // Check that the same number of rows are in the tables before and after the migration
    let initial_chat_count: u64 = count_table_rows(clickhouse, "ChatInference").await;
    let initial_json_count: u64 = count_table_rows(clickhouse, "JsonInference").await;

    let clean_start = run_migration().await;

    let final_chat_count: u64 = count_table_rows(clickhouse, "ChatInference").await;
    let final_json_count: u64 = count_table_rows(clickhouse, "JsonInference").await;
    assert_eq!(
        initial_chat_count, final_chat_count,
        "Lost data from ChatInference"
    );
    assert_eq!(
        initial_json_count, final_json_count,
        "Lost data from JsonInference"
    );

    // Check that existing rows are inserted into InferenceById and InferenceByEpisodeId,
    // and check an individual row from ChatInference and JsonInference.

    let final_inference_by_id_count: u64 =
        count_table_rows(clickhouse, "InferenceById FINAL").await;
    let final_inference_by_episode_id_count: u64 =
        count_table_rows(clickhouse, "InferenceByEpisodeId FINAL").await;

    assert_eq!(
        final_inference_by_id_count,
        final_chat_count + final_json_count,
        "Didn't insert all data into InferenceById"
    );
    assert_eq!(
        final_inference_by_episode_id_count,
        final_chat_count + final_json_count,
        "Didn't insert all data into InferenceByEpisodeId"
    );

    let sample_chat_row = clickhouse
        .run_query_synchronous(
            "SELECT toUInt128(id) as id_uint, toUInt128(episode_id) as episode_id_uint FROM ChatInference LIMIT 1 FORMAT JSONEachRow".to_string(),
            None,
        )
        .await
        .unwrap();

    let sample_chat_row_json = serde_json::from_str::<serde_json::Value>(&sample_chat_row).unwrap();
    let sample_chat_id = sample_chat_row_json["id_uint"].as_str().unwrap();
    let sample_chat_episode_id = sample_chat_row_json["episode_id_uint"].as_str().unwrap();

    println!("Querying for sample chat by id: {sample_chat_id}");

    let matching_chat_by_id = clickhouse
        .run_query_synchronous(
            format!("SELECT id_uint, toUInt128(episode_id) as episode_id_uint FROM InferenceById FINAL WHERE function_type = 'chat' AND id_uint = '{sample_chat_id}' LIMIT 1 FORMAT JSONEachRow"),
            None,
        )
        .await
        .unwrap();

    println!("Matching chat by id: `{matching_chat_by_id}`");

    let matching_chat_by_id_json =
        serde_json::from_str::<serde_json::Value>(&matching_chat_by_id).unwrap();
    assert_eq!(
        matching_chat_by_id_json["episode_id_uint"]
            .as_str()
            .unwrap(),
        sample_chat_episode_id
    );

    let matching_chat_by_episode_id = clickhouse
        .run_query_synchronous(
            format!("SELECT * FROM InferenceByEpisodeId FINAL WHERE function_type = 'chat' AND episode_id_uint = '{sample_chat_episode_id}' LIMIT 1 FORMAT JSONEachRow"),
            None,
        )
        .await
        .unwrap();

    let matching_chat_by_episode_id_json =
        serde_json::from_str::<serde_json::Value>(&matching_chat_by_episode_id).unwrap();
    assert_eq!(
        matching_chat_by_episode_id_json["id_uint"]
            .as_str()
            .unwrap(),
        sample_chat_id
    );

    let sample_json_row = clickhouse
        .run_query_synchronous(
            "SELECT toUInt128(id) as id_uint, toUInt128(episode_id) as episode_id_uint FROM JsonInference LIMIT 1 FORMAT JSONEachRow".to_string(),
            None,
        )
        .await
        .unwrap();

    let sample_json_row_json = serde_json::from_str::<serde_json::Value>(&sample_json_row).unwrap();
    let sample_json_id = sample_json_row_json["id_uint"].as_str().unwrap();
    let sample_json_episode_id = sample_json_row_json["episode_id_uint"].as_str().unwrap();

    let matching_json_by_id = clickhouse
        .run_query_synchronous(
            format!("SELECT id_uint, toUInt128(episode_id) as episode_id_uint FROM InferenceById FINAL WHERE function_type = 'json' AND id_uint = '{sample_json_id}' LIMIT 1 FORMAT JSONEachRow"),
            None,
        )
        .await
        .unwrap();

    let matching_json_by_id_json =
        serde_json::from_str::<serde_json::Value>(&matching_json_by_id).unwrap();
    assert_eq!(
        matching_json_by_id_json["episode_id_uint"]
            .as_str()
            .unwrap(),
        sample_json_episode_id
    );

    let matching_json_by_episode_id = clickhouse
        .run_query_synchronous(
            format!("SELECT * FROM InferenceByEpisodeId FINAL WHERE function_type = 'json' AND episode_id_uint = '{sample_json_episode_id}' LIMIT 1 FORMAT JSONEachRow"),
            None,
        )
        .await
        .unwrap();

    let matching_json_by_episode_id_json =
        serde_json::from_str::<serde_json::Value>(&matching_json_by_episode_id).unwrap();
    assert_eq!(
        matching_json_by_episode_id_json["id_uint"]
            .as_str()
            .unwrap(),
        sample_json_id
    );
    clean_start
}

#[tokio::test]
async fn test_clickhouse_migration_manager() {
    let clickhouse = get_clean_clickhouse();
    clickhouse.create_database().await.unwrap();
    // Run it twice to test that it is a no-op the second time
    clickhouse.create_database().await.unwrap();
    let migrations = make_all_migrations(&clickhouse);
    let initial_clean_start = Cell::new(true);
    // This runs all migrations up to and including the given migration number,
    // verifying that only the most recent migration is actually applied.
    let run_migrations_up_to = |migration_num: usize, logs_contain: fn(&str) -> bool| {
        let migrations = &migrations;
        let clickhouse = &clickhouse;
        let initial_clean_start = &initial_clean_start;
        async move {
            // All of the previous migrations should have already been run
            for (i, migration) in migrations.iter().enumerate().take(migration_num) {
                let clean_start = migration_manager::run_migration(migration.as_ref(), false)
                    .await
                    .unwrap();
                if i == 0 {
                    // We know that the first migration was run in a previous test, so clean start should be false
                    assert!(!clean_start);
                }
                let name = migrations[i].name();
                assert!(
                    !logs_contain(&format!("Applying migration: {name}")),
                    "Migration {name} should not have been applied"
                );
                assert!(
                    !logs_contain(&format!("Migration succeeded: {name}")),
                    "Migration {name} should not have succeeded (because it wasn't applied)"
                );
            }

            let run_migration = || async {
                migration_manager::run_migration(
                    migrations[migration_num].as_ref(),
                    initial_clean_start.get(),
                )
                .await
                .unwrap()
            };

            // The latest migration should get applied, since we haven't run it before
            let name = migrations[migration_num].name();

            let clean_start = match name.as_str() {
                "Migration0009" => {
                    // Insert our large fixtures - these will be used for Migration0009 all subsequent migrations
                    insert_large_fixtures(clickhouse).await;
                    // We're going to be inserting data into the tables, so run all subsequent
                    // migrations in non-clean-start mode.
                    initial_clean_start.set(false);
                    run_migration_0009_with_data(clickhouse, run_migration).await
                }
                "Migration0020" => run_migration_0020_with_data(clickhouse, run_migration).await,
                _ => run_migration().await,
            };

            if migration_num == 0 {
                // When running for the first time, we should have a clean start.
                assert!(clean_start);
            }
            // Migration 0029 will not be applied since 0023 is turned off.
            if name != "Migration0029" {
                assert!(logs_contain(&format!("Applying migration: {name}")));
                assert!(logs_contain(&format!("Migration succeeded: {name}")));
            }
            assert!(!logs_contain("Failed to apply migration"));
            assert!(!logs_contain("Failed migration success check"));
            assert!(!logs_contain("Failed to verify migration"));
            assert!(!logs_contain("ERROR"));
        }
    };

    #[traced_test]
    async fn run_all(migrations: &[Box<dyn Migration + Send + Sync + '_>]) {
        // Now, run all of the migrations, and verify that none of them apply
        for (i, migration) in migrations.iter().enumerate() {
            let clean_start = migration_manager::run_migration(migration.as_ref(), true)
                .await
                .unwrap();
            if i == 0 {
                // We know that the first migration was run in a previous test, so clean start should be false
                assert!(!clean_start);
            }
            let name = migrations[i].name();
            assert!(!logs_contain(&format!("Applying migration: {name}")));
            assert!(!logs_contain(&format!("Migration succeeded: {name}")));
        }

        assert!(!logs_contain("Failed to apply migration"));
        assert!(!logs_contain("Failed migration success check"));
        assert!(!logs_contain("Failed to verify migration"));
        assert!(!logs_contain("ERROR"));
    }

    invoke_all!(
        run_migrations_up_to,
        &migrations,
        // This array must match the length of 'migrations' - the macro
        // will throw an error if it doesn't.
        // This must be an array literal, so that the macro can generate a function
        // for each element in the array.
        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]
    );
    run_all(&migrations).await;

    let database = clickhouse.database();
    tracing::info!("Attempting to drop test database: {database}");

    clickhouse
        .run_query_synchronous(format!("DROP DATABASE {database}"), None)
        .await
        .unwrap();
}

#[tokio::test]
async fn test_bad_clickhouse_write() {
    let clickhouse = get_clickhouse().await;
    // "name" should be "metric_name" here but we are using the wrong field on purpose to check that the write fails
    let payload =
        json!({"target_id": Uuid::now_v7(), "value": true, "name": "test", "id": Uuid::now_v7()});
    let err = clickhouse
        .write(&[payload], "BooleanMetricFeedback")
        .await
        .unwrap_err();
    assert!(
        err.to_string()
            .contains("Unknown field found while parsing JSONEachRow format: name"),
        "Unexpected error: {err}"
    );
}

#[tokio::test]
async fn test_clean_clickhouse_start() {
    let clickhouse = get_clean_clickhouse();
    migration_manager::run(&clickhouse).await.unwrap();
}

#[tokio::test]
async fn test_concurrent_clickhouse_migrations() {
    let clickhouse = Arc::new(get_clean_clickhouse());
    let num_concurrent_starts = 50;
    let mut handles = Vec::with_capacity(num_concurrent_starts);
    for _ in 0..num_concurrent_starts {
        let clickhouse_clone = clickhouse.clone();
        handles.push(tokio::spawn(async move {
            migration_manager::run(&clickhouse_clone).await.unwrap();
        }));
    }
    for handle in handles {
        handle.await.unwrap();
    }
}

/// Migration 0013 has some checks that enforce that concurrent migrations can't break
/// the database.
/// This test enforces that the migration will error if there would be an invalid database state
/// rather than brick the database.
#[tokio::test]
async fn test_migration_0013_old_table() {
    let clickhouse = get_clean_clickhouse();
    clickhouse.create_database().await.unwrap();

    // When creating a new migration, add it to the end of this array,
    // and adjust the call to `invoke_all!` to include the new array index.
    let migrations: [Box<dyn Migration + '_>; 9] = [
        Box::new(Migration0000 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0002 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0003 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0004 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0005 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0006 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0008 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0009 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0011 {
            clickhouse: &clickhouse,
        }),
    ];

    // Run migrations up to right before 0013
    for migration in migrations.iter() {
        migration_manager::run_migration(migration.as_ref(), true)
            .await
            .unwrap();
    }
    // Manually create a table that should not exist
    let query = r#"
        CREATE TABLE IF NOT EXISTS InferenceById
        (
            id UUID, -- must be a UUIDv7
            function_name LowCardinality(String),
            variant_name LowCardinality(String),
            episode_id UUID, -- must be a UUIDv7,
            function_type Enum('chat' = 1, 'json' = 2)
        ) ENGINE = MergeTree()
        ORDER BY id;
    "#;
    let _ = clickhouse
        .run_query_synchronous(query.to_string(), None)
        .await
        .unwrap();
    let err = migration_manager::run_migration(
        &Migration0013 {
            clickhouse: &clickhouse,
        },
        false,
    )
    .await
    .unwrap_err();
    assert!(
        err.to_string().contains("InferenceById table is in an invalid state. Please contact TensorZero team.") ||
        err.to_string().contains("SELECT query outputs column with name 'id_uint', which is not found in the target table."),
        "Unexpected error: {err}",
    );
}

/// For this test, we will run all the migrations up to 0011, add some data to
/// the JSONInference table, then run migration 0013.
/// This should fail.
#[tokio::test]
async fn test_migration_0013_data_no_table() {
    let clickhouse = get_clean_clickhouse();
    clickhouse.create_database().await.unwrap();

    // When creating a new migration, add it to the end of this array,
    // and adjust the call to `invoke_all!` to include the new array index.
    let migrations: [Box<dyn Migration + '_>; 9] = [
        Box::new(Migration0000 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0002 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0003 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0004 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0005 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0006 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0008 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0009 {
            clickhouse: &clickhouse,
        }),
        Box::new(Migration0011 {
            clickhouse: &clickhouse,
        }),
    ];

    // Run migrations up to right before 0013
    for migration in migrations.iter() {
        migration_manager::run_migration(migration.as_ref(), true)
            .await
            .unwrap();
    }

    // Add a row to the JsonInference table (would be very odd to have data in this table
    // but not an InferenceById table).
    let query = r#"
        INSERT INTO JsonInference (id, function_name, variant_name, episode_id, input, output, output_schema, inference_params, processing_time_ms)
        VALUES (generateUUIDv7(), 'test_function', 'test_variant', generateUUIDv7(), 'input', 'output', 'output_schema', 'params', 100)
    "#;
    let _ = clickhouse
        .run_query_synchronous(query.to_string(), None)
        .await
        .unwrap();
    let err = migration_manager::run_migration(
        &Migration0013 {
            clickhouse: &clickhouse,
        },
        false,
    )
    .await
    .unwrap_err();
    assert!(err.to_string()
        .contains("Data already exists in the ChatInference or JsonInference tables and InferenceById or InferenceByEpisodeId is missing. Please contact TensorZero team"));
}
