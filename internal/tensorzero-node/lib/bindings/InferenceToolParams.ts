// This file was generated by [ts-rs](https://github.com/Aleph-Alpha/ts-rs). Do not edit this file manually.
import type { InferenceParams } from "./InferenceParams";
import type { Input } from "./Input";
import type { ProviderTool } from "./ProviderTool";
import type { Tool } from "./Tool";
import type { ToolChoice } from "./ToolChoice";
import type { JsonValue } from "./serde_json/JsonValue";

/**
 * Parameters for the inference tool (visible to LLM).
 */
export type InferenceToolParams = {
  /**
   * The function name to call. Exactly one of function_name or model_name required.
   */
  function_name: string | null;
  /**
   * Model name shorthand (e.g., "openai::gpt-4"). Alternative to function_name.
   */
  model_name: string | null;
  /**
   * The input for inference.
   */
  input: Input;
  /**
   * Inference parameters (temperature, max_tokens, etc.).
   */
  params: InferenceParams;
  /**
   * Pin a specific variant (optional, normally let API select).
   */
  variant_name: string | null;
  /**
   * Output schema override (for JSON functions).
   */
  output_schema: JsonValue | null;
  /**
   * A subset of static tools configured for the function that the inference is allowed to use. Optional.
   * If not provided, all static tools are allowed.
   */
  allowed_tools?: Array<string>;
  /**
   * Tools that the user provided at inference time (not in function config), in addition to the function-configured
   * tools, that are also allowed.
   */
  additional_tools?: Array<Tool>;
  /**
   * User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
   * Optional.
   */
  tool_choice?: ToolChoice;
  /**
   * Whether to use parallel tool calls in the inference. Optional.
   * If provided during inference, it will override the function-configured parallel tool calls.
   */
  parallel_tool_calls?: boolean;
  /**
   * Provider-specific tool configurations
   */
  provider_tools: Array<ProviderTool>;
};
