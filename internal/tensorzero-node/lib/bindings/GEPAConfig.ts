// This file was generated by [ts-rs](https://github.com/Aleph-Alpha/ts-rs). Do not edit this file manually.
import type { RetryConfig } from "./RetryConfig";

/**
 * GEPA (Genetic Evolution with Pareto Analysis) optimization configuration
 *
 * GEPA is a multi-objective optimization algorithm that maintains a Pareto frontier
 * of high-performing variants. It uses genetic programming techniques to evolve
 * prompt templates based on evaluation results.
 */
export type GEPAConfig = {
  /**
   * Name of the function being optimized
   */
  function_name: string;
  /**
   * Name of the evaluation used to score candidate variants
   */
  evaluation_name: string;
  /**
   * Optional list of variant_names to initialize GEPA with.
   * If None, will use all variants defined for the function.
   */
  initial_variants: Array<string> | null;
  /**
   * Prefix for the name of the new optimized variants
   */
  variant_prefix: string | null;
  /**
   * Number of training samples to analyze per iteration
   */
  batch_size: number;
  /**
   * Maximum number of training iterations
   */
  max_iterations: number;
  /**
   * Maximum number of concurrent inference calls
   */
  max_concurrency: number;
  /**
   * Model for analysis/prediction (e.g., "openai::gpt-5-mini")
   */
  analysis_model: string;
  /**
   * Model for mutation (e.g., "openai::gpt-5")
   */
  mutation_model: string;
  /**
   * Optional random seed for reproducibility
   */
  seed: number | null;
  /**
   * Client timeout in seconds for TensorZero gateway operations
   */
  timeout: bigint;
  /**
   * Whether to include inference input in InferenceWithAnalysis for mutation
   *
   * If true, the mutate function will see the inference input in addition to the output and analysis for each example in the batch.
   * This provides additional context but increases token usage significantly.
   *
   * **Warning:** Use with caution, especially with:
   * - Multi-turn conversations (many input messages)
   * - Large batch sizes (many analyses per mutation)
   *
   * These can cause context length overflow for the mutation model.
   */
  include_inference_input_for_mutation: boolean;
  /**
   * Retry configuration for inference calls during GEPA optimization
   * Applies to analyze function calls, mutate function calls, and all mutated variants
   */
  retries: RetryConfig;
  /**
   * Maximum number of tokens to generate for analysis and mutation model calls
   * Default: 16_384
   */
  max_tokens: number;
};
