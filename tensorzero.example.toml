#  NOTE: This is an example configuration file for TensorZero.
#        You can use this file as a reference for your own configuration by adding
#        your own models, functions, and metrics.

# ┌──────────────────────────────────────────────────────────────────────────────┐
# │                                   GENERAL                                    │
# └──────────────────────────────────────────────────────────────────────────────┘

# TODO: not used yet

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                   MODELS                                   │
# └────────────────────────────────────────────────────────────────────────────┘

[models."gpt-3.5-turbo"]
routing = ["openai", "azure"]

[models."gpt-3.5-turbo".providers.openai]
type = "openai"
name = "gpt-3.5-turbo"

[models."gpt-3.5-turbo".providers.azure]
type = "openai"
name = "gpt-35-turbo"
api_base = "https://your-endpoint.openai.azure.com/"

[models.claude-3-haiku-20240307]
routing = ["anthropic"]

[models.claude-3-haiku-20240307.providers.anthropic]
type = "anthropic"
name = "claude-3-haiku-20240307"

# ┌──────────────────────────────────────────────────────────────────────────────┐
# │                                  FUNCTIONS                                   │
# └──────────────────────────────────────────────────────────────────────────────┘

[functions.generate_draft]
type = "chat"  # "chat", "tool"
system_schema = "to/do.json"
output_schema = "to/do.json"
# optional: tool.parallel_tool_calls, system_schema, user_schema, assistant_schema, output_schema

[functions.generate_draft.variants.openai_promptA]
weight = 0.9
generation.model = "gpt-3.5-turbo"
generation.system_template = "to/do/promptA/system.jinja"
# optional: generation.user_template, generation.assistant_template, generation.function_call, generation.json_mode, generation.temperature, etc.

[functions.generate_draft.variants.openai_promptB]
weight = 0.1
generation.model = "gpt-3.5-turbo"
generation.system_template = "to/do/promptB/system.jinja"

# ┌──────────────────────────────────────────────────────────────────────────────┐
# │                                   METRICS                                    │
# └──────────────────────────────────────────────────────────────────────────────┘

[metrics.task_success]
type = "boolean"  # "boolean", "float"
optimize = "max"  # "min", "max"
level = "inference"  # "inference", "episode"

[metrics.user_rating]
type = "float"
optimize = "max"
level = "episode"
