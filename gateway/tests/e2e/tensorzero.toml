# NOTE: This is an example configuration file for TensorZero (used for E2E tests).
#       You can use this file as a reference for your own configuration by adding
#       your own models, functions, and metrics.

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                  GENERAL                                   │
# └────────────────────────────────────────────────────────────────────────────┘

[gateway]
bind_address = "0.0.0.0:3000"

[clickhouse]
database = "tensorzero_e2e_tests"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                   MODELS                                   │
# └────────────────────────────────────────────────────────────────────────────┘

[models."gpt-4o-mini-2024-07-18"]
routing = ["openai"]

[models."gpt-4o-mini-2024-07-18".providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"

[models."gpt-4o-mini-azure"]
routing = ["azure"]

[models."gpt-4o-mini-azure".providers.azure]
type = "azure"
deployment_id = "gpt4o-mini-20240718"
endpoint = "https://t0-azure-openai-east.openai.azure.com"

[models.claude-3-haiku-20240307]
routing = ["anthropic", "aws-bedrock"]

[models.claude-3-haiku-20240307.providers.anthropic]
type = "anthropic"
model_name = "claude-3-haiku-20240307"

[models.claude-3-haiku-20240307.providers.aws-bedrock]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"

[models.claude-3-haiku-20240307-us-east-1]
routing = ["aws-bedrock-us-east-1"]

[models.claude-3-haiku-20240307-us-east-1.providers.aws-bedrock-us-east-1]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"
region = "us-east-1"

[models.claude-3-haiku-20240307-uk-hogwarts-1]
routing = ["aws-bedrock-uk-hogwarts-1"]

[models.claude-3-haiku-20240307-uk-hogwarts-1.providers.aws-bedrock-uk-hogwarts-1]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"
region = "uk-hogwarts-1"

# Duplicate so that we can test just Anthropic no fallbacks
[models.claude-3-haiku-20240307-anthropic]
routing = ["anthropic"]

[models.claude-3-haiku-20240307-anthropic.providers.anthropic]
type = "anthropic"
model_name = "claude-3-haiku-20240307"

# Duplicate so that we can test just AWS Bedrock no fallbacks
[models.claude-3-haiku-20240307-aws-bedrock]
routing = ["aws-bedrock"]

[models.claude-3-haiku-20240307-aws-bedrock.providers.aws-bedrock]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"

[models."gemini-1.5-flash-001"]
routing = ["gcp_vertex_gemini"]

[models."gemini-1.5-flash-001".providers.gcp_vertex_gemini]
type = "gcp_vertex_gemini"
model_id = "gemini-1.5-flash-001"
location = "us-central1"
project_id = "tensorzero-public"

[models."gemini-1.5-pro-001"]
routing = ["gcp_vertex_gemini"]

[models."gemini-1.5-pro-001".providers.gcp_vertex_gemini]
type = "gcp_vertex_gemini"
model_id = "gemini-1.5-pro-001"
location = "us-central1"
project_id = "tensorzero-public"

[models."llama3.1-8b-instruct-fireworks"]
routing = ["fireworks"]

[models."llama3.1-8b-instruct-fireworks".providers.fireworks]
type = "fireworks"
model_name = "accounts/fireworks/models/llama-v3p1-8b-instruct"

[models.firefunction-v2]
routing = ["fireworks"]

[models.firefunction-v2.providers.fireworks]
type = "fireworks"
model_name = "accounts/fireworks/models/firefunction-v2"

[models."llama3.1-8b-instruct-together"]
routing = ["together"]

[models."llama3.1-8b-instruct-together".providers.together]
type = "together"
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"

[models."mixtral-8x7b-instruct-v0.1"]
routing = ["together"]

[models."mixtral-8x7b-instruct-v0.1".providers.together]
type = "together"
model_name = "mistralai/Mixtral-8x7B-Instruct-v0.1"

[models."microsoft/Phi-3.5-mini-instruct"]
routing = ["vllm"]

[models."microsoft/Phi-3.5-mini-instruct".providers.vllm]
type = "vllm"
model_name = "microsoft/Phi-3.5-mini-instruct"
api_base = "https://pun1owldydhycl-8000.proxy.runpod.net/v1/"

[models."open-mistral-nemo-2407"]
routing = ["mistral"]

[models."open-mistral-nemo-2407".providers.mistral]
type = "mistral"
model_name = "open-mistral-nemo-2407"

[models.test]
routing = ["good"]

[models.test.providers.good]
type = "dummy"
model_name = "good"

[models.error]
routing = ["error"]

[models.error.providers.error]
type = "dummy"
model_name = "error"

[models.test_fallback]
routing = ["error", "good"]

[models.test_fallback.providers.error]
type = "dummy"
model_name = "error"

[models.test_fallback.providers.good]
type = "dummy"
model_name = "good"

[models.json]
routing = ["json"]

[models.json.providers.json]
type = "dummy"
model_name = "json"

[models.tool]
routing = ["tool"]

[models.tool.providers.tool]
type = "dummy"
model_name = "tool"

[models.bad_tool]
routing = ["bad_tool"]

[models.bad_tool.providers.bad_tool]
type = "dummy"
model_name = "bad_tool"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                 FUNCTIONS                                  │
# └────────────────────────────────────────────────────────────────────────────┘

[functions.basic_test]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"

[functions.basic_test.variants.test]
type = "chat_completion"
weight = 1
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
temperature = 1.0
max_tokens = 100
seed = 69

[functions.basic_test.variants.anthropic]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock-us-east-1]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-us-east-1"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock-uk-hogwarts-1]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-uk-hogwarts-1"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.azure]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.fireworks]
type = "chat_completion"
weight = 0
model = "llama3.1-8b-instruct-fireworks"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.fireworks-firefunction]
type = "chat_completion"
weight = 0
model = "firefunction-v2"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
weight = 0
model = "gemini-1.5-flash-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
weight = 0
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.mistral]
type = "chat_completion"
weight = 0
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.openai]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.together]
type = "chat_completion"
weight = 0
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.vllm]
type = "chat_completion"
weight = 0
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.model_fallback_test]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"

[functions.model_fallback_test.variants.test]
type = "chat_completion"
weight = 1
model = "test_fallback"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.json_fail]
type = "json"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../fixtures/config/functions/basic_test/output_schema.json"

[functions.json_fail.variants.test]
type = "chat_completion"
weight = 1
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.json_success]
type = "json"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../fixtures/config/functions/basic_test/output_schema.json"

[functions.json_success.variants.test]
type = "chat_completion"
weight = 1
model = "json"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
max_tokens = 100

[functions.json_success.variants.anthropic]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.anthropic-implicit]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.aws-bedrock]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.aws-bedrock-implicit]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.azure]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.azure-implicit]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.fireworks]
type = "chat_completion"
weight = 0
model = "llama3.1-8b-instruct-fireworks"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.fireworks-implicit]
type = "chat_completion"
weight = 0
model = "firefunction-v2"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
weight = 0
model = "gemini-1.5-flash-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-flash-implicit]
type = "chat_completion"
weight = 0
model = "gemini-1.5-flash-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
weight = 0
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-pro-implicit]
type = "chat_completion"
weight = 0
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.mistral]
type = "chat_completion"
weight = 0
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.openai]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.openai-implicit]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.openai-strict]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.json_success.variants.together]
type = "chat_completion"
weight = 0
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.together-implicit]
type = "chat_completion"
weight = 0
model = "llama3.1-8b-instruct-together"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.vllm]
type = "chat_completion"
weight = 0
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.vllm-implicit]
type = "chat_completion"
weight = 0
model = "microsoft/Phi-3.5-mini-instruct"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.variant_failover]
type = "chat"
system_schema = "../../fixtures/config/functions/basic_test/system_schema.json"
user_schema = "../../fixtures/config/functions/variant_failover/user_schema.json"

[functions.variant_failover.variants.good]
type = "chat_completion"
weight = 0.5
model = "test"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover.variants.error]
type = "chat_completion"
weight = 0.5
model = "error"
system_template = "../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.prometheus_test1]
type = "chat"

[functions.prometheus_test1.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test2]
type = "chat"

[functions.prometheus_test2.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test3]
type = "chat"

[functions.prometheus_test3.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test4]
type = "chat"

[functions.prometheus_test4.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.weather_helper]
type = "chat"
system_schema = "../../fixtures/config/functions/weather_helper/system_schema.json"
tools = ["get_temperature"]
tool_choice = "auto"

[functions.weather_helper.variants.variant]
type = "chat_completion"
weight = 1
model = "tool"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.bad_tool]
type = "chat_completion"
weight = 0
model = "bad_tool"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.aws-bedrock]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.anthropic]
type = "chat_completion"
weight = 0
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.azure]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-azure"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.fireworks-firefunction]
type = "chat_completion"
weight = 0
model = "firefunction-v2"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
weight = 0
model = "gemini-1.5-flash-001"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
weight = 0
model = "gemini-1.5-pro-001"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.mistral]
type = "chat_completion"
weight = 0
model = "open-mistral-nemo-2407"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.openai]
type = "chat_completion"
weight = 0
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.together]
type = "chat_completion"
weight = 0
model = "mixtral-8x7b-instruct-v0.1"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper_parallel]
type = "chat"
system_schema = "../../fixtures/config/functions/weather_helper/system_schema.json"
tools = ["get_temperature", "get_humidity"]
tool_choice = "auto"
# We use an inference-time parameter to set `parallel_tool_calls = true` for the test

[functions.weather_helper_parallel.variants.openai]
type = "chat_completion"
weight = 1
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                  METRICS                                   │
# └────────────────────────────────────────────────────────────────────────────┘

[metrics.task_success]
type = "boolean"
optimize = "max"
level = "inference"

[metrics.goal_achieved]
type = "boolean"
optimize = "max"
level = "episode"

[metrics.user_rating]
type = "float"
optimize = "max"
level = "episode"

[metrics.brevity_score]
type = "float"
optimize = "max"
level = "inference"

[metrics.prometheus_test_boolean1]
type = "boolean"
optimize = "max"
level = "inference"

[metrics.prometheus_test_boolean2]
type = "boolean"
optimize = "max"
level = "inference"

[metrics.prometheus_test_float1]
type = "float"
optimize = "max"
level = "inference"

[metrics.prometheus_test_float2]
type = "float"
optimize = "max"
level = "inference"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                  TOOLS                                     │
# └────────────────────────────────────────────────────────────────────────────┘

[tools.get_temperature]
description = "Get the current temperature in a given location"
parameters = "../../fixtures/config/tools/get_temperature.json"

[tools.get_humidity]
description = "Get the current humidity in a given location"
parameters = "../../fixtures/config/tools/get_humidity.json"
