{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from clickhouse_driver import Client\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLICKHOUSE_URL = os.getenv(\"CLICKHOUSE_URL\")\n",
    "\n",
    "# Example: \"http://localhost:8123/?database=tensorzero\" (\"https://user:password@host:port/?database=database\")\n",
    "assert CLICKHOUSE_URL is not None, \"CLICKHOUSE_URL is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: \"clickhouse://localhost:9000/tensorzero\"\n",
    "CLICKHOUSE_NATIVE_URL = os.getenv(\"CLICKHOUSE_NATIVE_URL\")\n",
    "\n",
    "assert CLICKHOUSE_NATIVE_URL is not None, \"CLICKHOUSE_NATIVE_URL is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = AsyncTensorZeroGateway(\"http://localhost:3000\", timeout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"conllpp.csv\")\n",
    "df.head()\n",
    "df.output = df.output.apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"split\"] == 0]\n",
    "val_df = df[df[\"split\"] == 1]\n",
    "test_df = df[df[\"split\"] == 2]\n",
    "\n",
    "# Shuffle the splits\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df = val_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines the function that we'll actually use to extract entities from text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retry the inference in case of a timeout and hide this in an inner function\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(multiplier=1, max=10))\n",
    "async def _get_entities(\n",
    "    text: str,\n",
    "    client: AsyncTensorZeroGateway,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> InferenceResponse:\n",
    "    return await client.inference(\n",
    "        function_name=\"extract_entities\",\n",
    "        input={\"messages\": [{\"role\": \"user\", \"content\": text}]},\n",
    "        dryrun=dryrun,\n",
    "        variant_name=variant_name,\n",
    "    )\n",
    "\n",
    "\n",
    "# Call this function to get the entities from the text\n",
    "async def get_entities(\n",
    "    text: str,\n",
    "    client: AsyncTensorZeroGateway,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> Optional[InferenceResponse]:\n",
    "    try:\n",
    "        return await _get_entities(text, client, variant_name, dryrun)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two code blocks we define two methods of evaluating the performance of an NER model: Exact Match and Jaccard Similarity.\n",
    "We will use these metrics to evaluate the performance of each variant of our model. Our Jaccard similarity metric gives partial credit and is more lenient than exact match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, List[str]]) -> List[str]:\n",
    "    res = []\n",
    "    for k, v in d.items():\n",
    "        assert isinstance(v, list)\n",
    "        for elt in v:\n",
    "            res.append(f\"__{k.upper()}__::{elt}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match between the predicted and gold entities (the sharpest metric we use to evaluate NER)\n",
    "def exact_match(predicted: Dict[str, List[str]], gold: Dict[str, List[str]]) -> bool:\n",
    "    return set(flatten_dict(predicted)) == set(flatten_dict(gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity between the predicted and gold entities\n",
    "# (a more lenient metric that gives partial credit for correct entities)\n",
    "# NOTE: This is a different implementation from the original code by Predibase, so the metrics won't be directly comparable.\n",
    "def jaccard_similarity(\n",
    "    predicted: Dict[str, List[str]], gold: Dict[str, List[str]]\n",
    ") -> float:\n",
    "    target_entities = flatten_dict(gold)\n",
    "    pred_entities = flatten_dict(predicted)\n",
    "    target_count = Counter(target_entities)\n",
    "    pred_count = Counter(pred_entities)\n",
    "    num = 0\n",
    "    den = 0\n",
    "    all_keys = set(target_entities).union(set(pred_entities))\n",
    "    for key in all_keys:\n",
    "        num += min(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "        den += max(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "    if den == 0:\n",
    "        return 1\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run inference using TensorZero on the training set to collect data for training future variants. We will evaluate the predictions on the training set and send the feedback to TensorZero as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these to run inference on more or fewer examples\n",
    "# or to respect your provider's rate limits\n",
    "NUM_TRAIN_PREDICTIONS = 1000\n",
    "MAX_CONCURRENT_REQUESTS = 100\n",
    "\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "\n",
    "async def make_inference(text: str, client: AsyncTensorZeroGateway):\n",
    "    async with semaphore:\n",
    "        return await get_entities(text, client)\n",
    "\n",
    "\n",
    "# We run inference in parallel to speed things up\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        make_inference(text, tensorzero_client)\n",
    "        for text in train_df[\"input\"][:NUM_TRAIN_PREDICTIONS]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(response: Optional[InferenceResponse], gold_data: Dict[str, List[str]]):\n",
    "    predicted = response.output.parsed if response else None\n",
    "    valid_json = predicted is not None\n",
    "    matched = exact_match(predicted, gold_data) if predicted else False\n",
    "    jaccard = jaccard_similarity(predicted, gold_data) if predicted else 0\n",
    "    return valid_json, matched, jaccard\n",
    "\n",
    "\n",
    "async def evaluate_send_feedback(\n",
    "    response: Optional[InferenceResponse], gold_data: Dict[str, List[str]]\n",
    "):\n",
    "    valid_json, matched, jaccard = evaluate(response, gold_data)\n",
    "    async with semaphore:\n",
    "        await asyncio.gather(\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"valid_json\",\n",
    "                value=valid_json,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"exact_match\",\n",
    "                value=matched,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"jaccard_similarity\",\n",
    "                value=jaccard,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the predictions on the training set and send the feedback to TensorZero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        evaluate_send_feedback(response, gold)\n",
    "        for response, gold in zip(responses, train_df[\"output\"][:NUM_TRAIN_PREDICTIONS])\n",
    "        if response is not None\n",
    "    ]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected data on the training set, we can query the database to see how well each variant performed.\n",
    "First, we'll check the exact match metric for each variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_client = Client.from_url(CLICKHOUSE_NATIVE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"exact_match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference ID\n",
    "df = clickhouse_client.query_dataframe(\n",
    "    \"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    b.value\n",
    "FROM \n",
    "    Inference i\n",
    "JOIN \n",
    "    BooleanMetricFeedback b ON i.id = b.target_id\n",
    "WHERE \n",
    "    i.function_name = 'extract_entities'\n",
    "    AND b.metric_name = %(metric_name)s\"\"\",\n",
    "    {\"metric_name\": metric_name},\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average score for each variant\n",
    "df.groupby(\"variant_name\")[\"value\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll check the jaccard similarity metric for each variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference ID\n",
    "df = clickhouse_client.query_dataframe(\n",
    "    \"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    f.value\n",
    "FROM \n",
    "    Inference i\n",
    "JOIN \n",
    "    FloatMetricFeedback f ON i.id = f.target_id\n",
    "WHERE \n",
    "    i.function_name = 'extract_entities'\n",
    "    AND f.metric_name = 'jaccard_similarity'\"\"\",\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average score for each variant\n",
    "df.groupby(\"variant_name\")[\"value\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_to_evaluate = df[\"variant_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll evaluate each variant on the test set. In order to be sure that we're not leaking any data, we'll use the `dryrun` flag to make sure the test set is not leaked here.\n",
    "\n",
    "We will also \"pin\" the `variant_name` for each inference request to ensure that we're evaluating the same variant across all requests for a fair trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_PREDICTIONS = 500\n",
    "test_set = test_df.iloc[:NUM_TEST_PREDICTIONS]\n",
    "\n",
    "\n",
    "async def make_inference(\n",
    "    text: str, client: AsyncTensorZeroGateway, variant_name: Optional[str] = None\n",
    "):\n",
    "    async with semaphore:\n",
    "        # We use dryrun=True to make sure the test set is not leaked here\n",
    "        return await get_entities(text, client, variant_name=variant_name, dryrun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_responses = []\n",
    "for variant_name in variants_to_evaluate:\n",
    "    variant_task = tqdm_asyncio.gather(\n",
    "        *[\n",
    "            make_inference(text, tensorzero_client, variant_name=variant_name)\n",
    "            for text in test_set[\"input\"]\n",
    "        ],\n",
    "        desc=f\"Evaluating variant: {variant_name}\",\n",
    "    )\n",
    "    variant_result = await variant_task\n",
    "    variant_responses.append(variant_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll evaluate the performance of each variant on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variant_name, responses in zip(variants_to_evaluate, variant_responses):\n",
    "    jaccards = []\n",
    "    well_formed_jsons = []\n",
    "    exact_matches = []\n",
    "    print(f\"Evaluating variant: {variant_name}\")\n",
    "    print(f\"Number of responses: {len(responses)}\")\n",
    "    for response, gold in zip(responses, test_set[\"output\"]):\n",
    "        valid_json, matched, jaccard = evaluate(response, gold)\n",
    "        jaccards.append(jaccard)\n",
    "        well_formed_jsons.append(valid_json)\n",
    "        exact_matches.append(matched)\n",
    "    print(\n",
    "        f\"Average Well-formed JSON: {sum(well_formed_jsons) / len(well_formed_jsons):.1%}\"\n",
    "    )\n",
    "    print(f\"Average Jaccard Similarity: {sum(jaccards) / len(jaccards):.1%}\")\n",
    "    print(f\"Average Exact Match: {sum(exact_matches) / len(exact_matches):.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should fine-tune a custom Llama 3 8B model using the feedback collected from the training set.\n",
    "We have a recipe available in `recipes/supervised_fine_tuning/fireworks` that you can use to fine-tune a Llama 3 8B model.\n",
    "Once you've fine-tuned your model, you can deploy it and re-run inference on the test set to see how well it performs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
