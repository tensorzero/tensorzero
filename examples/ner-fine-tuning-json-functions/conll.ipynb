{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from clickhouse_driver import Client\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLICKHOUSE_NATIVE_URL = \"clickhouse://localhost:9000/tensorzero\"  # update if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = AsyncTensorZeroGateway(\"http://localhost:3000\", timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"conllpp.csv\")\n",
    "df.head()\n",
    "df.output = df.output.apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"split\"] == 0]\n",
    "val_df = df[df[\"split\"] == 1]\n",
    "test_df = df[df[\"split\"] == 2]\n",
    "\n",
    "# Shuffle the splits\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df = val_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retry the inference in case of a timeout and hide this in an inner function\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(multiplier=1, max=10))\n",
    "async def _get_entities(\n",
    "    text: str,\n",
    "    client: AsyncTensorZeroGateway,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> InferenceResponse:\n",
    "    return await client.inference(\n",
    "        function_name=\"extract_entities\",\n",
    "        input={\"messages\": [{\"role\": \"user\", \"content\": text}]},\n",
    "        dryrun=dryrun,\n",
    "        variant_name=variant_name,\n",
    "    )\n",
    "\n",
    "\n",
    "async def get_entities(\n",
    "    text: str,\n",
    "    client: AsyncTensorZeroGateway,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> Optional[InferenceResponse]:\n",
    "    try:\n",
    "        return await _get_entities(text, client, variant_name, dryrun)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match between the predicted and gold entities (the sharpest metric we use to evaluate NER)\n",
    "def exact_match(predicted: Dict[str, List[str]], gold: Dict[str, List[str]]) -> bool:\n",
    "    if predicted.keys() != gold.keys():\n",
    "        return False\n",
    "    for key, value in gold.items():\n",
    "        if set(item.lower() for item in predicted[key]) != set(\n",
    "            item.lower() for item in value\n",
    "        ):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, List[str]]) -> List[str]:\n",
    "    res = []\n",
    "    for k, v in d.items():\n",
    "        assert isinstance(v, list)\n",
    "        for elt in v:\n",
    "            res.append(f\"__{k.upper()}__::{elt}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "# Jaccard similarity between the predicted and gold entities\n",
    "# (a more lenient metric that gives partial credit for correct entities)\n",
    "# NOTE: This is a different implementation from the original code by Predibase, so the metrics won't be directly comparable.\n",
    "def jaccard_similarity(\n",
    "    predicted: Dict[str, List[str]], gold: Dict[str, List[str]]\n",
    ") -> float:\n",
    "    target_entities = flatten_dict(gold)\n",
    "    pred_entities = flatten_dict(predicted)\n",
    "    target_count = Counter(target_entities)\n",
    "    pred_count = Counter(pred_entities)\n",
    "    num = 0\n",
    "    den = 0\n",
    "    all_keys = set(target_entities).union(set(pred_entities))\n",
    "    for key in all_keys:\n",
    "        num += min(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "        den += max(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "    if den == 0:\n",
    "        return 1\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run inference using TensorZero on the training set to collect data for training future variants. We will evaluate the predictions on the training set and send the feedback to TensorZero as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these to run inference on more or fewer examples\n",
    "# or to respect your provider's rate limits\n",
    "NUM_TRAIN_PREDICTIONS = 1000\n",
    "MAX_CONCURRENT_REQUESTS = 100\n",
    "\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "\n",
    "\n",
    "async def make_inference(text: str, client: AsyncTensorZeroGateway):\n",
    "    async with semaphore:\n",
    "        return await get_entities(text, client)\n",
    "\n",
    "\n",
    "# We run inference in parallel to speed things up\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        make_inference(text, tensorzero_client)\n",
    "        for text in train_df[\"input\"][:NUM_TRAIN_PREDICTIONS]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(response: Optional[InferenceResponse], gold_data: Dict[str, List[str]]):\n",
    "    predicted = response.output.parsed if response else None\n",
    "    valid_json = predicted is not None\n",
    "    matched = exact_match(predicted, gold_data) if predicted else False\n",
    "    jaccard = jaccard_similarity(predicted, gold_data) if predicted else 0\n",
    "    return valid_json, matched, jaccard\n",
    "\n",
    "\n",
    "async def evaluate_send_feedback(\n",
    "    response: Optional[InferenceResponse], gold_data: Dict[str, List[str]]\n",
    "):\n",
    "    valid_json, matched, jaccard = evaluate(response, gold_data)\n",
    "    async with semaphore:\n",
    "        await asyncio.gather(\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"valid_json\",\n",
    "                value=valid_json,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"exact_match\",\n",
    "                value=matched,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"jaccard_similarity\",\n",
    "                value=jaccard,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions on the training set and send the feedback to TensorZero\n",
    "await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        evaluate_send_feedback(response, gold)\n",
    "        for response, gold in zip(responses, train_df[\"output\"][:NUM_TRAIN_PREDICTIONS])\n",
    "    ]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected data on the training set, we can query the database to see how well each variant performed.\n",
    "First, we'll check the exact match metric for each variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_client = Client.from_url(CLICKHOUSE_NATIVE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"exact_match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference ID\n",
    "df = clickhouse_client.query_dataframe(\n",
    "    \"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    b.value\n",
    "FROM \n",
    "    Inference i\n",
    "JOIN \n",
    "    BooleanMetricFeedback b ON i.id = b.target_id\n",
    "WHERE \n",
    "    i.function_name = 'extract_entities'\n",
    "    AND b.metric_name = %(metric_name)s\"\"\",\n",
    "    {\"metric_name\": metric_name},\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average score for each variant\n",
    "df.groupby(\"variant_name\")[\"value\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll check the jaccard similarity metric for each variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference ID\n",
    "df = clickhouse_client.query_dataframe(\n",
    "    \"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    f.value\n",
    "FROM \n",
    "    Inference i\n",
    "JOIN \n",
    "    FloatMetricFeedback f ON i.id = f.target_id\n",
    "WHERE \n",
    "    i.function_name = 'extract_entities'\n",
    "    AND f.metric_name = 'jaccard_similarity'\"\"\",\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average score for each variant\n",
    "df.groupby(\"variant_name\")[\"value\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_to_evaluate = df[\"variant_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll evaluate each variant on the test set. In order to be sure that we're not leaking any data, we'll use the dryrun flag to make sure the test set is not leaked here.\n",
    "\n",
    "We will also \"pin\" the variant name to the inference request to ensure that we're evaluating the same variant across all requests for a fair trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_PREDICTIONS = 500\n",
    "test_set = test_df.iloc[:NUM_TEST_PREDICTIONS]\n",
    "# We limit the number of concurrent requests to 5 to avoid overloading the server\n",
    "semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "\n",
    "async def make_inference(\n",
    "    text: str, client: AsyncTensorZeroGateway, variant_name: Optional[str] = None\n",
    "):\n",
    "    async with semaphore:\n",
    "        # We use dryrun=True to make sure the test set is not leaked here\n",
    "        return await get_entities(text, client, variant_name=variant_name, dryrun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_inference_tasks = []\n",
    "for variant_name in variants_to_evaluate:\n",
    "    variant_inference_tasks.append(\n",
    "        asyncio.gather(\n",
    "            *[\n",
    "                make_inference(text, tensorzero_client, variant_name=variant_name)\n",
    "                for text in test_set[\"input\"]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "variant_responses = await asyncio.gather(*variant_inference_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll evaluate the performance of each variant on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for variant_name, responses in zip(variants_to_evaluate, variant_responses):\n",
    "    jaccards = []\n",
    "    well_formed_jsons = []\n",
    "    exact_matches = []\n",
    "    print(f\"Evaluating variant: {variant_name}\")\n",
    "    print(f\"Number of responses: {len(responses)}\")\n",
    "    for response, gold in zip(responses, test_set[\"output\"]):\n",
    "        valid_json, matched, jaccard = evaluate(response, gold)\n",
    "        jaccards.append(jaccard)\n",
    "        well_formed_jsons.append(valid_json)\n",
    "        exact_matches.append(matched)\n",
    "    print(f\"Variant: {variant_name}\")\n",
    "    print(f\"Average Jaccard Similarity: {sum(jaccards) / len(jaccards)}\")\n",
    "    print(\n",
    "        f\"Average Well-formed JSON: {sum(well_formed_jsons) / len(well_formed_jsons)}\"\n",
    "    )\n",
    "    print(f\"Average Exact Match: {sum(exact_matches) / len(exact_matches)}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
