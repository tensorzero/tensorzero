{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e78a32-9c0d-40e6-84fd-a7f497f98092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Optional, Tuple\n",
    "from uuid import UUID\n",
    "\n",
    "from clickhouse_connect import get_client\n",
    "from openai import AsyncOpenAI\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c815e24-0396-4942-a729-1b0b88ae0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLICKHOUSE_URL = os.getenv(\"CLICKHOUSE_URL\")\n",
    "\n",
    "# Example: \"http://localhost:8123/tensorzero\" (\"https://user:password@host:port/database\")\n",
    "assert CLICKHOUSE_URL is not None, \"CLICKHOUSE_URL is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = AsyncTensorZeroGateway(\"http://localhost:3000\", timeout=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"CLICKHOUSE_URL\" in os.environ, \"CLICKHOUSE_URL environment variable not set\"\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef80de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nounlist.txt\", \"r\") as file:\n",
    "    nouns = [line.strip() for line in file]\n",
    "    random.shuffle(nouns)\n",
    "\n",
    "print(f\"There are {len(nouns)} nouns in the list of haiku topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d64d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def infer_simple(client: AsyncTensorZeroGateway, topic: str) -> InferenceResponse:\n",
    "    haiku_result: InferenceResponse = await client.inference(\n",
    "        function_name=\"write_haiku\",\n",
    "        input={\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"value\": {\"topic\": topic}}],\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "    return haiku_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def70a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_relevant_examples(input_data_serialized: str, k: int = 10):\n",
    "    result = await openai_client.embeddings.create(\n",
    "        input=input_data_serialized, model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embedding = result.data[0].embedding\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        input,\n",
    "        output,\n",
    "        cosineDistance(embedding, {embedding}) as distance\n",
    "        FROM DiclExample\n",
    "        WHERE function_name = 'write_haiku' AND variant_name = 'haiku_gpt4o_mini_dicl'\n",
    "        ORDER BY distance DESC\n",
    "        LIMIT {k}\n",
    "    \"\"\"\n",
    "    results = clickhouse_client.query_df(query)\n",
    "    # Extract the haiku text from the 'output' column\n",
    "    results[\"output\"] = results[\"output\"].apply(lambda x: json.loads(x)[0][\"text\"])\n",
    "\n",
    "    # Remove the outer quotes and unescape inner quotes\n",
    "    # results['output'] = results['output'].str.strip('\"').str.replace('\\\\\"', '\"')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8791361",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def infer_dicl(client: AsyncTensorZeroGateway, topic: str) -> InferenceResponse:\n",
    "    retrieval_data = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"value\": {\"topic\": topic}}]}\n",
    "        ]\n",
    "    }\n",
    "    retrieval_data_serialized = json.dumps(retrieval_data)\n",
    "    relevant_examples = await retrieve_relevant_examples(retrieval_data_serialized)\n",
    "    messages = []\n",
    "    for _, row in relevant_examples.iterrows():\n",
    "        messages.append({\"role\": \"user\", \"content\": row[\"input\"]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": row[\"output\"]})\n",
    "    current_input = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"value\": {\"topic\": topic}}]}\n",
    "        ]\n",
    "    }\n",
    "    current_input_serialized = json.dumps(current_input)\n",
    "    messages.append({\"role\": \"user\", \"content\": current_input_serialized})\n",
    "    haiku_result: InferenceResponse = await client.inference(\n",
    "        function_name=\"write_haiku_no_schema\",\n",
    "        input={\"messages\": messages},\n",
    "    )\n",
    "    return haiku_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce086cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await infer_dicl(tensorzero_client, \"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_grade_haiku(\n",
    "    topic: str,\n",
    "    client: AsyncTensorZeroGateway,\n",
    ") -> Optional[Tuple[str, bool, UUID]]:\n",
    "    # Infer against the TensorZero gateway using the write_haiku function\n",
    "    # This will naturally sample from the variants configured in `tensorzero.toml`\n",
    "    try:\n",
    "        haiku_result = await infer_dicl(client, topic)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # The LLM is instructed to conclude with the haiku, so we extract the last 3 lines\n",
    "    haiku_text = haiku_result.content[0].text\n",
    "    haiku_lines = haiku_text.strip().split(\"\\n\")\n",
    "    haiku_text = \"\\n\".join(haiku_lines[-3:])\n",
    "    inference_id = haiku_result.inference_id\n",
    "\n",
    "    # Judge the haiku using a separate TensorZero function, but we can use the same episode_id to associate these inferences\n",
    "    try:\n",
    "        judge_result: InferenceResponse = await client.inference(\n",
    "            function_name=\"judge_haiku\",\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"value\": {\"topic\": topic, \"haiku\": haiku_text},\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            episode_id=haiku_result.episode_id,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "    score = judge_result.output.parsed[\"score\"]\n",
    "    return haiku_text, score, inference_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a bunch of haiku writing and grading tasks concurrently\n",
    "max_concurrent_requests = 10\n",
    "num_requests = 1000\n",
    "semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "\n",
    "async def ratelimited_write_grade_haiku(noun, client):\n",
    "    async with semaphore:\n",
    "        return await write_grade_haiku(noun, client)\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    ratelimited_write_grade_haiku(noun, tensorzero_client)\n",
    "    for noun in nouns[:num_requests]\n",
    "]\n",
    "results = await tqdm_asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb56a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_haiku_feedback(\n",
    "    client: AsyncTensorZeroGateway,\n",
    "    inference_id: UUID,\n",
    "    score: bool,\n",
    "    semaphore: asyncio.Semaphore,\n",
    "):\n",
    "    async with semaphore:\n",
    "        await client.feedback(\n",
    "            metric_name=\"haiku_score\", inference_id=inference_id, value=score\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send feedback to the haiku grading function\n",
    "results = [result for result in results if result is not None]\n",
    "tasks = [\n",
    "    send_haiku_feedback(tensorzero_client, result[2], result[1], semaphore)\n",
    "    for result in results\n",
    "]\n",
    "await tqdm_asyncio.gather(*tasks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780dfd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_client = get_client(dsn=CLICKHOUSE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference ID\n",
    "df = clickhouse_client.query_df(\"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    b.value\n",
    "FROM \n",
    "    ChatInference i\n",
    "JOIN \n",
    "    BooleanMetricFeedback b ON i.id = b.target_id\n",
    "WHERE \n",
    "    i.function_name = 'write_haiku_no_schema'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b78584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average score for each variant\n",
    "df.groupby(\"variant_name\")[\"value\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
