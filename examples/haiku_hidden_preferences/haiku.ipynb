{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e78a32-9c0d-40e6-84fd-a7f497f98092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from typing import Optional, Tuple\n",
    "from uuid import UUID\n",
    "\n",
    "from clickhouse_driver import Client\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = AsyncTensorZeroGateway(\"http://localhost:3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef80de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nounlist.txt\", \"r\") as file:\n",
    "    nouns = [line.strip() for line in file]\n",
    "    random.shuffle(nouns)\n",
    "\n",
    "print(f\"There are {len(nouns)} nouns in the list of haiku topics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_grade_haiku(\n",
    "    topic: str,\n",
    "    client: AsyncTensorZeroGateway,\n",
    ") -> Optional[Tuple[str, bool, UUID]]:\n",
    "    # Infer against the TensorZero gateway using the write_haiku function\n",
    "    # This will naturally sample from the variants configured in `tensorzero.toml`\n",
    "    try:\n",
    "        haiku_result: InferenceResponse = await client.inference(\n",
    "            function_name=\"write_haiku\",\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\"type\": \"text\", \"value\": {\"topic\": topic}}],\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    # The LLM is instructed to conclude with the haiku, so we extract the last 3 lines\n",
    "    haiku_text = haiku_result.output[0].text\n",
    "    haiku_lines = haiku_text.strip().split(\"\\n\")\n",
    "    haiku_text = \"\\n\".join(haiku_lines[-3:])\n",
    "    inference_id = haiku_result.inference_id\n",
    "\n",
    "    # Judge the haiku using a separate TensorZero function, but we can use the same episode_id to associate these inferences\n",
    "    try:\n",
    "        judge_result: InferenceResponse = await client.inference(\n",
    "            function_name=\"judge_haiku\",\n",
    "            input={\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"value\": {\"topic\": topic, \"haiku\": haiku_text},\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            episode_id=haiku_result.episode_id,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    score = judge_result.output.parsed[\"score\"]\n",
    "    return haiku_text, score, inference_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0201056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a bunch of haiku writing and grading tasks concurrently\n",
    "max_concurrent_requests = 50\n",
    "num_requests = 500\n",
    "semaphore = asyncio.Semaphore(max_concurrent_requests)\n",
    "\n",
    "\n",
    "async def ratelimited_write_grade_haiku(noun, client):\n",
    "    async with semaphore:\n",
    "        return await write_grade_haiku(noun, client)\n",
    "\n",
    "\n",
    "tasks = [\n",
    "    ratelimited_write_grade_haiku(noun, tensorzero_client)\n",
    "    for noun in nouns[:num_requests]\n",
    "]\n",
    "results = await tqdm_asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb56a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_haiku_feedback(\n",
    "    client: AsyncTensorZeroGateway, inference_id: UUID, score: bool\n",
    "):\n",
    "    await client.feedback(\"haiku_score\", inference_id=inference_id, value=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3561fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send feedback to the haiku grading function\n",
    "results = [result for result in results if result is not None]\n",
    "tasks = [\n",
    "    send_haiku_feedback(tensorzero_client, result[2], result[1]) for result in results\n",
    "]\n",
    "await tqdm_asyncio.gather(*tasks);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780dfd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_client = Client(host=\"localhost\", database=\"tensorzero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5e121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference ID\n",
    "df = clickhouse_client.query_dataframe(\"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    b.value\n",
    "FROM \n",
    "    Inference i\n",
    "JOIN \n",
    "    BooleanMetricFeedback b ON i.id = b.target_id\n",
    "WHERE \n",
    "    i.function_name = 'write_haiku'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b78584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average score for each variant\n",
    "df.groupby(\"variant_name\")[\"value\"].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
