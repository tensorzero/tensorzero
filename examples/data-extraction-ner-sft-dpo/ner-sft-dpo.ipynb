{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00dfeb0",
   "metadata": {},
   "source": [
    "## Example: NER + SFT + DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cb0ae",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "767c156c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703152d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config/tensorzero.toml\"\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"\n",
    "MODEL_NAME = \"gpt-4o-2024-08-06\"\n",
    "VAL_FRACTION = 0.2\n",
    "MAX_SAMPLES = 500\n",
    "\n",
    "\n",
    "TENSORZERO_GATEWAY_URL = \"http://localhost:3000\"\n",
    "\n",
    "\n",
    "\n",
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ\n",
    "\n",
    "openai_client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bd53c",
   "metadata": {},
   "source": [
    "### STEP 1: SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42b3fd",
   "metadata": {},
   "source": [
    "### STEP 2: DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc2f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "#config = json.load(open(CONFIG_PATH)) if CONFIG_PATH.endswith(\".json\") else {}\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242a6a5",
   "metadata": {},
   "source": [
    "Ensure that the function and variant being fine-tuned are present in the provided config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c1c9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01cc42",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we will use for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247f9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63881094",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28ede1",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d462c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9052e80",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec0d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"json\": \"JsonInference\"}.get(function_type)\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663f9e9",
   "metadata": {},
   "source": [
    "Query ClickHouse for inference, feedback, and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f453bac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "                                        name\n",
      "0                       BatchIdByInferenceId\n",
      "1                   BatchIdByInferenceIdView\n",
      "2                        BatchModelInference\n",
      "3                               BatchRequest\n",
      "4                      BooleanMetricFeedback\n",
      "5            BooleanMetricFeedbackByTargetId\n",
      "6        BooleanMetricFeedbackByTargetIdView\n",
      "7               BooleanMetricFeedbackTagView\n",
      "8                              ChatInference\n",
      "9               ChatInferenceByEpisodeIdView\n",
      "10                     ChatInferenceByIdView\n",
      "11                    ChatInferenceDatapoint\n",
      "12                      ChatInferenceTagView\n",
      "13                           CommentFeedback\n",
      "14                 CommentFeedbackByTargetId\n",
      "15             CommentFeedbackByTargetIdView\n",
      "16                    CommentFeedbackTagView\n",
      "17                     DemonstrationFeedback\n",
      "18        DemonstrationFeedbackByInferenceId\n",
      "19    DemonstrationFeedbackByInferenceIdView\n",
      "20              DemonstrationFeedbackTagView\n",
      "21                              DeploymentID\n",
      "22                      DynamicEvaluationRun\n",
      "23         DynamicEvaluationRunByProjectName\n",
      "24     DynamicEvaluationRunByProjectNameView\n",
      "25               DynamicEvaluationRunEpisode\n",
      "26        DynamicEvaluationRunEpisodeByRunId\n",
      "27    DynamicEvaluationRunEpisodeByRunIdView\n",
      "28           DynamicInContextLearningExample\n",
      "29                               FeedbackTag\n",
      "30                       FloatMetricFeedback\n",
      "31             FloatMetricFeedbackByTargetId\n",
      "32         FloatMetricFeedbackByTargetIdView\n",
      "33                FloatMetricFeedbackTagView\n",
      "34                      InferenceByEpisodeId\n",
      "35                             InferenceById\n",
      "36                              InferenceTag\n",
      "37                             JsonInference\n",
      "38              JsonInferenceByEpisodeIdView\n",
      "39                     JsonInferenceByIdView\n",
      "40                    JsonInferenceDatapoint\n",
      "41                      JsonInferenceTagView\n",
      "42                            ModelInference\n",
      "43                       ModelInferenceCache\n",
      "44  StaticEvaluationBooleanHumanFeedbackView\n",
      "45    StaticEvaluationFloatHumanFeedbackView\n",
      "46             StaticEvaluationHumanFeedback\n",
      "47                      TagChatInferenceView\n",
      "48                              TagInference\n",
      "49                      TagJsonInferenceView\n",
      "50                       TensorZeroMigration\n",
      "   count()\n",
      "0        0\n",
      "   count()\n",
      "0        0\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Query ClickHouse for data\n",
    "# ---------------------------\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    i.variant_name AS variant,\n",
    "    i.episode_id AS episode_id,\n",
    "    i.input AS input,\n",
    "    i.output AS non_preferred_output,\n",
    "    d.value AS preferred_output\n",
    "FROM\n",
    "    JsonInference AS i\n",
    "INNER JOIN DemonstrationFeedback AS d ON i.id = d.inference_id\n",
    "WHERE\n",
    "    (i.function_name = %(function_name)s)\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\"function_name\": FUNCTION_NAME, \"max_samples\": MAX_SAMPLES}\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "df.head()\n",
    "#------------------------\n",
    "\n",
    "query1 = f\"\"\"\n",
    "SELECT DISTINCT function_name\n",
    "FROM {inference_table_name}\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df_debug = clickhouse_client.query_df(query1)\n",
    "print(df_debug)\n",
    "\n",
    "tables = clickhouse_client.query_df(\"SHOW TABLES\")\n",
    "print(tables)\n",
    "#print(clickhouse_client.query_df(\"DESCRIBE TABLE JsonInference\"))\n",
    "\n",
    "#print(clickhouse_client.query_df(\"SELECT * FROM JsonInference LIMIT 5\"))\n",
    "\n",
    "print(clickhouse_client.query_df(\"SELECT count() FROM JsonInference\"))\n",
    "print(clickhouse_client.query_df(\"SELECT count() FROM ModelInference\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d65aba",
   "metadata": {},
   "source": [
    "render message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7154792",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set a DataFrame without columns to the column openai_messages",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/b2/dwp9ltkx1q1_p3ns9dqhs_lh0000gn/T/ipykernel_59743/203560917.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m \n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m df[\u001b[33m\"openai_messages\"\u001b[39m] = df.apply(sample_to_openai_messages, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Drop null rows\u001b[39;00m\n\u001b[32m    164\u001b[39m df = df[df[\u001b[33m\"openai_messages\"\u001b[39m].notna()]\n",
      "\u001b[32m~/Desktop/CodeDay2025/tensorzero/.venv/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4297\u001b[39m             self._setitem_frame(key, value)\n\u001b[32m   4298\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(key, (Series, np.ndarray, list, Index)):\n\u001b[32m   4299\u001b[39m             self._setitem_array(key, value)\n\u001b[32m   4300\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(value, DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m4301\u001b[39m             self._set_item_frame_value(key, value)\n\u001b[32m   4302\u001b[39m         elif (\n\u001b[32m   4303\u001b[39m             is_list_like(value)\n\u001b[32m   4304\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.columns.is_unique\n",
      "\u001b[32m~/Desktop/CodeDay2025/tensorzero/.venv/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4460\u001b[39m                 \u001b[33m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[39m\n\u001b[32m   4461\u001b[39m                 f\"column {key}\"\n\u001b[32m   4462\u001b[39m             )\n\u001b[32m   4463\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(value.columns) == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4464\u001b[39m             raise ValueError(\n\u001b[32m   4465\u001b[39m                 f\"Cannot set a DataFrame without columns to the column {key}\"\n\u001b[32m   4466\u001b[39m             )\n\u001b[32m   4467\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: Cannot set a DataFrame without columns to the column openai_messages"
     ]
    }
   ],
   "source": [
    "def render_message(message: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message[\"role\"]\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message[\"content\"]:\n",
    "        if content_block[\"type\"] == \"text\":\n",
    "            parsed_content = content_block[\"value\"]\n",
    "            if not isinstance(parsed_content, str):\n",
    "                parsed_content = env.render_template(role, **parsed_content)\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block[\"type\"] == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block[\"value\"]})\n",
    "        elif content_block[\"type\"] == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_call\" and role == \"assistant\":\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_result\" and role == \"user\":\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            role_message[\"content\"] = content\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output: List[Dict[str, Any]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    if function_type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    elif function_type == \"chat\":\n",
    "        for content_block in output:\n",
    "            if content_block[\"type\"] == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": content_block[\"text\"]})\n",
    "            elif content_block[\"type\"] == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "                )\n",
    "            elif content_block[\"type\"] == \"tool_call\":\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = content\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample) -> List[Dict[str, str]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    result = {\n",
    "        \"input\": {\"messages\": [], \"tools\": [], \"parallel_tool_calls\": True},\n",
    "        \"preferred_output\": [],\n",
    "        \"non_preferred_output\": [],\n",
    "    }\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "        else:\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        result[\"input\"][\"messages\"].extend(render_message(message))\n",
    "\n",
    "    # Add the demonstration (preferred output)\n",
    "    preferred_output = json.loads(sample[\"preferred_output\"])\n",
    "    rendered_preferred_output = render_output(preferred_output)\n",
    "    if rendered_preferred_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    result[\"preferred_output\"].append(rendered_preferred_output)\n",
    "\n",
    "    # Add the inference output (non-preferred output)\n",
    "    non_preferred_output = json.loads(sample[\"non_preferred_output\"])\n",
    "    rendered_non_preferred_output = render_output(non_preferred_output)\n",
    "    if rendered_non_preferred_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    result[\"non_preferred_output\"].append(rendered_non_preferred_output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "df[\"openai_messages\"] = df.apply(sample_to_openai_messages, axis=1)\n",
    "\n",
    "# Drop null rows\n",
    "df = df[df[\"openai_messages\"].notna()]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dpo(example):\n",
    "    return {\n",
    "        \"input\": {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": example[\"prompt\"]}]\n",
    "        },\n",
    "        \"preferred_output\": [{\"role\": \"assistant\", \"content\": example[\"completion\"]}],\n",
    "        \"non_preferred_output\": [{\"role\": \"assistant\", \"content\": json.loads(example[\"non_preferred_output\"])[\"raw\"]}]\n",
    "    }\n",
    "\n",
    "dpo_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        input_data = json.loads(row[\"input\"])\n",
    "        output_data = json.loads(row[\"preferred_output\"])\n",
    "        prompt = input_data[\"messages\"][-1][\"content\"]\n",
    "        completion = output_data[\"raw\"]\n",
    "        dpo_rows.append({\"prompt\": prompt, \"completion\": completion, \"non_preferred_output\": row[\"non_preferred_output\"]})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "dpo_dataset = [format_dpo(r) for r in dpo_rows]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88ab92",
   "metadata": {},
   "source": [
    "## (Data format would go here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d92c31",
   "metadata": {},
   "source": [
    "Upload the prepared datasets to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_openai(df, openai_client) -> str:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n",
    "        for item in df[\"openai_messages\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        print(f\"File persisted on path [{f.name}]\")\n",
    "\n",
    "        with open(f.name, \"rb\") as file:\n",
    "            file_object = openai_client.files.create(file=file, purpose=\"fine-tune\")\n",
    "\n",
    "        return file_object.id\n",
    "\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "dpo_fine_tuning_object_id = upload_dataset_to_openai(train_df, openai_client)\n",
    "val_file_object_id = upload_dataset_to_openai(val_df, openai_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6242ec9",
   "metadata": {},
   "source": [
    "Launch the fine-tuning job and wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb70fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
    "    training_file=dpo_fine_tuning_object_id,\n",
    "    validation_file=val_file_object_id,\n",
    "    model=MODEL_NAME,\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.2},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "        pprint(job_status.to_dict())\n",
    "        if job_status.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"The fine-tuning job has compeleted with result {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe01ba2",
   "metadata": {},
   "source": [
    "TODO: Adding the fine-tuned model to the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = job_status.fine_tuned_model\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"openai\"],\n",
    "            \"providers\": {\"openai\": {\"type\": \"openai\", \"model_name\": fine_tuned_model}},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba9979",
   "metadata": {},
   "source": [
    "TODO: Adding a new variant to your function to use the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3576241",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"model\": fine_tuned_model,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {fine_tuned_model: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
