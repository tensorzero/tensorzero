{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00dfeb0",
   "metadata": {},
   "source": [
    "## Example: NER + SFT + DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cb0ae",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad0d90",
   "metadata": {},
   "source": [
    "####  IMPORTANT: Update the gateway URL below if you're not using the standard setup provided in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORZERO_GATEWAY_URL = \"http://localhost:3000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec60582",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c24063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only a subset of the dataset to speed things up\n",
    "NUM_TRAIN_DATAPOINTS = 500\n",
    "NUM_VAL_DATAPOINTS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str) -> (pd.DataFrame, pd.DataFrame):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(path)\n",
    "    df.output = df.output.apply(json.loads)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    train_df = df[df[\"split\"] == 0]\n",
    "    val_df = df[df[\"split\"] == 1]\n",
    "\n",
    "    # Shuffle the splits\n",
    "    train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    val_df = val_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "    # Select only a subset of the dataset to speed things up\n",
    "    train_df = train_df.iloc[:NUM_TRAIN_DATAPOINTS]\n",
    "    val_df = val_df.iloc[:NUM_VAL_DATAPOINTS]\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1eacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = load_dataset(\"data/conllpp.csv\")\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de6ba5",
   "metadata": {},
   "source": [
    "### Extract Entities\n",
    "IMPORTANT: REduce the number of concurrent request if you're running into rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_REQUESTS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6cb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = await AsyncTensorZeroGateway.build_http(\n",
    "    gateway_url=TENSORZERO_GATEWAY_URL, timeout=15\n",
    ")\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e096eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_entities(\n",
    "    text: str,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> Optional[InferenceResponse]:\n",
    "    # Use a semaphore to avoid rate limits\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            return await tensorzero_client.inference(\n",
    "                function_name=\"extract_entities\",\n",
    "                input={\"messages\": [{\"role\": \"user\", \"content\": text}]},\n",
    "                dryrun=dryrun,\n",
    "                variant_name=variant_name,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {type(e).__name__}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference in parallel to speed things up\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[get_entities(text) for text in train_df[\"input\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c98900",
   "metadata": {},
   "source": [
    "### Evaluate the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, List[str]]) -> List[str]:\n",
    "    res = []\n",
    "    for k, v in d.items():\n",
    "        assert isinstance(v, list)\n",
    "        for elt in v:\n",
    "            res.append(f\"__{k.upper()}__::{elt}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "# Exact match between the predicted and ground truth entities (the sharpest metric we use to evaluate NER)\n",
    "def compute_exact_match(\n",
    "    predicted: Dict[str, List[str]], ground_truth: Dict[str, List[str]]\n",
    ") -> bool:\n",
    "    return set(flatten_dict(predicted)) == set(flatten_dict(ground_truth))\n",
    "\n",
    "\n",
    "# Jaccard similarity between the predicted and ground_truth entities\n",
    "# (a more lenient metric that gives partial credit for correct entities)\n",
    "# This is a different implementation from the original code by Predibase, so the metrics won't be directly comparable.\n",
    "def compute_jaccard_similarity(\n",
    "    predicted: Dict[str, List[str]], ground_truth: Dict[str, List[str]]\n",
    ") -> float:\n",
    "    target_entities = flatten_dict(ground_truth)\n",
    "    pred_entities = flatten_dict(predicted)\n",
    "    target_count = Counter(target_entities)\n",
    "    pred_count = Counter(pred_entities)\n",
    "    num = 0\n",
    "    den = 0\n",
    "    all_keys = set(target_entities).union(set(pred_entities))\n",
    "    for key in all_keys:\n",
    "        num += min(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "        den += max(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "    if den == 0:\n",
    "        return 1\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(\n",
    "    response: Optional[InferenceResponse], ground_truth_data: Dict[str, List[str]]\n",
    "):\n",
    "    predicted = response.output.parsed if response else None\n",
    "\n",
    "    # `predicted` is None if the model failed to return a valid JSON that complies with the output schema\n",
    "    valid_output = predicted is not None\n",
    "\n",
    "    # Compute the other metrics\n",
    "    exact_match = (\n",
    "        compute_exact_match(predicted, ground_truth_data) if predicted else False\n",
    "    )\n",
    "    jaccard_similarity = (\n",
    "        compute_jaccard_similarity(predicted, ground_truth_data) if predicted else 0\n",
    "    )\n",
    "\n",
    "    return valid_output, exact_match, jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ab262",
   "metadata": {},
   "outputs": [],
   "source": [
    "for response, ground_truth in tqdm(\n",
    "    zip(responses, train_df[\"output\"]), total=len(responses)\n",
    "):\n",
    "    # Don't send feedback if the request failed completely\n",
    "    if response is None:\n",
    "        continue\n",
    "\n",
    "    # Evaluate the example\n",
    "    valid_output, exact_match, jaccard_similarity = evaluate_response(\n",
    "        response, ground_truth\n",
    "    )\n",
    "\n",
    "    # Send the metrics feedback to TensorZero\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"valid_output\",\n",
    "        value=valid_output,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"exact_match\",\n",
    "        value=exact_match,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"jaccard_similarity\",\n",
    "        value=jaccard_similarity,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    # Send the demonstration feedback to TensorZero\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"demonstration\",\n",
    "        value=ground_truth,\n",
    "        inference_id=response.inference_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c534c3d",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "IMPORTANT: Update the list blow when you create new variants in `tensorzero.toml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the variants in `tensorzero.toml` that we want to evaluate\n",
    "VARIANTS_TO_EVALUATE = [\n",
    "    \"gpt_4o\",\n",
    "    \"gpt_4o_mini\",\n",
    "    \"gpt_4o_mini_sft_fine_tuned\",\n",
    "    \"gpt_4o_mini_sft_dpo_fine_tuned\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771b2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}  # variant_name => (valid_output, exact_match, jaccard_similarity)\n",
    "\n",
    "for variant_name in VARIANTS_TO_EVALUATE:\n",
    "    # Run inference on the validation set\n",
    "    responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            get_entities(\n",
    "                text,\n",
    "                variant_name=variant_name,  # pin to the specific variant we want to evaluate\n",
    "                dryrun=True,  # don't store results to avoid leaking data\n",
    "            )\n",
    "            for text in val_df[\"input\"]\n",
    "        ],\n",
    "        desc=f\"Evaluating variant: {variant_name}\",\n",
    "    )\n",
    "\n",
    "    # Evaluate the performance of the variant\n",
    "    valid_output_scores = []\n",
    "    exact_match_scores = []\n",
    "    jaccard_similarity_scores = []\n",
    "\n",
    "    for response, ground_truth in zip(responses, val_df[\"output\"]):\n",
    "        valid_output, exact_match, jaccard_similarity = evaluate_response(\n",
    "            response, ground_truth\n",
    "        )\n",
    "        valid_output_scores.append(valid_output)\n",
    "        exact_match_scores.append(exact_match)\n",
    "        jaccard_similarity_scores.append(jaccard_similarity)\n",
    "\n",
    "    scores[variant_name] = {\n",
    "        \"valid_output\": valid_output_scores,\n",
    "        \"exact_match\": exact_match_scores,\n",
    "        \"jaccard_similarity\": jaccard_similarity_scores,\n",
    "    }\n",
    "\n",
    "    # Print the performance of the variant\n",
    "    print(f\"Valid Output: {sum(valid_output_scores) / len(valid_output_scores):.1%}\")\n",
    "    print(f\"Exact Match: {sum(exact_match_scores) / len(exact_match_scores):.1%}\")\n",
    "    print(\n",
    "        f\"Jaccard Similarity (mean): {sum(jaccard_similarity_scores) / len(jaccard_similarity_scores):.1%}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394918e3",
   "metadata": {},
   "source": [
    "### Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec425445",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = []\n",
    "\n",
    "for variant_name, variant_scores in scores.items():\n",
    "    exact_match_score = sum(variant_scores[\"exact_match\"]) / len(\n",
    "        variant_scores[\"exact_match\"]\n",
    "    )\n",
    "    scores_df.append(\n",
    "        {\n",
    "            \"Variant\": variant_name,\n",
    "            \"Metric\": \"exact_match\",\n",
    "            \"Score\": exact_match_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    jaccard_similarity_score = sum(variant_scores[\"jaccard_similarity\"]) / len(\n",
    "        variant_scores[\"jaccard_similarity\"]\n",
    "    )\n",
    "\n",
    "    scores_df.append(\n",
    "        {\n",
    "            \"Variant\": variant_name,\n",
    "            \"Metric\": \"jaccard_similarity\",\n",
    "            \"Score\": jaccard_similarity_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "scores_df = pd.DataFrame(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6931650",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = (\n",
    "    alt.Chart(scores_df)\n",
    "    .encode(\n",
    "        x=alt.X(\"Score:Q\", axis=alt.Axis(format=\"%\"), scale=alt.Scale(domain=[0, 1])),\n",
    "        y=\"Variant:N\",\n",
    "        yOffset=\"Metric:N\",\n",
    "        color=\"Metric:N\",\n",
    "        text=alt.Text(\"Score:Q\", format=\".1%\"),\n",
    "    )\n",
    "    .properties(title=\"Metrics by Variant\")\n",
    ")\n",
    "\n",
    "chart = chart.mark_bar() + chart.mark_text(align=\"left\", dx=2)\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703152d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config/tensorzero.toml\"\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"\n",
    "MODEL_NAME = \"gpt-4o-2024-08-06\"\n",
    "VAL_FRACTION = 0.2\n",
    "MAX_SAMPLES = 500\n",
    "\n",
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bd53c",
   "metadata": {},
   "source": [
    "### STEP 1: SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42b3fd",
   "metadata": {},
   "source": [
    "### STEP 2: DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242a6a5",
   "metadata": {},
   "source": [
    "Ensure that the function and variant being fine-tuned are present in the provided config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01cc42",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we will use for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63881094",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28ede1",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9052e80",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"json\": \"JsonInference\"}.get(function_type)\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663f9e9",
   "metadata": {},
   "source": [
    "Query ClickHouse for inference, feedback, and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f453bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Query ClickHouse for data\n",
    "# ---------------------------\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    i.variant_name AS variant,\n",
    "    i.episode_id AS episode_id,\n",
    "    i.input AS input,\n",
    "    i.output AS non_preferred_output,\n",
    "    d.value AS preferred_output\n",
    "FROM\n",
    "    JsonInference AS i\n",
    "INNER JOIN DemonstrationFeedback AS d ON i.id = d.inference_id\n",
    "WHERE\n",
    "    (i.function_name = %(function_name)s)\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\"function_name\": FUNCTION_NAME, \"max_samples\": MAX_SAMPLES}\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "df.head()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d65aba",
   "metadata": {},
   "source": [
    "render message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7154792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(message: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message[\"role\"]\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message[\"content\"]:\n",
    "        if content_block[\"type\"] == \"text\":\n",
    "            parsed_content = content_block[\"value\"]\n",
    "            if not isinstance(parsed_content, str):\n",
    "                parsed_content = env.render_template(role, **parsed_content)\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block[\"type\"] == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block[\"value\"]})\n",
    "        elif content_block[\"type\"] == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_call\" and role == \"assistant\":\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_result\" and role == \"user\":\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            role_message[\"content\"] = content\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output: List[Dict[str, Any]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    if function_type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    elif function_type == \"chat\":\n",
    "        for content_block in output:\n",
    "            if content_block[\"type\"] == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": content_block[\"text\"]})\n",
    "            elif content_block[\"type\"] == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "                )\n",
    "            elif content_block[\"type\"] == \"tool_call\":\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = content\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample) -> List[Dict[str, str]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    result = {\n",
    "        \"input\": {\"messages\": [], \"tools\": [], \"parallel_tool_calls\": True},\n",
    "        \"preferred_output\": [],\n",
    "        \"non_preferred_output\": [],\n",
    "    }\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "        else:\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        result[\"input\"][\"messages\"].extend(render_message(message))\n",
    "\n",
    "    # Add the demonstration (preferred output)\n",
    "    preferred_output = json.loads(sample[\"preferred_output\"])\n",
    "    rendered_preferred_output = render_output(preferred_output)\n",
    "    if rendered_preferred_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    result[\"preferred_output\"].append(rendered_preferred_output)\n",
    "\n",
    "    # Add the inference output (non-preferred output)\n",
    "    non_preferred_output = json.loads(sample[\"non_preferred_output\"])\n",
    "    rendered_non_preferred_output = render_output(non_preferred_output)\n",
    "    if rendered_non_preferred_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    result[\"non_preferred_output\"].append(rendered_non_preferred_output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "df[\"openai_messages\"] = df.apply(sample_to_openai_messages, axis=1)\n",
    "\n",
    "# Drop null rows\n",
    "df = df[df[\"openai_messages\"].notna()]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611af61c",
   "metadata": {},
   "source": [
    "Split data into training and validation sets for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1208fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique episode_ids\n",
    "unique_episode_ids = df[\"episode_id\"].unique()\n",
    "\n",
    "# Shuffle the unique episode_ids\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_episode_ids)\n",
    "\n",
    "# Calculate the split index for episode_ids\n",
    "split_index = int(len(unique_episode_ids) * (1 - VAL_FRACTION))\n",
    "\n",
    "# Split the episode_ids into training and validation sets\n",
    "train_episode_ids = unique_episode_ids[:split_index]\n",
    "val_episode_ids = unique_episode_ids[split_index:]\n",
    "\n",
    "# Create training and validation DataFrames based on episode_ids\n",
    "train_df = df[df[\"episode_id\"].isin(train_episode_ids)]\n",
    "val_df = df[df[\"episode_id\"].isin(val_episode_ids)]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Actual validation fraction: {len(val_df) / len(df):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41216e0",
   "metadata": {},
   "source": [
    "Upload the preared datasets to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_openai(df, openai_client) -> str:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n",
    "        for item in df[\"openai_messages\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        print(f\"File persisted on path [{f.name}]\")\n",
    "\n",
    "        with open(f.name, \"rb\") as file:\n",
    "            file_object = openai_client.files.create(file=file, purpose=\"fine-tune\")\n",
    "\n",
    "        return file_object.id\n",
    "\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "dpo_fine_tuning_object_id = upload_dataset_to_openai(train_df, openai_client)\n",
    "val_file_object_id = upload_dataset_to_openai(val_df, openai_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70a2456",
   "metadata": {},
   "source": [
    "Launch the fine-tuning job and wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00fde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
    "    training_file=dpo_fine_tuning_object_id,\n",
    "    validation_file=val_file_object_id,\n",
    "    model=MODEL_NAME,\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.2},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "        pprint(job_status.to_dict())\n",
    "        if job_status.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"The fine-tuning job has compeleted with result {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe01ba2",
   "metadata": {},
   "source": [
    "TODO: Adding the fine-tuned model to the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = job_status.fine_tuned_model\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"openai\"],\n",
    "            \"providers\": {\"openai\": {\"type\": \"openai\", \"model_name\": fine_tuned_model}},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba9979",
   "metadata": {},
   "source": [
    "TODO: Adding a new variant to your function to use the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3576241",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"model\": fine_tuned_model,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {fine_tuned_model: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
