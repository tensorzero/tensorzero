{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00dfeb0",
   "metadata": {},
   "source": [
    "## Example: NER + SFT + DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cb0ae",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "767c156c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad0d90",
   "metadata": {},
   "source": [
    "####  IMPORTANT: Update the gateway URL below if you're not using the standard setup provided in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ddc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORZERO_GATEWAY_URL = \"http://localhost:3000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec60582",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8c24063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only a subset of the dataset to speed things up\n",
    "NUM_TRAIN_DATAPOINTS = 500\n",
    "NUM_VAL_DATAPOINTS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9ffb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str) -> (pd.DataFrame, pd.DataFrame):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(path)\n",
    "    df.output = df.output.apply(json.loads)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    train_df = df[df[\"split\"] == 0]\n",
    "    val_df = df[df[\"split\"] == 1]\n",
    "\n",
    "    # Shuffle the splits\n",
    "    train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    val_df = val_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "    # Select only a subset of the dataset to speed things up\n",
    "    train_df = train_df.iloc[:NUM_TRAIN_DATAPOINTS]\n",
    "    val_df = val_df.iloc[:NUM_VAL_DATAPOINTS]\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff1eacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (500, 5)\n",
      "Validation data shape: (500, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = load_dataset(\"data/conllpp.csv\")\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de6ba5",
   "metadata": {},
   "source": [
    "### Extract Entities\n",
    "IMPORTANT: REduce the number of concurrent request if you're running into rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81bf8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_REQUESTS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab6cb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = await AsyncTensorZeroGateway.build_http(\n",
    "    gateway_url=TENSORZERO_GATEWAY_URL, timeout=15\n",
    ")\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94e096eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_entities(\n",
    "    text: str,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> Optional[InferenceResponse]:\n",
    "    # Use a semaphore to avoid rate limits\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            return await tensorzero_client.inference(\n",
    "                function_name=\"extract_entities\",\n",
    "                input={\"messages\": [{\"role\": \"user\", \"content\": text}]},\n",
    "                dryrun=dryrun,\n",
    "                variant_name=variant_name,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {type(e).__name__}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74de52a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:59<00:00,  8.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run inference in parallel to speed things up\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[get_entities(text) for text in train_df[\"input\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c98900",
   "metadata": {},
   "source": [
    "### Evaluate the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3eb8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, List[str]]) -> List[str]:\n",
    "    res = []\n",
    "    for k, v in d.items():\n",
    "        assert isinstance(v, list)\n",
    "        for elt in v:\n",
    "            res.append(f\"__{k.upper()}__::{elt}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "# Exact match between the predicted and ground truth entities (the sharpest metric we use to evaluate NER)\n",
    "def compute_exact_match(\n",
    "    predicted: Dict[str, List[str]], ground_truth: Dict[str, List[str]]\n",
    ") -> bool:\n",
    "    return set(flatten_dict(predicted)) == set(flatten_dict(ground_truth))\n",
    "\n",
    "# Jaccard similarity between the predicted and ground_truth entities\n",
    "# (a more lenient metric that gives partial credit for correct entities)\n",
    "# This is a different implementation from the original code by Predibase, so the metrics won't be directly comparable.\n",
    "def compute_jaccard_similarity(\n",
    "    predicted: Dict[str, List[str]], ground_truth: Dict[str, List[str]]\n",
    ") -> float:\n",
    "    target_entities = flatten_dict(ground_truth)\n",
    "    pred_entities = flatten_dict(predicted)\n",
    "    target_count = Counter(target_entities)\n",
    "    pred_count = Counter(pred_entities)\n",
    "    num = 0\n",
    "    den = 0\n",
    "    all_keys = set(target_entities).union(set(pred_entities))\n",
    "    for key in all_keys:\n",
    "        num += min(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "        den += max(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "    if den == 0:\n",
    "        return 1\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2f996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(\n",
    "    response: Optional[InferenceResponse], ground_truth_data: Dict[str, List[str]]\n",
    "):\n",
    "    predicted = response.output.parsed if response else None\n",
    "\n",
    "    # `predicted` is None if the model failed to return a valid JSON that complies with the output schema\n",
    "    valid_output = predicted is not None\n",
    "\n",
    "    # Compute the other metrics\n",
    "    exact_match = (\n",
    "        compute_exact_match(predicted, ground_truth_data) if predicted else False\n",
    "    )\n",
    "    jaccard_similarity = (\n",
    "        compute_jaccard_similarity(predicted, ground_truth_data) if predicted else 0\n",
    "    )\n",
    "\n",
    "    return valid_output, exact_match, jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "312ab262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 122.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for response, ground_truth in tqdm(\n",
    "    zip(responses, train_df[\"output\"]), total=len(responses)\n",
    "):\n",
    "    # Don't send feedback if the request failed completely\n",
    "    if response is None:\n",
    "        continue\n",
    "\n",
    "    # Evaluate the example\n",
    "    valid_output, exact_match, jaccard_similarity = evaluate_response(\n",
    "        response, ground_truth\n",
    "    )\n",
    "\n",
    "    # Send the metrics feedback to TensorZero\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"valid_output\",\n",
    "        value=valid_output,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"exact_match\",\n",
    "        value=exact_match,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"jaccard_similarity\",\n",
    "        value=jaccard_similarity,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    # Send the demonstration feedback to TensorZero\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"demonstration\",\n",
    "        value=ground_truth,\n",
    "        inference_id=response.inference_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c534c3d",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "IMPORTANT: Update the list blow when you create new variants in `tensorzero.toml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d46b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the variants in `tensorzero.toml` that we want to evaluate\n",
    "VARIANTS_TO_EVALUATE = [\n",
    "    \"gpt_4o\",\n",
    "    \"gpt_4o_mini\",\n",
    "    # \"gpt_4o_mini_fine_tuned\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "771b2b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating variant: gpt_4o: 100%|██████████| 500/500 [01:03<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Output: 100.0%\n",
      "Exact Match: 49.2%\n",
      "Jaccard Similarity (mean): 61.4%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating variant: gpt_4o_mini: 100%|██████████| 500/500 [00:58<00:00,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Output: 100.0%\n",
      "Exact Match: 8.2%\n",
      "Jaccard Similarity (mean): 33.9%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = {}  # variant_name => (valid_output, exact_match, jaccard_similarity)\n",
    "\n",
    "for variant_name in VARIANTS_TO_EVALUATE:\n",
    "    # Run inference on the validation set\n",
    "    responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            get_entities(\n",
    "                text,\n",
    "                variant_name=variant_name,  # pin to the specific variant we want to evaluate\n",
    "                dryrun=True,  # don't store results to avoid leaking data\n",
    "            )\n",
    "            for text in val_df[\"input\"]\n",
    "        ],\n",
    "        desc=f\"Evaluating variant: {variant_name}\",\n",
    "    )\n",
    "\n",
    "    # Evaluate the performance of the variant\n",
    "    valid_output_scores = []\n",
    "    exact_match_scores = []\n",
    "    jaccard_similarity_scores = []\n",
    "\n",
    "    for response, ground_truth in zip(responses, val_df[\"output\"]):\n",
    "        valid_output, exact_match, jaccard_similarity = evaluate_response(\n",
    "            response, ground_truth\n",
    "        )\n",
    "        valid_output_scores.append(valid_output)\n",
    "        exact_match_scores.append(exact_match)\n",
    "        jaccard_similarity_scores.append(jaccard_similarity)\n",
    "\n",
    "    scores[variant_name] = {\n",
    "        \"valid_output\": valid_output_scores,\n",
    "        \"exact_match\": exact_match_scores,\n",
    "        \"jaccard_similarity\": jaccard_similarity_scores,\n",
    "    }\n",
    "\n",
    "    # Print the performance of the variant\n",
    "    print(f\"Valid Output: {sum(valid_output_scores) / len(valid_output_scores):.1%}\")\n",
    "    print(f\"Exact Match: {sum(exact_match_scores) / len(exact_match_scores):.1%}\")\n",
    "    print(\n",
    "        f\"Jaccard Similarity (mean): {sum(jaccard_similarity_scores) / len(jaccard_similarity_scores):.1%}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394918e3",
   "metadata": {},
   "source": [
    "### Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec425445",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = []\n",
    "\n",
    "for variant_name, variant_scores in scores.items():\n",
    "    exact_match_score = sum(variant_scores[\"exact_match\"]) / len(\n",
    "        variant_scores[\"exact_match\"]\n",
    "    )\n",
    "    scores_df.append(\n",
    "        {\n",
    "            \"Variant\": variant_name,\n",
    "            \"Metric\": \"exact_match\",\n",
    "            \"Score\": exact_match_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    jaccard_similarity_score = sum(variant_scores[\"jaccard_similarity\"]) / len(\n",
    "        variant_scores[\"jaccard_similarity\"]\n",
    "    )\n",
    "\n",
    "    scores_df.append(\n",
    "        {\n",
    "            \"Variant\": variant_name,\n",
    "            \"Metric\": \"jaccard_similarity\",\n",
    "            \"Score\": jaccard_similarity_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "scores_df = pd.DataFrame(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6931650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-2bed85e9ba274d5dbd312f9f076046c0.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-2bed85e9ba274d5dbd312f9f076046c0.vega-embed details,\n",
       "  #altair-viz-2bed85e9ba274d5dbd312f9f076046c0.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-2bed85e9ba274d5dbd312f9f076046c0\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-2bed85e9ba274d5dbd312f9f076046c0\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-2bed85e9ba274d5dbd312f9f076046c0\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Metric\", \"type\": \"nominal\"}, \"text\": {\"field\": \"Score\", \"format\": \".1%\", \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"format\": \"%\"}, \"field\": \"Score\", \"scale\": {\"domain\": [0, 1]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"Variant\", \"type\": \"nominal\"}, \"yOffset\": {\"field\": \"Metric\", \"type\": \"nominal\"}}, \"title\": \"Metrics by Variant\"}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"dx\": 2}, \"encoding\": {\"color\": {\"field\": \"Metric\", \"type\": \"nominal\"}, \"text\": {\"field\": \"Score\", \"format\": \".1%\", \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"format\": \"%\"}, \"field\": \"Score\", \"scale\": {\"domain\": [0, 1]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"Variant\", \"type\": \"nominal\"}, \"yOffset\": {\"field\": \"Metric\", \"type\": \"nominal\"}}, \"title\": \"Metrics by Variant\"}], \"data\": {\"name\": \"data-993b0d6a89e9ba9d990cf63312e07833\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-993b0d6a89e9ba9d990cf63312e07833\": [{\"Variant\": \"gpt_4o\", \"Metric\": \"exact_match\", \"Score\": 0.492}, {\"Variant\": \"gpt_4o\", \"Metric\": \"jaccard_similarity\", \"Score\": 0.6144764790764791}, {\"Variant\": \"gpt_4o_mini\", \"Metric\": \"exact_match\", \"Score\": 0.082}, {\"Variant\": \"gpt_4o_mini\", \"Metric\": \"jaccard_similarity\", \"Score\": 0.33909758297258297}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = (\n",
    "    alt.Chart(scores_df)\n",
    "    .encode(\n",
    "        x=alt.X(\"Score:Q\", axis=alt.Axis(format=\"%\"), scale=alt.Scale(domain=[0, 1])),\n",
    "        y=\"Variant:N\",\n",
    "        yOffset=\"Metric:N\",\n",
    "        color=\"Metric:N\",\n",
    "        text=alt.Text(\"Score:Q\", format=\".1%\"),\n",
    "    )\n",
    "    .properties(title=\"Metrics by Variant\")\n",
    ")\n",
    "\n",
    "chart = chart.mark_bar() + chart.mark_text(align=\"left\", dx=2)\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "703152d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config/tensorzero.toml\"\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"\n",
    "MODEL_NAME = \"gpt-4o-2024-08-06\"\n",
    "VAL_FRACTION = 0.2\n",
    "MAX_SAMPLES = 500\n",
    "\n",
    "\n",
    "\n",
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ\n",
    "\n",
    "openai_client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bd53c",
   "metadata": {},
   "source": [
    "### STEP 1: SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42b3fd",
   "metadata": {},
   "source": [
    "### STEP 2: DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fc2f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "#config = json.load(open(CONFIG_PATH)) if CONFIG_PATH.endswith(\".json\") else {}\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242a6a5",
   "metadata": {},
   "source": [
    "Ensure that the function and variant being fine-tuned are present in the provided config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c1c9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01cc42",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we will use for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "247f9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63881094",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28ede1",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d462c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9052e80",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ec0d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"json\": \"JsonInference\"}.get(function_type)\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663f9e9",
   "metadata": {},
   "source": [
    "Query ClickHouse for inference, feedback, and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f453bac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         variant                            episode_id  \\\n",
      "0         gpt_4o  0198675d-ecc6-78f1-8e16-01853e01056d   \n",
      "1         gpt_4o  0198675d-6965-7372-8eab-9ac0955f2c4e   \n",
      "2         gpt_4o  0198675d-c5e5-7cb0-8f89-e8a19fe1930b   \n",
      "3         gpt_4o  0198675d-6d3a-7e70-8fa1-6046772f8d21   \n",
      "4         gpt_4o  0198675e-2d6e-7941-9616-52d1ee7871a4   \n",
      "..           ...                                   ...   \n",
      "495  gpt_4o_mini  0198675e-3881-73c3-a01a-e3e77101403d   \n",
      "496  gpt_4o_mini  0198675e-26ab-7211-a4c1-8c5c0711bf1b   \n",
      "497  gpt_4o_mini  0198675e-0d93-7e90-babd-d4f95e30ff9b   \n",
      "498  gpt_4o_mini  0198675e-1714-7410-baec-07734bf1daf0   \n",
      "499  gpt_4o_mini  0198675d-d144-7890-bba1-656820d0a721   \n",
      "\n",
      "                                                 input  \\\n",
      "0    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "1    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "2    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "3    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "4    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "..                                                 ...   \n",
      "495  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "496  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "497  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "498  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "499  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "\n",
      "                                  non_preferred_output  \\\n",
      "0    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
      "1    {\"raw\":\"{\\\"person\\\":[\\\"Peter Nicol\\\",\\\"Julian ...   \n",
      "2    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
      "3    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
      "4    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
      "..                                                 ...   \n",
      "495  {\"raw\":\"{\\n    \\\"person\\\": [\\\"Banharn\\\"],\\n   ...   \n",
      "496  {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
      "497  {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
      "498  {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
      "499  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
      "\n",
      "                                      preferred_output  \n",
      "0    {\"raw\":\"{\\\"person\\\":[\\\"Slight\\\"],\\\"organizatio...  \n",
      "1    {\"raw\":\"{\\\"person\\\":[\\\"Peter Nicol\\\",\\\"Julian ...  \n",
      "2    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "3    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "4    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "..                                                 ...  \n",
      "495  {\"raw\":\"{\\\"person\\\":[\\\"Banharn\\\"],\\\"organizati...  \n",
      "496  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "497  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"Sud...  \n",
      "498  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "499  {\"raw\":\"{\\\"person\\\":[\\\"Davis\\\"],\\\"organization...  \n",
      "\n",
      "[500 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Query ClickHouse for data\n",
    "# ---------------------------\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    i.variant_name AS variant,\n",
    "    i.episode_id AS episode_id,\n",
    "    i.input AS input,\n",
    "    i.output AS non_preferred_output,\n",
    "    d.value AS preferred_output\n",
    "FROM\n",
    "    JsonInference AS i\n",
    "INNER JOIN DemonstrationFeedback AS d ON i.id = d.inference_id\n",
    "WHERE\n",
    "    (i.function_name = %(function_name)s)\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\"function_name\": FUNCTION_NAME, \"max_samples\": MAX_SAMPLES}\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "df.head()\n",
    "print(df)\n",
    "#------------------------\n",
    "\n",
    "query1 = f\"\"\"\n",
    "SELECT DISTINCT function_name\n",
    "FROM {inference_table_name}\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df_debug = clickhouse_client.query_df(query1)\n",
    "#print(df_debug)\n",
    "\n",
    "tables = clickhouse_client.query_df(\"SHOW TABLES\")\n",
    "#print(tables)\n",
    "#print(clickhouse_client.query_df(\"DESCRIBE TABLE JsonInference\"))\n",
    "\n",
    "#print(clickhouse_client.query_df(\"SELECT * FROM JsonInference LIMIT 5\"))\n",
    "\n",
    "#print(clickhouse_client.query_df(\"SELECT count() FROM JsonInference\"))\n",
    "#print(clickhouse_client.query_df(\"SELECT count() FROM ModelInference\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d65aba",
   "metadata": {},
   "source": [
    "render message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7154792",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set a DataFrame without columns to the column openai_messages",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/b2/dwp9ltkx1q1_p3ns9dqhs_lh0000gn/T/ipykernel_59743/203560917.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    157\u001b[39m \n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m df[\u001b[33m\"openai_messages\"\u001b[39m] = df.apply(sample_to_openai_messages, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Drop null rows\u001b[39;00m\n\u001b[32m    164\u001b[39m df = df[df[\u001b[33m\"openai_messages\"\u001b[39m].notna()]\n",
      "\u001b[32m~/Desktop/CodeDay2025/tensorzero/.venv/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4297\u001b[39m             self._setitem_frame(key, value)\n\u001b[32m   4298\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(key, (Series, np.ndarray, list, Index)):\n\u001b[32m   4299\u001b[39m             self._setitem_array(key, value)\n\u001b[32m   4300\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(value, DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m4301\u001b[39m             self._set_item_frame_value(key, value)\n\u001b[32m   4302\u001b[39m         elif (\n\u001b[32m   4303\u001b[39m             is_list_like(value)\n\u001b[32m   4304\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m self.columns.is_unique\n",
      "\u001b[32m~/Desktop/CodeDay2025/tensorzero/.venv/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, value)\u001b[39m\n\u001b[32m   4460\u001b[39m                 \u001b[33m\"Cannot set a DataFrame with multiple columns to the single \"\u001b[39m\n\u001b[32m   4461\u001b[39m                 f\"column {key}\"\n\u001b[32m   4462\u001b[39m             )\n\u001b[32m   4463\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(value.columns) == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4464\u001b[39m             raise ValueError(\n\u001b[32m   4465\u001b[39m                 f\"Cannot set a DataFrame without columns to the column {key}\"\n\u001b[32m   4466\u001b[39m             )\n\u001b[32m   4467\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: Cannot set a DataFrame without columns to the column openai_messages"
     ]
    }
   ],
   "source": [
    "def render_message(message: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message[\"role\"]\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message[\"content\"]:\n",
    "        if content_block[\"type\"] == \"text\":\n",
    "            parsed_content = content_block[\"value\"]\n",
    "            if not isinstance(parsed_content, str):\n",
    "                parsed_content = env.render_template(role, **parsed_content)\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block[\"type\"] == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block[\"value\"]})\n",
    "        elif content_block[\"type\"] == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_call\" and role == \"assistant\":\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_result\" and role == \"user\":\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            role_message[\"content\"] = content\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output: List[Dict[str, Any]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    if function_type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    elif function_type == \"chat\":\n",
    "        for content_block in output:\n",
    "            if content_block[\"type\"] == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": content_block[\"text\"]})\n",
    "            elif content_block[\"type\"] == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "                )\n",
    "            elif content_block[\"type\"] == \"tool_call\":\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = content\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample) -> List[Dict[str, str]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    result = {\n",
    "        \"input\": {\"messages\": [], \"tools\": [], \"parallel_tool_calls\": True},\n",
    "        \"preferred_output\": [],\n",
    "        \"non_preferred_output\": [],\n",
    "    }\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "        else:\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        result[\"input\"][\"messages\"].extend(render_message(message))\n",
    "\n",
    "    # Add the demonstration (preferred output)\n",
    "    preferred_output = json.loads(sample[\"preferred_output\"])\n",
    "    rendered_preferred_output = render_output(preferred_output)\n",
    "    if rendered_preferred_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    result[\"preferred_output\"].append(rendered_preferred_output)\n",
    "\n",
    "    # Add the inference output (non-preferred output)\n",
    "    non_preferred_output = json.loads(sample[\"non_preferred_output\"])\n",
    "    rendered_non_preferred_output = render_output(non_preferred_output)\n",
    "    if rendered_non_preferred_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    result[\"non_preferred_output\"].append(rendered_non_preferred_output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "df[\"openai_messages\"] = df.apply(sample_to_openai_messages, axis=1)\n",
    "\n",
    "# Drop null rows\n",
    "df = df[df[\"openai_messages\"].notna()]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dpo(example):\n",
    "    return {\n",
    "        \"input\": {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": example[\"prompt\"]}]\n",
    "        },\n",
    "        \"preferred_output\": [{\"role\": \"assistant\", \"content\": example[\"completion\"]}],\n",
    "        \"non_preferred_output\": [{\"role\": \"assistant\", \"content\": json.loads(example[\"non_preferred_output\"])[\"raw\"]}]\n",
    "    }\n",
    "\n",
    "dpo_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        input_data = json.loads(row[\"input\"])\n",
    "        output_data = json.loads(row[\"preferred_output\"])\n",
    "        prompt = input_data[\"messages\"][-1][\"content\"]\n",
    "        completion = output_data[\"raw\"]\n",
    "        dpo_rows.append({\"prompt\": prompt, \"completion\": completion, \"non_preferred_output\": row[\"non_preferred_output\"]})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "dpo_dataset = [format_dpo(r) for r in dpo_rows]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88ab92",
   "metadata": {},
   "source": [
    "## (Data format would go here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d92c31",
   "metadata": {},
   "source": [
    "Upload the prepared datasets to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_openai(df, openai_client) -> str:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n",
    "        for item in df[\"openai_messages\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        print(f\"File persisted on path [{f.name}]\")\n",
    "\n",
    "        with open(f.name, \"rb\") as file:\n",
    "            file_object = openai_client.files.create(file=file, purpose=\"fine-tune\")\n",
    "\n",
    "        return file_object.id\n",
    "\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "dpo_fine_tuning_object_id = upload_dataset_to_openai(train_df, openai_client)\n",
    "val_file_object_id = upload_dataset_to_openai(val_df, openai_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6242ec9",
   "metadata": {},
   "source": [
    "Launch the fine-tuning job and wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb70fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
    "    training_file=dpo_fine_tuning_object_id,\n",
    "    validation_file=val_file_object_id,\n",
    "    model=MODEL_NAME,\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.2},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "        pprint(job_status.to_dict())\n",
    "        if job_status.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"The fine-tuning job has compeleted with result {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe01ba2",
   "metadata": {},
   "source": [
    "TODO: Adding the fine-tuned model to the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = job_status.fine_tuned_model\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"openai\"],\n",
    "            \"providers\": {\"openai\": {\"type\": \"openai\", \"model_name\": fine_tuned_model}},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba9979",
   "metadata": {},
   "source": [
    "TODO: Adding a new variant to your function to use the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3576241",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"model\": fine_tuned_model,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {fine_tuned_model: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
