{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00dfeb0",
   "metadata": {},
   "source": [
    "## Example: NER + SFT + DPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cb0ae",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "767c156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from IPython.display import clear_output\n",
    "from minijinja import Environment\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "#from dotenv import load_dotenv\n",
    "\n",
    "#load_dotenv()  #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad0d90",
   "metadata": {},
   "source": [
    "####  IMPORTANT: Update the gateway URL below if you're not using the standard setup provided in this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22ddc355",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORZERO_GATEWAY_URL = \"http://localhost:3000\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec60582",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8c24063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only a subset of the dataset to speed things up\n",
    "#NUM_TRAIN_DATAPOINTS = 500\n",
    "#NUM_VAL_DATAPOINTS = 500\n",
    "\n",
    "# test with smaller samples\n",
    "NUM_TRAIN_DATAPOINTS = 100\n",
    "NUM_VAL_DATAPOINTS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9ffb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path: str) -> (pd.DataFrame, pd.DataFrame):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(path)\n",
    "    df.output = df.output.apply(json.loads)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    train_df = df[df[\"split\"] == 0]\n",
    "    val_df = df[df[\"split\"] == 1]\n",
    "\n",
    "    # Shuffle the splits\n",
    "    train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    val_df = val_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "    # Select only a subset of the dataset to speed things up\n",
    "    train_df = train_df.iloc[:NUM_TRAIN_DATAPOINTS]\n",
    "    val_df = val_df.iloc[:NUM_VAL_DATAPOINTS]\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ff1eacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (100, 5)\n",
      "Validation data shape: (100, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = load_dataset(\"data/conllpp.csv\")\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de6ba5",
   "metadata": {},
   "source": [
    "### Extract Entities\n",
    "IMPORTANT: REduce the number of concurrent request if you're running into rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81bf8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CONCURRENT_REQUESTS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ab6cb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = await AsyncTensorZeroGateway.build_http(\n",
    "    gateway_url=TENSORZERO_GATEWAY_URL, timeout=15\n",
    ")\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94e096eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_entities(\n",
    "    text: str,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> Optional[InferenceResponse]:\n",
    "    # Use a semaphore to avoid rate limits\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            return await tensorzero_client.inference(\n",
    "                function_name=\"extract_entities\",\n",
    "                input={\"messages\": [{\"role\": \"user\", \"content\": text}]},\n",
    "                dryrun=dryrun,\n",
    "                variant_name=variant_name,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {type(e).__name__}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74de52a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:19<00:00,  5.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run inference in parallel to speed things up\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[get_entities(text) for text in train_df[\"input\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c98900",
   "metadata": {},
   "source": [
    "### Evaluate the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3eb8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, List[str]]) -> List[str]:\n",
    "    res = []\n",
    "    for k, v in d.items():\n",
    "        assert isinstance(v, list)\n",
    "        for elt in v:\n",
    "            res.append(f\"__{k.upper()}__::{elt}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "# Exact match between the predicted and ground truth entities (the sharpest metric we use to evaluate NER)\n",
    "def compute_exact_match(\n",
    "    predicted: Dict[str, List[str]], ground_truth: Dict[str, List[str]]\n",
    ") -> bool:\n",
    "    return set(flatten_dict(predicted)) == set(flatten_dict(ground_truth))\n",
    "\n",
    "# Jaccard similarity between the predicted and ground_truth entities\n",
    "# (a more lenient metric that gives partial credit for correct entities)\n",
    "# This is a different implementation from the original code by Predibase, so the metrics won't be directly comparable.\n",
    "def compute_jaccard_similarity(\n",
    "    predicted: Dict[str, List[str]], ground_truth: Dict[str, List[str]]\n",
    ") -> float:\n",
    "    target_entities = flatten_dict(ground_truth)\n",
    "    pred_entities = flatten_dict(predicted)\n",
    "    target_count = Counter(target_entities)\n",
    "    pred_count = Counter(pred_entities)\n",
    "    num = 0\n",
    "    den = 0\n",
    "    all_keys = set(target_entities).union(set(pred_entities))\n",
    "    for key in all_keys:\n",
    "        num += min(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "        den += max(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "    if den == 0:\n",
    "        return 1\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2f996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(\n",
    "    response: Optional[InferenceResponse], ground_truth_data: Dict[str, List[str]]\n",
    "):\n",
    "    predicted = response.output.parsed if response else None\n",
    "\n",
    "    # `predicted` is None if the model failed to return a valid JSON that complies with the output schema\n",
    "    valid_output = predicted is not None\n",
    "\n",
    "    # Compute the other metrics\n",
    "    exact_match = (\n",
    "        compute_exact_match(predicted, ground_truth_data) if predicted else False\n",
    "    )\n",
    "    jaccard_similarity = (\n",
    "        compute_jaccard_similarity(predicted, ground_truth_data) if predicted else 0\n",
    "    )\n",
    "\n",
    "    return valid_output, exact_match, jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "312ab262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 96.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for response, ground_truth in tqdm(\n",
    "    zip(responses, train_df[\"output\"]), total=len(responses)\n",
    "):\n",
    "    # Don't send feedback if the request failed completely\n",
    "    if response is None:\n",
    "        continue\n",
    "\n",
    "    # Evaluate the example\n",
    "    valid_output, exact_match, jaccard_similarity = evaluate_response(\n",
    "        response, ground_truth\n",
    "    )\n",
    "\n",
    "    # Send the metrics feedback to TensorZero\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"valid_output\",\n",
    "        value=valid_output,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"exact_match\",\n",
    "        value=exact_match,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"jaccard_similarity\",\n",
    "        value=jaccard_similarity,\n",
    "        inference_id=response.inference_id,\n",
    "    )\n",
    "\n",
    "    # Send the demonstration feedback to TensorZero\n",
    "    await tensorzero_client.feedback(\n",
    "        metric_name=\"demonstration\",\n",
    "        value=ground_truth,\n",
    "        inference_id=response.inference_id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c534c3d",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "IMPORTANT: Update the list blow when you create new variants in `tensorzero.toml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d46b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the variants in `tensorzero.toml` that we want to evaluate\n",
    "VARIANTS_TO_EVALUATE = [\n",
    "    \"gpt_4o\",\n",
    "    \"gpt_4o_mini\",\n",
    "    \"gpt_4o_mini_fine_tuned\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "771b2b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating variant: gpt_4o: 100%|██████████| 100/100 [00:13<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Output: 100.0%\n",
      "Exact Match: 56.0%\n",
      "Jaccard Similarity (mean): 66.0%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating variant: gpt_4o_mini: 100%|██████████| 100/100 [00:14<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Output: 100.0%\n",
      "Exact Match: 18.0%\n",
      "Jaccard Similarity (mean): 38.0%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating variant: gpt_4o_mini_fine_tuned:  42%|████▏     | 42/100 [00:04<00:05, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-04T18:28:09.350219Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_internal::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "Error occurred: TensorZeroError: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: gpt_4o_mini_fine_tuned: All model providers failed to infer with errors: openai: Error from openai server: {\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_eaf1f6a1092843a9b9aeb4773adb822c in your email.)\\\",\\n    \\\"type\\\": \\\"server_error\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating variant: gpt_4o_mini_fine_tuned:  96%|█████████▌| 96/100 [00:09<00:00, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-08-04T18:28:13.952519Z\u001b[0m \u001b[33m WARN\u001b[0m \u001b[2mtensorzero_internal::error\u001b[0m\u001b[2m:\u001b[0m Request failed: HTTP status server error (502 Bad Gateway) for url (http://localhost:3000/inference)\n",
      "Error occurred: TensorZeroError: TensorZeroError (status code 502): {\"error\":\"All variants failed with errors: gpt_4o_mini_fine_tuned: All model providers failed to infer with errors: openai: Error from openai server: {\\n  \\\"error\\\": {\\n    \\\"message\\\": \\\"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_5fa658c6898c4d5983c74b14abfaaeb8 in your email.)\\\",\\n    \\\"type\\\": \\\"server_error\\\",\\n    \\\"param\\\": null,\\n    \\\"code\\\": null\\n  }\\n}\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating variant: gpt_4o_mini_fine_tuned: 100%|██████████| 100/100 [00:09<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Output: 98.0%\n",
      "Exact Match: 71.0%\n",
      "Jaccard Similarity (mean): 77.3%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = {}  # variant_name => (valid_output, exact_match, jaccard_similarity)\n",
    "\n",
    "for variant_name in VARIANTS_TO_EVALUATE:\n",
    "    # Run inference on the validation set\n",
    "    responses = await tqdm_asyncio.gather(\n",
    "        *[\n",
    "            get_entities(\n",
    "                text,\n",
    "                variant_name=variant_name,  # pin to the specific variant we want to evaluate\n",
    "                dryrun=True,  # don't store results to avoid leaking data\n",
    "            )\n",
    "            for text in val_df[\"input\"]\n",
    "        ],\n",
    "        desc=f\"Evaluating variant: {variant_name}\",\n",
    "    )\n",
    "\n",
    "    # Evaluate the performance of the variant\n",
    "    valid_output_scores = []\n",
    "    exact_match_scores = []\n",
    "    jaccard_similarity_scores = []\n",
    "\n",
    "    for response, ground_truth in zip(responses, val_df[\"output\"]):\n",
    "        valid_output, exact_match, jaccard_similarity = evaluate_response(\n",
    "            response, ground_truth\n",
    "        )\n",
    "        valid_output_scores.append(valid_output)\n",
    "        exact_match_scores.append(exact_match)\n",
    "        jaccard_similarity_scores.append(jaccard_similarity)\n",
    "\n",
    "    scores[variant_name] = {\n",
    "        \"valid_output\": valid_output_scores,\n",
    "        \"exact_match\": exact_match_scores,\n",
    "        \"jaccard_similarity\": jaccard_similarity_scores,\n",
    "    }\n",
    "\n",
    "    # Print the performance of the variant\n",
    "    print(f\"Valid Output: {sum(valid_output_scores) / len(valid_output_scores):.1%}\")\n",
    "    print(f\"Exact Match: {sum(exact_match_scores) / len(exact_match_scores):.1%}\")\n",
    "    print(\n",
    "        f\"Jaccard Similarity (mean): {sum(jaccard_similarity_scores) / len(jaccard_similarity_scores):.1%}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394918e3",
   "metadata": {},
   "source": [
    "### Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec425445",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = []\n",
    "\n",
    "for variant_name, variant_scores in scores.items():\n",
    "    exact_match_score = sum(variant_scores[\"exact_match\"]) / len(\n",
    "        variant_scores[\"exact_match\"]\n",
    "    )\n",
    "    scores_df.append(\n",
    "        {\n",
    "            \"Variant\": variant_name,\n",
    "            \"Metric\": \"exact_match\",\n",
    "            \"Score\": exact_match_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    jaccard_similarity_score = sum(variant_scores[\"jaccard_similarity\"]) / len(\n",
    "        variant_scores[\"jaccard_similarity\"]\n",
    "    )\n",
    "\n",
    "    scores_df.append(\n",
    "        {\n",
    "            \"Variant\": variant_name,\n",
    "            \"Metric\": \"jaccard_similarity\",\n",
    "            \"Score\": jaccard_similarity_score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "scores_df = pd.DataFrame(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6931650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-e5743c1c733d4e6c904f6718a656b324.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-e5743c1c733d4e6c904f6718a656b324.vega-embed details,\n",
       "  #altair-viz-e5743c1c733d4e6c904f6718a656b324.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-e5743c1c733d4e6c904f6718a656b324\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-e5743c1c733d4e6c904f6718a656b324\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-e5743c1c733d4e6c904f6718a656b324\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Metric\", \"type\": \"nominal\"}, \"text\": {\"field\": \"Score\", \"format\": \".1%\", \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"format\": \"%\"}, \"field\": \"Score\", \"scale\": {\"domain\": [0, 1]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"Variant\", \"type\": \"nominal\"}, \"yOffset\": {\"field\": \"Metric\", \"type\": \"nominal\"}}, \"title\": \"Metrics by Variant\"}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"dx\": 2}, \"encoding\": {\"color\": {\"field\": \"Metric\", \"type\": \"nominal\"}, \"text\": {\"field\": \"Score\", \"format\": \".1%\", \"type\": \"quantitative\"}, \"x\": {\"axis\": {\"format\": \"%\"}, \"field\": \"Score\", \"scale\": {\"domain\": [0, 1]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"Variant\", \"type\": \"nominal\"}, \"yOffset\": {\"field\": \"Metric\", \"type\": \"nominal\"}}, \"title\": \"Metrics by Variant\"}], \"data\": {\"name\": \"data-bba9216f68421dce37209021e76b45a6\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-bba9216f68421dce37209021e76b45a6\": [{\"Variant\": \"gpt_4o\", \"Metric\": \"exact_match\", \"Score\": 0.56}, {\"Variant\": \"gpt_4o\", \"Metric\": \"jaccard_similarity\", \"Score\": 0.6601349206349205}, {\"Variant\": \"gpt_4o_mini\", \"Metric\": \"exact_match\", \"Score\": 0.18}, {\"Variant\": \"gpt_4o_mini\", \"Metric\": \"jaccard_similarity\", \"Score\": 0.37998137973137974}, {\"Variant\": \"gpt_4o_mini_fine_tuned\", \"Metric\": \"exact_match\", \"Score\": 0.71}, {\"Variant\": \"gpt_4o_mini_fine_tuned\", \"Metric\": \"jaccard_similarity\", \"Score\": 0.7730555555555556}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = (\n",
    "    alt.Chart(scores_df)\n",
    "    .encode(\n",
    "        x=alt.X(\"Score:Q\", axis=alt.Axis(format=\"%\"), scale=alt.Scale(domain=[0, 1])),\n",
    "        y=\"Variant:N\",\n",
    "        yOffset=\"Metric:N\",\n",
    "        color=\"Metric:N\",\n",
    "        text=alt.Text(\"Score:Q\", format=\".1%\"),\n",
    "    )\n",
    "    .properties(title=\"Metrics by Variant\")\n",
    ")\n",
    "\n",
    "chart = chart.mark_bar() + chart.mark_text(align=\"left\", dx=2)\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "703152d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"config/tensorzero.toml\"\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"\n",
    "MODEL_NAME = \"gpt-4o-2024-08-06\"\n",
    "VAL_FRACTION = 0.2\n",
    "MAX_SAMPLES = 500\n",
    "\n",
    "\n",
    "\n",
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ\n",
    "\n",
    "#openai_client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bd53c",
   "metadata": {},
   "source": [
    "### STEP 1: SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42b3fd",
   "metadata": {},
   "source": [
    "### STEP 2: DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fc2f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "#config = json.load(open(CONFIG_PATH)) if CONFIG_PATH.endswith(\".json\") else {}\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242a6a5",
   "metadata": {},
   "source": [
    "Ensure that the function and variant being fine-tuned are present in the provided config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c1c9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01cc42",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we will use for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "247f9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63881094",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28ede1",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d462c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9052e80",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ec0d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"json\": \"JsonInference\"}.get(function_type)\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663f9e9",
   "metadata": {},
   "source": [
    "Query ClickHouse for inference, feedback, and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f453bac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         variant                            episode_id  \\\n",
      "0    gpt_4o_mini  019875b2-c8a2-7cc3-9fc9-7c469d534347   \n",
      "1    gpt_4o_mini  019875b2-cc57-7ea1-8109-d8cc0b333b8d   \n",
      "2    gpt_4o_mini  019875b2-cc58-7bd2-9467-184f2ab7d99f   \n",
      "3         gpt_4o  019875b2-c5e4-79e1-a27f-435d8e103a4f   \n",
      "4         gpt_4o  019875b2-a79f-7ef3-9d29-bc052839d23e   \n",
      "..           ...                                   ...   \n",
      "115  gpt_4o_mini  019875ad-01b0-7393-91ee-674007b2508e   \n",
      "116  gpt_4o_mini  019875b2-c804-77e1-9883-70d1d3f8dc82   \n",
      "117  gpt_4o_mini  019875b2-a151-7123-a04f-05fc6581f804   \n",
      "118  gpt_4o_mini  019875b2-b128-7251-a358-56a1969e281a   \n",
      "119  gpt_4o_mini  019875b2-a358-7cc3-b2d6-c5a09d3b7149   \n",
      "\n",
      "                                                 input  \\\n",
      "0    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "1    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "2    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "3    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "4    {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "..                                                 ...   \n",
      "115  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "116  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "117  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "118  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "119  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
      "\n",
      "                                  non_preferred_output  \\\n",
      "0    {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
      "1    {\"raw\":\"{\\n    \\\"person\\\": [\\\"Thomas Radstrom\\...   \n",
      "2    {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
      "3    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"ECO...   \n",
      "4    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
      "..                                                 ...   \n",
      "115  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
      "116  {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
      "117  {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
      "118  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
      "119  {\"raw\":\"{\\n    \\\"person\\\": [\\\"Weaver\\\"],\\n    ...   \n",
      "\n",
      "                                      preferred_output  \n",
      "0    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "1    {\"raw\":\"{\\\"person\\\":[\\\"Thomas Radstrom\\\"],\\\"or...  \n",
      "2    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "3    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"ECO...  \n",
      "4    {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "..                                                 ...  \n",
      "115  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"TOR...  \n",
      "116  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"OLY...  \n",
      "117  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "118  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...  \n",
      "119  {\"raw\":\"{\\\"person\\\":[\\\"Weaver\\\"],\\\"organizatio...  \n",
      "\n",
      "[120 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Query ClickHouse for data\n",
    "# ---------------------------\n",
    "query = f\"\"\"\n",
    "SELECT\n",
    "    i.variant_name AS variant,\n",
    "    i.episode_id AS episode_id,\n",
    "    i.input AS input,\n",
    "    i.output AS non_preferred_output,\n",
    "    d.value AS preferred_output\n",
    "FROM\n",
    "    JsonInference AS i\n",
    "INNER JOIN DemonstrationFeedback AS d ON i.id = d.inference_id\n",
    "WHERE\n",
    "    (i.function_name = %(function_name)s)\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\"function_name\": FUNCTION_NAME, \"max_samples\": MAX_SAMPLES}\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "df.head()\n",
    "print(df)\n",
    "#------------------------\n",
    "\n",
    "query1 = f\"\"\"\n",
    "SELECT DISTINCT function_name\n",
    "FROM {inference_table_name}\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df_debug = clickhouse_client.query_df(query1)\n",
    "#print(df_debug)\n",
    "\n",
    "tables = clickhouse_client.query_df(\"SHOW TABLES\")\n",
    "#print(tables)\n",
    "#print(clickhouse_client.query_df(\"DESCRIBE TABLE JsonInference\"))\n",
    "\n",
    "#print(clickhouse_client.query_df(\"SELECT * FROM JsonInference LIMIT 5\"))\n",
    "\n",
    "#print(clickhouse_client.query_df(\"SELECT count() FROM JsonInference\"))\n",
    "#print(clickhouse_client.query_df(\"SELECT count() FROM ModelInference\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d65aba",
   "metadata": {},
   "source": [
    "render message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7154792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>input</th>\n",
       "      <th>non_preferred_output</th>\n",
       "      <th>preferred_output</th>\n",
       "      <th>openai_messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_4o_mini</td>\n",
       "      <td>019875b2-c8a2-7cc3-9fc9-7c469d534347</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>{\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...</td>\n",
       "      <td>{\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...</td>\n",
       "      <td>{'input': {'messages': [{'role': 'system', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_4o_mini</td>\n",
       "      <td>019875b2-cc57-7ea1-8109-d8cc0b333b8d</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>{\"raw\":\"{\\n    \\\"person\\\": [\\\"Thomas Radstrom\\...</td>\n",
       "      <td>{\"raw\":\"{\\\"person\\\":[\\\"Thomas Radstrom\\\"],\\\"or...</td>\n",
       "      <td>{'input': {'messages': [{'role': 'system', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt_4o_mini</td>\n",
       "      <td>019875b2-cc58-7bd2-9467-184f2ab7d99f</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>{\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...</td>\n",
       "      <td>{\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...</td>\n",
       "      <td>{'input': {'messages': [{'role': 'system', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>019875b2-c5e4-79e1-a27f-435d8e103a4f</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>{\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"ECO...</td>\n",
       "      <td>{\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"ECO...</td>\n",
       "      <td>{'input': {'messages': [{'role': 'system', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>019875b2-a79f-7ef3-9d29-bc052839d23e</td>\n",
       "      <td>{\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...</td>\n",
       "      <td>{\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...</td>\n",
       "      <td>{\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...</td>\n",
       "      <td>{'input': {'messages': [{'role': 'system', 'co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       variant                            episode_id  \\\n",
       "0  gpt_4o_mini  019875b2-c8a2-7cc3-9fc9-7c469d534347   \n",
       "1  gpt_4o_mini  019875b2-cc57-7ea1-8109-d8cc0b333b8d   \n",
       "2  gpt_4o_mini  019875b2-cc58-7bd2-9467-184f2ab7d99f   \n",
       "3       gpt_4o  019875b2-c5e4-79e1-a27f-435d8e103a4f   \n",
       "4       gpt_4o  019875b2-a79f-7ef3-9d29-bc052839d23e   \n",
       "\n",
       "                                               input  \\\n",
       "0  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "1  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "2  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "3  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "4  {\"messages\":[{\"role\":\"user\",\"content\":[{\"type\"...   \n",
       "\n",
       "                                non_preferred_output  \\\n",
       "0  {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
       "1  {\"raw\":\"{\\n    \\\"person\\\": [\\\"Thomas Radstrom\\...   \n",
       "2  {\"raw\":\"{\\n    \\\"person\\\": [],\\n    \\\"organiza...   \n",
       "3  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"ECO...   \n",
       "4  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
       "\n",
       "                                    preferred_output  \\\n",
       "0  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
       "1  {\"raw\":\"{\\\"person\\\":[\\\"Thomas Radstrom\\\"],\\\"or...   \n",
       "2  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
       "3  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[\\\"ECO...   \n",
       "4  {\"raw\":\"{\\\"person\\\":[],\\\"organization\\\":[],\\\"l...   \n",
       "\n",
       "                                     openai_messages  \n",
       "0  {'input': {'messages': [{'role': 'system', 'co...  \n",
       "1  {'input': {'messages': [{'role': 'system', 'co...  \n",
       "2  {'input': {'messages': [{'role': 'system', 'co...  \n",
       "3  {'input': {'messages': [{'role': 'system', 'co...  \n",
       "4  {'input': {'messages': [{'role': 'system', 'co...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def render_message(message: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message[\"role\"]\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message[\"content\"]:\n",
    "        if content_block[\"type\"] == \"text\":\n",
    "            parsed_content = content_block[\"value\"]\n",
    "            if not isinstance(parsed_content, str):\n",
    "                parsed_content = env.render_template(role, **parsed_content)\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block[\"type\"] == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block[\"value\"]})\n",
    "        elif content_block[\"type\"] == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_call\" and role == \"assistant\":\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_result\" and role == \"user\":\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            role_message[\"content\"] = content\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output: List[Dict[str, Any]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    if function_type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    elif function_type == \"chat\":\n",
    "        for content_block in output:\n",
    "            if content_block[\"type\"] == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": content_block[\"text\"]})\n",
    "            elif content_block[\"type\"] == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "                )\n",
    "            elif content_block[\"type\"] == \"tool_call\":\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = content\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample) -> List[Dict[str, str]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    result = {\n",
    "        \"input\": {\"messages\": [], \"tools\": [], \"parallel_tool_calls\": True},\n",
    "        \"preferred_output\": [],\n",
    "        \"non_preferred_output\": [],\n",
    "    }\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "        else:\n",
    "            result[\"input\"][\"messages\"].append(\n",
    "                {\"role\": \"system\", \"content\": system_message}\n",
    "            )\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        result[\"input\"][\"messages\"].extend(render_message(message))\n",
    "\n",
    "    # Add the demonstration (preferred output)\n",
    "    preferred_output = json.loads(sample[\"preferred_output\"])\n",
    "    rendered_preferred_output = render_output(preferred_output)\n",
    "    if rendered_preferred_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    result[\"preferred_output\"].append(rendered_preferred_output)\n",
    "\n",
    "    # Add the inference output (non-preferred output)\n",
    "    non_preferred_output = json.loads(sample[\"non_preferred_output\"])\n",
    "    rendered_non_preferred_output = render_output(non_preferred_output)\n",
    "    if rendered_non_preferred_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    result[\"non_preferred_output\"].append(rendered_non_preferred_output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "df[\"openai_messages\"] = df.apply(sample_to_openai_messages, axis=1)\n",
    "\n",
    "# Drop null rows\n",
    "df = df[df[\"openai_messages\"].notna()]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f67e6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dpo(example):\n",
    "    return {\n",
    "        \"input\": {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": example[\"prompt\"]}]\n",
    "        },\n",
    "        \"preferred_output\": [{\"role\": \"assistant\", \"content\": example[\"completion\"]}],\n",
    "        \"non_preferred_output\": [{\"role\": \"assistant\", \"content\": json.loads(example[\"non_preferred_output\"])[\"raw\"]}]\n",
    "    }\n",
    "\n",
    "dpo_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        input_data = json.loads(row[\"input\"])\n",
    "        output_data = json.loads(row[\"preferred_output\"])\n",
    "        prompt = input_data[\"messages\"][-1][\"content\"]\n",
    "        completion = output_data[\"raw\"]\n",
    "        dpo_rows.append({\"prompt\": prompt, \"completion\": completion, \"non_preferred_output\": row[\"non_preferred_output\"]})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "dpo_dataset = [format_dpo(r) for r in dpo_rows]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88ab92",
   "metadata": {},
   "source": [
    "## (Data format would go here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d92c31",
   "metadata": {},
   "source": [
    "Upload the prepared datasets to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_dataset_to_openai(df, openai_client) -> str:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".jsonl\", delete=False) as f:\n",
    "        for item in df[\"openai_messages\"]:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "        f.flush()\n",
    "\n",
    "        print(f\"File persisted on path [{f.name}]\")\n",
    "\n",
    "        with open(f.name, \"rb\") as file:\n",
    "            file_object = openai_client.files.create(file=file, purpose=\"fine-tune\")\n",
    "\n",
    "        return file_object.id\n",
    "\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "dpo_fine_tuning_object_id = upload_dataset_to_openai(train_df, openai_client)\n",
    "val_file_object_id = upload_dataset_to_openai(val_df, openai_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6242ec9",
   "metadata": {},
   "source": [
    "Launch the fine-tuning job and wait for it to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb70fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_job = openai_client.fine_tuning.jobs.create(\n",
    "    training_file=dpo_fine_tuning_object_id,\n",
    "    validation_file=val_file_object_id,\n",
    "    model=MODEL_NAME,\n",
    "    method={\n",
    "        \"type\": \"dpo\",\n",
    "        \"dpo\": {\n",
    "            \"hyperparameters\": {\"beta\": 0.2},\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    try:\n",
    "        job_status = openai_client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "        pprint(job_status.to_dict())\n",
    "        if job_status.status in (\"succeeded\", \"failed\", \"cancelled\"):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "print(f\"The fine-tuning job has compeleted with result {job_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe01ba2",
   "metadata": {},
   "source": [
    "TODO: Adding the fine-tuned model to the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = job_status.fine_tuned_model\n",
    "model_config = {\n",
    "    \"models\": {\n",
    "        fine_tuned_model: {\n",
    "            \"routing\": [\"openai\"],\n",
    "            \"providers\": {\"openai\": {\"type\": \"openai\", \"model_name\": fine_tuned_model}},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(toml.dumps(model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba9979",
   "metadata": {},
   "source": [
    "TODO: Adding a new variant to your function to use the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3576241",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_config = {\n",
    "    \"type\": \"chat_completion\",\n",
    "    \"model\": fine_tuned_model,\n",
    "}\n",
    "\n",
    "system_template = variant.get(\"system_template\")\n",
    "if system_template:\n",
    "    variant_config[\"system_template\"] = system_template\n",
    "\n",
    "user_template = variant.get(\"user_template\")\n",
    "if user_template:\n",
    "    variant_config[\"user_template\"] = user_template\n",
    "\n",
    "assistant_template = variant.get(\"assistant_template\")\n",
    "if assistant_template:\n",
    "    variant_config[\"assistant_template\"] = assistant_template\n",
    "\n",
    "full_variant_config = {\n",
    "    \"functions\": {FUNCTION_NAME: {\"variants\": {fine_tuned_model: variant_config}}}\n",
    "}\n",
    "\n",
    "print(toml.dumps(full_variant_config))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
