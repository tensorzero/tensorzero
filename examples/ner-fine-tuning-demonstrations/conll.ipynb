{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import neatplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from clickhouse_connect import get_client\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from tensorzero import AsyncTensorZeroGateway, InferenceResponse\n",
    "from tqdm.asyncio import tqdm_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neatplot.set_style(\"notex\")\n",
    "CLICKHOUSE_URL = os.getenv(\"CLICKHOUSE_URL\")\n",
    "\n",
    "# Example: \"http://localhost:8123/tensorzero\" (\"https://user:password@host:port/database\")\n",
    "assert CLICKHOUSE_URL is not None, \"CLICKHOUSE_URL is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorzero_client = AsyncTensorZeroGateway(\"http://localhost:3000\", timeout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the CSV file provided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"conllpp.csv\")\n",
    "df.head()\n",
    "df.output = df.output.apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"split\"] == 0]\n",
    "val_df = df[df[\"split\"] == 1]\n",
    "test_df = df[df[\"split\"] == 2]\n",
    "\n",
    "# Shuffle the splits\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "val_df = val_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines the function that we'll actually use to extract entities from text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We retry the inference in case of a timeout and hide this in an inner function\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_random_exponential(multiplier=1, max=10))\n",
    "async def _get_entities(\n",
    "    text: str,\n",
    "    client: AsyncTensorZeroGateway,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> InferenceResponse:\n",
    "    return await client.inference(\n",
    "        function_name=\"extract_entities\",\n",
    "        input={\"messages\": [{\"role\": \"user\", \"content\": text}]},\n",
    "        dryrun=dryrun,\n",
    "        variant_name=variant_name,\n",
    "    )\n",
    "\n",
    "\n",
    "# Call this function to get the entities from the text\n",
    "async def get_entities(\n",
    "    text: str,\n",
    "    client: AsyncTensorZeroGateway,\n",
    "    variant_name: Optional[str] = None,\n",
    "    dryrun: bool = False,\n",
    ") -> Optional[InferenceResponse]:\n",
    "    try:\n",
    "        return await _get_entities(text, client, variant_name, dryrun)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two code blocks we define two methods of evaluating the performance of an NER model: Exact Match and Jaccard Similarity.\n",
    "We will use these metrics to evaluate the performance of each variant of our model.\n",
    "Our Jaccard similarity metric gives partial credit and is more lenient than exact match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, List[str]]) -> List[str]:\n",
    "    res = []\n",
    "    for k, v in d.items():\n",
    "        assert isinstance(v, list)\n",
    "        for elt in v:\n",
    "            res.append(f\"__{k.upper()}__::{elt}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match between the predicted and gold entities (the sharpest metric we use to evaluate NER)\n",
    "def exact_match(predicted: Dict[str, List[str]], gold: Dict[str, List[str]]) -> bool:\n",
    "    return set(flatten_dict(predicted)) == set(flatten_dict(gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity between the predicted and gold entities\n",
    "# (a more lenient metric that gives partial credit for correct entities)\n",
    "# NOTE: This is a different implementation from the original code by Predibase, so the metrics won't be directly comparable.\n",
    "def jaccard_similarity(\n",
    "    predicted: Dict[str, List[str]], gold: Dict[str, List[str]]\n",
    ") -> float:\n",
    "    target_entities = flatten_dict(gold)\n",
    "    pred_entities = flatten_dict(predicted)\n",
    "    target_count = Counter(target_entities)\n",
    "    pred_count = Counter(pred_entities)\n",
    "    num = 0\n",
    "    den = 0\n",
    "    all_keys = set(target_entities).union(set(pred_entities))\n",
    "    for key in all_keys:\n",
    "        num += min(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "        den += max(target_count.get(key, 0), pred_count.get(key, 0))\n",
    "    if den == 0:\n",
    "        return 1\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll run inference using TensorZero on the training set to collect data for training future variants.\n",
    "We will evaluate the predictions on the training set and send the feedback to TensorZero as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these to run inference on more or fewer examples\n",
    "# or to respect your provider's rate limits\n",
    "NUM_TRAIN_PREDICTIONS = 100\n",
    "MAX_CONCURRENT_REQUESTS = 20\n",
    "\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def make_inference(text: str, client: AsyncTensorZeroGateway):\n",
    "    async with semaphore:\n",
    "        return await get_entities(text, client)\n",
    "\n",
    "\n",
    "# We run inference in parallel to speed things up\n",
    "responses = await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        make_inference(text, tensorzero_client)\n",
    "        for text in train_df[\"input\"][:NUM_TRAIN_PREDICTIONS]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(response: Optional[InferenceResponse], gold_data: Dict[str, List[str]]):\n",
    "    predicted = response.output.parsed if response else None\n",
    "    valid_json = predicted is not None\n",
    "    matched = exact_match(predicted, gold_data) if predicted else False\n",
    "    jaccard = jaccard_similarity(predicted, gold_data) if predicted else 0\n",
    "    return valid_json, matched, jaccard\n",
    "\n",
    "\n",
    "async def evaluate_send_feedback(\n",
    "    response: Optional[InferenceResponse], gold_data: Dict[str, List[str]]\n",
    "):\n",
    "    valid_json, matched, jaccard = evaluate(response, gold_data)\n",
    "    async with semaphore:\n",
    "        feedback_tasks = [\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"valid_json\",\n",
    "                value=valid_json,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"exact_match\",\n",
    "                value=matched,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "            tensorzero_client.feedback(\n",
    "                metric_name=\"jaccard_similarity\",\n",
    "                value=jaccard,\n",
    "                inference_id=response.inference_id,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        if not matched:\n",
    "            # Send the demonstration to TensorZero as a serialized JSON string\n",
    "            feedback_tasks.append(\n",
    "                tensorzero_client.feedback(\n",
    "                    metric_name=\"demonstration\",\n",
    "                    value=gold_data,\n",
    "                    inference_id=response.inference_id,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        await asyncio.gather(*feedback_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the predictions on the training set and send the feedback to TensorZero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await tqdm_asyncio.gather(\n",
    "    *[\n",
    "        evaluate_send_feedback(response, gold)\n",
    "        for response, gold in zip(responses, train_df[\"output\"][:NUM_TRAIN_PREDICTIONS])\n",
    "        if response is not None\n",
    "    ]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've collected data on the training set, we can query the database to see how well each variant performed.\n",
    "You should see the performance of the GPT-4o mini variant for each metric.\n",
    "First, we'll check the exact match metric for each variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickhouse_client = get_client(dsn=CLICKHOUSE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"exact_match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference ID\n",
    "df = clickhouse_client.query_df(\n",
    "    \"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    b.value\n",
    "FROM \n",
    "    JsonInference i\n",
    "JOIN \n",
    "    BooleanMetricFeedback b ON i.id = b.target_id\n",
    "WHERE \n",
    "    i.function_name = 'extract_entities'\n",
    "    AND b.metric_name = %(metric_name)s\"\"\",\n",
    "    {\"metric_name\": metric_name},\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average score for each variant\n",
    "df.groupby(\"variant_name\")[\"value\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll check the jaccard similarity metric for each variant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the inferences and feedback from the database and join them on the inference ID\n",
    "df = clickhouse_client.query_df(\n",
    "    \"\"\"SELECT \n",
    "    i.variant_name, \n",
    "    i.input, \n",
    "    i.output, \n",
    "    f.value\n",
    "FROM \n",
    "    JsonInference i\n",
    "JOIN \n",
    "    FloatMetricFeedback f ON i.id = f.target_id\n",
    "WHERE \n",
    "    i.function_name = 'extract_entities'\n",
    "    AND f.metric_name = 'jaccard_similarity'\"\"\",\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the average score for each variant\n",
    "df.groupby(\"variant_name\")[\"value\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you have accumulated a dataset of training \"demonstrations\" for each variant.\n",
    "You should use the TensorZero recipe (at `recipes/supervised_fine_tuning/demonstrations/openai/`) to fine-tune a custom GPT-4o mini model on these demonstrations in order to improve the performance of the model on the test set.\n",
    "After you do so, paste the config output by the notebook into your `tensorzero.toml`, give it a nonzero weight, and restart the gateway to begin testing the new variant and model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_to_evaluate = df[\"variant_name\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll evaluate each variant on the test set. In order to be sure that we're not leaking any data, we'll use the `dryrun` flag to make sure the test set is not leaked here.\n",
    "\n",
    "We will also \"pin\" the `variant_name` for each inference request to ensure that we're evaluating the same variant across all requests for a fair trial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_PREDICTIONS = 500\n",
    "test_set = test_df.iloc[:NUM_TEST_PREDICTIONS]\n",
    "\n",
    "\n",
    "async def make_inference(\n",
    "    text: str, client: AsyncTensorZeroGateway, variant_name: Optional[str] = None\n",
    "):\n",
    "    async with semaphore:\n",
    "        # We use dryrun=True to make sure the test set is not leaked here\n",
    "        return await get_entities(text, client, variant_name=variant_name, dryrun=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_responses = []\n",
    "for variant_name in variants_to_evaluate:\n",
    "    variant_task = tqdm_asyncio.gather(\n",
    "        *[\n",
    "            make_inference(text, tensorzero_client, variant_name=variant_name)\n",
    "            for text in test_set[\"input\"]\n",
    "        ],\n",
    "        desc=f\"Evaluating variant: {variant_name}\",\n",
    "    )\n",
    "    variant_result = await variant_task\n",
    "    variant_responses.append(variant_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll evaluate the performance of each variant on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_data = {}\n",
    "for variant_name, responses in zip(variants_to_evaluate, variant_responses):\n",
    "    jaccards = []\n",
    "    well_formed_jsons = []\n",
    "    exact_matches = []\n",
    "    print(f\"Evaluating variant: {variant_name}\")\n",
    "    print(f\"Number of responses: {len(responses)}\")\n",
    "    for response, gold in zip(responses, test_set[\"output\"]):\n",
    "        valid_json, matched, jaccard = evaluate(response, gold)\n",
    "        jaccards.append(jaccard)\n",
    "        well_formed_jsons.append(valid_json)\n",
    "        exact_matches.append(matched)\n",
    "    variant_data[variant_name] = {\n",
    "        \"jaccard_similarity\": jaccards,\n",
    "        \"well_formed_json\": well_formed_jsons,\n",
    "        \"exact_match\": exact_matches,\n",
    "    }\n",
    "    print(\n",
    "        f\"Average Well-formed JSON: {sum(well_formed_jsons) / len(well_formed_jsons):.1%}\"\n",
    "    )\n",
    "    print(f\"Average Jaccard Similarity: {sum(jaccards) / len(jaccards):.1%}\")\n",
    "    print(f\"Average Exact Match: {sum(exact_matches) / len(exact_matches):.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_ci(variant_data):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    for i, (metric, ax) in enumerate(\n",
    "        [(\"exact_match\", ax1), (\"jaccard_similarity\", ax2)]\n",
    "    ):\n",
    "        means = []\n",
    "        cis = []\n",
    "\n",
    "        for variant, data in variant_data.items():\n",
    "            values = data[metric]\n",
    "            n = len(values)\n",
    "            mean = np.mean(values)\n",
    "            means.append(mean)\n",
    "\n",
    "            if metric == \"exact_match\":\n",
    "                # Binomial test for exact matches\n",
    "                ci_low, ci_high = stats.binomtest(int(sum(values)), n).proportion_ci()\n",
    "            else:\n",
    "                # Normal approximation for Jaccard similarity\n",
    "                se = stats.sem(values)\n",
    "                ci_low, ci_high = stats.t.interval(0.95, n - 1, loc=mean, scale=se)\n",
    "\n",
    "            cis.append((mean - ci_low, ci_high - mean))\n",
    "\n",
    "        x = range(len(variant_data))\n",
    "        ax.bar(x, means, yerr=list(zip(*cis)), capsize=5, alpha=0.8)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(variant_data.keys(), rotation=45, ha=\"right\")\n",
    "        ax.set_title(f\"Average {metric.replace('_', ' ').title()}\")\n",
    "        ax.set_ylim(0, 1)\n",
    "\n",
    "        for i, v in enumerate(means):\n",
    "            ax.text(i, v / 2, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_metrics_with_ci(variant_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
