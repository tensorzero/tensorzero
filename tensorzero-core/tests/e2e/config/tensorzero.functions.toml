[functions.basic_test_model_timeout]
type = "chat"

[functions.basic_test_model_timeout.variants.slow_variant]
type = "chat_completion"
model = "slow_model_timeout"

[functions.basic_test_model_timeout.variants.slow_second_chunk]
type = "chat_completion"
model = "slow_second_chunk_timeout"

[functions.basic_test_variant_timeout]
type = "chat"

[functions.basic_test_variant_timeout.variants.slow_timeout]
type = "chat_completion"
model = "slow"

[functions.basic_test_variant_timeout.variants.slow_timeout.timeouts]
non_streaming = { total_ms = 400 }
streaming = { ttft_ms = 500 }

[functions.basic_test_variant_timeout.variants.slow_second_chunk]
type = "chat_completion"
model = "slow_second_chunk"

[functions.basic_test_variant_timeout.variants.slow_second_chunk.timeouts]
non_streaming = { total_ms = 400 }
streaming = { ttft_ms = 500 }

[functions.basic_test_variant_timeout.variants.best_of_n]
type = "experimental_best_of_n_sampling"
candidates = ["good", "slow_timeout", "reasoner"]

[functions.basic_test_variant_timeout.variants.best_of_n.evaluator]
model = "dummy::best_of_n_0"

[functions.basic_test_variant_timeout.variants.good]
type = "chat_completion"
model = "dummy::good"

[functions.basic_test_variant_timeout.variants.reasoner]
type = "chat_completion"
model = "dummy::reasoner"

# Model provider timeout tests

[functions.basic_test_timeout]
type = "chat"

[functions.basic_test_timeout.variants.slow_second_chunk]
type = "chat_completion"
model = "slow_second_chunk"

[functions.basic_test_timeout.variants.good]
type = "chat_completion"
model = "dummy::good"

[functions.basic_test_timeout.variants.reasoner]
type = "chat_completion"
model = "dummy::reasoner"

[functions.basic_test_timeout.variants.timeout]
type = "chat_completion"
model = "slow_with_timeout"

[functions.basic_test_timeout.variants.best_of_n]
type = "experimental_best_of_n_sampling"
candidates = ["good", "timeout", "reasoner"]

[functions.basic_test_timeout.variants.best_of_n.evaluator]
model = "dummy::best_of_n_0"

[functions.basic_test_timeout.variants.best_of_n_judge_timeout]
type = "experimental_best_of_n_sampling"
candidates = ["good", "reasoner"]

[functions.basic_test_timeout.variants.best_of_n_judge_timeout.evaluator]
model = "slow_with_timeout"

[functions.basic_test_template_no_schema]
type = "chat"

[functions.basic_test_template_no_schema.variants.test]
type = "chat_completion"
model = "dummy::echo_request_messages"
input_wrappers.system = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/system_template.minijinja"
assistant_template = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/assistant_template.minijinja"
user_template = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/user_template.minijinja"
templates.my_custom_template.path = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/my_custom_template.minijinja"

[functions.basic_test_template_no_schema.variants.mixture_of_n]
type = "experimental_mixture_of_n"
candidates = ["test", "test"]

[functions.basic_test_template_no_schema.variants.mixture_of_n.fuser]
model = "dummy::echo_request_messages"
system_template = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/system_template_2.minijinja"
input_wrappers.assistant = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/assistant_template_2.minijinja"
user_template = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/user_template_2.minijinja"

[functions.basic_test_template_no_schema.variants.best_of_n]
type = "experimental_best_of_n_sampling"
candidates = ["test", "test"]

[functions.basic_test_template_no_schema.variants.best_of_n.evaluator]
model = "dummy::best_of_n_0"
system_template = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/system_template_2.minijinja"
assistant_template = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/assistant_template_2.minijinja"
input_wrappers.user = "../../../fixtures/config/functions/basic_test_template_no_schema/prompt/user_template_2.minijinja"


[functions.basic_test_no_system_schema]
type = "chat"

[functions.basic_test_no_system_schema.variants.test]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.basic_test]
type = "chat"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"

[functions.basic_test.variants.test]
type = "chat_completion"
weight = 1
model = "test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
temperature = 1.0
max_tokens = 100
seed = 69

[functions.basic_test.variants.test2]
type = "chat_completion"
model = "test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.basic_test.variants.error]
type = "chat_completion"
model = "error"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.basic_test.variants.slow]
type = "chat_completion"
model = "slow"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"


[functions.basic_test.variants.test_dynamic_api_key]
type = "chat_completion"
model = "test_key"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
temperature = 1.0
max_tokens = 100
seed = 69

[functions.basic_test.variants.anthropic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.anthropic-extra-body]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.anthropic-extra-headers]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "x-api-key", value = "invalid_anthropic_auth" }]

[functions.basic_test.variants.anthropic-dynamic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.anthropic-shorthand]
type = "chat_completion"
model = "anthropic::claude-3-haiku-20240307"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock-thinking]
type = "chat_completion"
model = "claude-3-7-sonnet-20250219-thinking-aws-bedrock"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 2048
thinking_budget_tokens = 1024

[functions.basic_test.variants.aws-bedrock-deepseek-r1]
type = "chat_completion"
model = "deepseek-r1-aws-bedrock"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 500

[functions.basic_test.variants.aws-bedrock-extra-body]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/inferenceConfig/temperature", value = 0.123 }]

[functions.basic_test.variants.aws-bedrock-extra-headers]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Content-Length", value = "2" }]

[functions.basic_test.variants.aws-sagemaker-extra-body]
type = "chat_completion"
model = "gemma-3-1b-aws-sagemaker-openai"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.aws-sagemaker-extra-headers]
type = "chat_completion"
model = "gemma-3-1b-aws-sagemaker-openai"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "x-amz-security-token", value = "InvalidToken" }]


[functions.basic_test.variants.aws-bedrock-us-east-1]
type = "chat_completion"
model = "claude-3-haiku-20240307-us-east-1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-bedrock-uk-hogwarts-1]
type = "chat_completion"
model = "claude-3-haiku-20240307-uk-hogwarts-1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-sagemaker-openai]
type = "chat_completion"
model = "gemma-3-1b-aws-sagemaker-openai"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.aws-sagemaker-tgi]
type = "chat_completion"
model = "gemma-3-1b-aws-sagemaker-tgi"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.azure]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.azure-extra-body]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.azure-extra-headers]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "api-key", value = "invalid_azure_openai_auth" }]

[functions.basic_test.variants.azure-dynamic]
type = "chat_completion"
model = "gpt-4o-mini-azure-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.azure-ai-foundry]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.azure-ai-foundry-extra-body]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.azure-ai-foundry-extra-headers]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "api-key", value = "invalid_azure_openai_auth" }]

[functions.basic_test.variants.azure-ai-foundry-dynamic]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.fireworks]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 500

[functions.basic_test.variants.fireworks-extra-body]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.fireworks-extra-headers]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-r1-0528"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_fireworks_auth" }]

[functions.basic_test.variants.fireworks-dynamic]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.fireworks-shorthand]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-gemini-flash-extra-body]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/generationConfig/temperature", value = 0.123 }]

[functions.basic_test.variants.gcp-vertex-gemini-flash-extra-headers]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_gcp_vertex_auth" }]

[functions.basic_test.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
model = "gcp-gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 500

[functions.basic_test.variants.gcp-vertex-gemini-flash-lite-tuned]
type = "chat_completion"
model = "gemini-2.0-flash-lite-tuned"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gemini-bad-model-name]
type = "chat_completion"
model = "gemini-wrong-field"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b-extra-body]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/generationConfig/temperature", value = 0.123 }]

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b-extra-headers]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Content-Length", value = "2" }]

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b-dynamic]
type = "chat_completion"
model = "gemini-2.0-flash-lite-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-flash-8b-shorthand]
type = "chat_completion"
model = "google_ai_studio_gemini::gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.google-ai-studio-gemini-2_5-pro]
type = "chat_completion"
model = "gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 500

[functions.basic_test.variants.google-ai-studio-gemini-2_5-pro-dynamic]
type = "chat_completion"
model = "gemini-2.5-pro-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 500

[functions.basic_test.variants.gcp-vertex-haiku]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-haiku-extra-body]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.gcp-vertex-haiku-extra-headers]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Content-Length", value = "2" }]

[functions.basic_test.variants.groq]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 500

[functions.basic_test.variants.groq-shorthand]
type = "chat_completion"
model = "groq::meta-llama/llama-4-scout-17b-16e-instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.groq-extra-body]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.groq-extra-headers]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [
    { name = "Authorization", value = "Bearer: invalid_openai_auth" },
]

[functions.basic_test.variants.groq-dynamic]
type = "chat_completion"
model = "groq-qwen-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.mistral]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.mistral-extra-body]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.mistral-extra-headers]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_mistral_auth" }]

[functions.basic_test.variants.mistral-dynamic]
type = "chat_completion"
model = "open-mistral-nemo-2407-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.mistral-shorthand]
type = "chat_completion"
model = "mistral::open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100


[functions.basic_test.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.openai-responses]
type = "chat_completion"
model = "responses-gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.openai-extra-body]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [
    { pointer = "/temperature", value = 0.123 },
    { pointer = "/invalid-field-should-be-deleted", value = "Bad value" },
    { pointer = "/invalid-field-should-be-deleted", delete = true },
]

[functions.basic_test.variants.openai-extra-body-provider-config]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18-extra-body"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [
    { pointer = "/temperature", value = 0.123 },
    { pointer = "/max_completion_tokens", value = 123 },
]

[functions.basic_test.variants.openai-extra-headers]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [
    { name = "Authorization", value = "Bearer: invalid_openai_auth" },
]

[functions.basic_test.variants.openai-o1]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 1000

[functions.basic_test.variants.openai-dynamic]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.openai-shorthand]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp_vertex_gemini_shorthand]
type = "chat_completion"
model = "gcp_vertex_gemini::projects/tensorzero-public/locations/us-central1/publishers/google/models/gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.gcp_vertex_gemini_shorthand_endpoint]
type = "chat_completion"
model = "gcp_vertex_gemini::projects/tensorzero-public/locations/us-central1/endpoints/945488740422254592"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

# Note - GCP doesn't allow fine-tuning any Anthropic models, so we just test the 'publishers/anthropic' shorthand
[functions.basic_test.variants.gcp_vertex_anthropic_shorthand]
type = "chat_completion"
model = "gcp_vertex_anthropic::projects/tensorzero-public/locations/us-central1/publishers/anthropic/models/claude-3-haiku@20240307"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.o4-mini]
type = "chat_completion"
model = "o4-mini"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.basic_test.variants.o3-mini]
type = "chat_completion"
model = "o3-mini"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
reasoning_effort = "low"

## OpenRouter

[functions.basic_test.variants.openrouter]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.openrouter-extra-body]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.openrouter-extra-headers]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [
    { name = "Authorization", value = "Bearer: invalid_openai_auth" },
]

[functions.basic_test.variants.openrouter-dynamic]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter_dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.openrouter-shorthand]
type = "chat_completion"
model = "openrouter::openai/gpt-4o-mini"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.tgi]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.tgi-extra-body]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.tgi-extra-headers]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_tgi_auth" }]

[functions.basic_test.variants.together]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.together-extra-body]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.together-extra-headers]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [
    { name = "Authorization", value = "Bearer invalid_together_auth" },
]

[functions.basic_test.variants.together-dynamic]
type = "chat_completion"
model = "llama3.1-8b-instruct-together-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.together-shorthand]
type = "chat_completion"
model = "together::meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.together-tool]
type = "chat_completion"
model = "llama3.1-405b-instruct-turbo-together"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.sglang]
type = "chat_completion"
model = "Qwen/Qwen2.5-1.5B-Instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.sglang-extra-body]
type = "chat_completion"
model = "Qwen/Qwen2.5-1.5B-Instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.vllm]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.vllm-gpt-oss-20b]
type = "chat_completion"
model = "gpt-oss-20b-vllm"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.vllm-extra-body]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.vllm-extra-headers]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "invalid_vllm_auth" }]

[functions.basic_test.variants.vllm-dynamic]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.xai]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.xai-extra-body]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.xai-extra-headers]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "Bearer invalid_xai_auth" }]

[functions.basic_test.variants.xai-dynamic]
type = "chat_completion"
model = "grok_2_1212-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.xai-shorthand]
type = "chat_completion"
model = "xai::grok-2-1212"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.hyperbolic]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.hyperbolic-extra-body]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.hyperbolic-extra-headers]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [
    { name = "Authorization", value = "Bearer invalid_hyperbolic_auth" },
]

[functions.basic_test.variants.hyperbolic-dynamic]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.hyperbolic-shorthand]
type = "chat_completion"
model = "hyperbolic::meta-llama/Meta-Llama-3-70B-Instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.deepseek-chat]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.deepseek-chat-extra-body]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.deepseek-chat-extra-headers]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
extra_headers = [{ name = "Authorization", value = "Bearer sk-bad-tensorzero" }]

[functions.basic_test.variants.deepseek-reasoner]
type = "chat_completion"
model = "deepseek-reasoner"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 800

[functions.basic_test.variants.together-deepseek-r1]
type = "chat_completion"
model = "together-deepseek-r1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 800

[functions.basic_test.variants.reasoner]
type = "chat_completion"
model = "reasoner"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 800

[functions.basic_test.variants.deepseek-dynamic]
type = "chat_completion"
model = "deepseek-chat-dynamic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.deepseek-shorthand]
type = "chat_completion"
model = "deepseek::deepseek-chat"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.fireworks-deepseek]
type = "chat_completion"
model = "deepseek-r1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 800

[functions.basic_test.variants.fireworks-gpt-oss-20b]
type = "chat_completion"
model = "gpt-oss-20b-fireworks"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 800

[functions.basic_test.variants.flaky]
type = "chat_completion"
model = "flaky_basic_test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
retries = { num_retries = 5, max_delay_s = 0.1 }

[functions.basic_test.variants.err_in_stream]
type = "chat_completion"
model = "err_in_stream"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.basic_test.variants.dicl]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
embedding_model = "text-embedding-3-small"
k = 3
max_tokens = 100

[functions.basic_test.variants.empty_dicl]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
embedding_model = "text-embedding-3-small"
k = 3
max_tokens = 100

[functions.basic_test.variants.empty_dicl_extra_body]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
embedding_model = "text-embedding-3-small"
k = 3
max_tokens = 100
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.basic_test.variants.empty_dicl_shorthand]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
embedding_model = "openai::text-embedding-3-small"
k = 3
max_tokens = 100

[functions.model_fallback_test]
type = "chat"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"

[functions.model_fallback_test.variants.test]
type = "chat_completion"
weight = 1
model = "test_fallback"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100

[functions.json_fail]
type = "json"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../../fixtures/config/functions/basic_test/output_schema.json"

[functions.json_fail.variants.test]
type = "chat_completion"
weight = 1
model = "test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success_no_input_schema]
type = "json"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../../fixtures/config/functions/basic_test/output_schema.json"

[functions.json_success_no_input_schema.variants.test]
type = "chat_completion"
weight = 1
model = "json"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success]
type = "json"
schemas.system.path = "../../../fixtures/config/functions/basic_test/system_schema.json"
user_schema = "../../../fixtures/config/functions/json_success/user_schema.json"
output_schema = "../../../fixtures/config/functions/basic_test/output_schema.json"

[functions.json_success.variants.test]
type = "chat_completion"
weight = 1
model = "json"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
templates.user.path = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.test-diff-schema]
type = "chat_completion"
weight = 0
model = "dummy::json_diff_schema"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.anthropic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.anthropic-strict]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.anthropic-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.aws-bedrock]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.aws-bedrock-deepseek-r1]
type = "chat_completion"
model = "deepseek-r1-aws-bedrock"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 500

[functions.json_success.variants.aws-bedrock-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.aws-bedrock-strict]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.azure]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.azure-strict]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.azure-implicit]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.azure-ai-foundry]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.azure-ai-foundry-strict]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.azure-ai-foundry-implicit]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.fireworks]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.fireworks-gpt-oss-20b]
type = "chat_completion"
model = "gpt-oss-20b-fireworks"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
# Fireworks gives us 'Failed to format non-streaming choice: Expected message start token but ran out of tokens' if
# we try to explicitly request a 'json_object' response format for gpt-oss-20b.
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.fireworks-implicit]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.deepseek-reasoner]
type = "chat_completion"
model = "deepseek-reasoner"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 800

[functions.json_success.variants.together-deepseek-r1]
type = "chat_completion"
model = "together-deepseek-r1"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 800

[functions.json_success.variants.fireworks-strict]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-flash-strict]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.gcp-vertex-gemini-flash-implicit]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-flash-lite-tuned]
type = "chat_completion"
model = "gemini-2.0-flash-lite-tuned"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
model = "gcp-gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 500

[functions.json_success.variants.gcp-vertex-gemini-pro-implicit]
type = "chat_completion"
model = "gcp-gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 500

[functions.json_success.variants.gcp-vertex-haiku]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-haiku-strict]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.gcp-vertex-haiku-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.google-ai-studio-gemini-flash-8b]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.google-ai-studio-gemini-flash-8b-strict]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"


[functions.json_success.variants.google-ai-studio-gemini-flash-8b-implicit]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.google-ai-studio-gemini-2_5-pro]
type = "chat_completion"
model = "gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 500

[functions.json_success.variants.google-ai-studio-gemini-2_5-pro-implicit]
type = "chat_completion"
model = "gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 500

[functions.json_success.variants.groq]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 500

[functions.json_success.variants.groq-implicit]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 500

[functions.json_success.variants.groq-strict]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 500

[functions.json_success.variants.json_reasoner]
type = "chat_completion"
model = "json_reasoner"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"
[functions.json_success.variants.mistral]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.mistral-strict]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.openai-responses]
type = "chat_completion"
model = "responses-gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.openai-responses-strict]
type = "chat_completion"
model = "responses-gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.json_success.variants.openai-strict]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.openai-o1]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 1000

[functions.json_success.variants.openai-implicit]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.openai-cot]
type = "experimental_chain_of_thought"
model = "openai::gpt-4.1-nano-2025-04-14"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

## OpenRouter

[functions.json_success.variants.openrouter]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.openrouter-implicit]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.openrouter-strict]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.json_success.variants.tgi]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.tgi-strict]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.together]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.together-strict]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.together-implicit]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.json_success.variants.sglang]
type = "chat_completion"
model = "Qwen/Qwen2.5-1.5B-Instruct"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.sglang-strict]
type = "chat_completion"
model = "Qwen/Qwen2.5-1.5B-Instruct"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.vllm]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.vllm-gpt-oss-20b]
type = "chat_completion"
model = "gpt-oss-20b-vllm"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.vllm-strict]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"


[functions.json_success.variants.dicl]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini-2024-07-18"
system_instructions = "../../../fixtures/config/functions/json_success/prompt/system_instructions.txt"
embedding_model = "text-embedding-3-small"
k = 3
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.deepseek-chat]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.deepseek-chat-strict]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.fireworks-deepseek]
type = "chat_completion"
model = "deepseek-r1"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 800

[functions.json_success.variants.xai]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.xai-strict]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.json_success.variants.chain_of_thought]
type = "experimental_chain_of_thought"
model = "dummy::json_cot"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.json_success.variants.chain_of_thought_implicit_tool]
type = "experimental_chain_of_thought"
model = "openai::gpt-4.1-nano-2025-04-14"
system_template = "../../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "implicit_tool"

[functions.json_success.variants.anthropic_json_mode_off]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.aws_bedrock_json_mode_off]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.azure_json_mode_off]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.azure-ai-foundry_json_mode_off]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.deepseek_chat_json_mode_off]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.fireworks_json_mode_off]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.together_deepseek_r1_json_mode_off]
type = "chat_completion"
model = "together-deepseek-r1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.gcp_vertex_gemini_flash_json_mode_off]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.gcp_vertex_gemini_pro_json_mode_off]
type = "chat_completion"
model = "gcp-gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.gcp_vertex_haiku_json_mode_off]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.google_ai_studio_gemini_flash_8b_json_mode_off]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.google_ai_studio_gemini_pro_002_json_mode_off]
type = "chat_completion"
model = "gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.mistral_json_mode_off]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.openai-responses_json_mode_off]
type = "chat_completion"
model = "responses-gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.openai_json_mode_off]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.openai_o1_json_mode_off]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.sglang_json_mode_off]
type = "chat_completion"
model = "Qwen/Qwen2.5-1.5B-Instruct"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.tgi_json_mode_off]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.together_json_mode_off]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.vllm_json_mode_off]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.json_success.variants.xai_json_mode_off]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "off"
max_tokens = 100

[functions.dynamic_json]
type = "json"
system_schema = "../../../fixtures/config/functions/dynamic_json/system_schema.json"
user_schema = "../../../fixtures/config/functions/dynamic_json/user_schema.json"

[functions.dynamic_json.variants.test]
type = "chat_completion"
weight = 1
model = "json"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.anthropic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.anthropic-strict]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.anthropic-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.aws-bedrock]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.aws-bedrock-deepseek-r1]
type = "chat_completion"
model = "deepseek-r1-aws-bedrock"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 500

[functions.dynamic_json.variants.aws-bedrock-strict]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.aws-bedrock-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.azure]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.azure-strict]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.azure-implicit]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.azure-ai-foundry]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.azure-ai-foundry-strict]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.azure-ai-foundry-implicit]
type = "chat_completion"
model = "llama-3.3-70b-instruct-azure"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.fireworks]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.fireworks-strict]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.fireworks-implicit]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-gemini-flash-strict]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.gcp-vertex-gemini-flash-implicit]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
model = "gcp-gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 500

[functions.dynamic_json.variants.gcp-vertex-gemini-pro-implicit]
type = "chat_completion"
model = "gcp-gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 500

[functions.dynamic_json.variants.gcp-vertex-gemini-flash-lite-tuned]
type = "chat_completion"
model = "gemini-2.0-flash-lite-tuned"
json_mode = "strict"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100

[functions.dynamic_json.variants.google-ai-studio-gemini-flash-8b]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.google-ai-studio-gemini-flash-8b-strict]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.google-ai-studio-gemini-flash-8b-implicit]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.google-ai-studio-gemini-2_5-pro]
type = "chat_completion"
model = "gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 500

[functions.dynamic_json.variants.google-ai-studio-gemini-2_5-pro-implicit]
type = "chat_completion"
model = "gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 500

[functions.dynamic_json.variants.gcp-vertex-haiku]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.gcp-vertex-haiku-strict]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.gcp-vertex-haiku-implicit]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.groq]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 500

[functions.dynamic_json.variants.groq-strict]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 500
json_mode = "strict"

[functions.dynamic_json.variants.groq-implicit]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 500


[functions.dynamic_json.variants.mistral]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.mistral-strict]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.dynamic_json.variants.openai-responses]
type = "chat_completion"
model = "responses-gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.openai-responses-strict]
type = "chat_completion"
model = "responses-gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.dynamic_json.variants.openai-strict]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.openai-o1]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 1000


[functions.dynamic_json.variants.openai-implicit]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.openai-cot]
type = "experimental_chain_of_thought"
model = "openai::gpt-4.1-nano-2025-04-14"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

## OpenRouter

[functions.dynamic_json.variants.openrouter]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.openrouter-strict]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.openrouter-implicit]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.sglang]
type = "chat_completion"
model = "Qwen/Qwen2.5-1.5B-Instruct"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "strict"
max_tokens = 100

[functions.dynamic_json.variants.sglang-strict]
type = "chat_completion"
model = "Qwen/Qwen2.5-1.5B-Instruct"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.tgi]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.tgi-strict]
type = "chat_completion"
model = "phi-3.5-mini-instruct-tgi"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.together]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.together-strict]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.together-implicit]
type = "chat_completion"
model = "llama3.1-8b-instruct-together"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.vllm]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.vllm-strict]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.vllm-implicit]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "implicit_tool"
max_tokens = 100

[functions.dynamic_json.variants.deepseek-chat]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.deepseek-chat-strict]
type = "chat_completion"
model = "deepseek-chat"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.dynamic_json.variants.xai]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.dynamic_json.variants.xai-strict]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/dynamic_json/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/dynamic_json/prompt/user_template.minijinja"
max_tokens = 100
json_mode = "strict"

[functions.variant_failover]
type = "chat"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"
user_schema = "../../../fixtures/config/functions/variant_failover/user_schema.json"

[functions.variant_failover.variants.good]
type = "chat_completion"
weight = 0.5
model = "test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover.variants.error]
type = "chat_completion"
weight = 0.5
model = "error"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover_zero_weight]
type = "chat"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"
user_schema = "../../../fixtures/config/functions/variant_failover/user_schema.json"

[functions.variant_failover_zero_weight.variants.first_error]
type = "chat_completion"
weight = 0.5
model = "dummy::error_1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover_zero_weight.variants.second_error]
type = "chat_completion"
weight = 0.5
model = "dummy::error_2"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover_zero_weight.variants.no_weight]
type = "chat_completion"
model = "dummy::error_no_weight"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.variant_failover_zero_weight.variants.zero_weight]
type = "chat_completion"
weight = 0
model = "dummy::error_zero_weight"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/variant_failover/prompt/user_template.minijinja"
max_tokens = 100

[functions.prometheus_test1]
type = "chat"

[functions.prometheus_test1.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test2]
type = "chat"

[functions.prometheus_test2.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test3]
type = "chat"

[functions.prometheus_test3.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.prometheus_test4]
type = "chat"

[functions.prometheus_test4.variants.variant]
type = "chat_completion"
weight = 1
model = "test"
max_tokens = 100

[functions.null_chat]
type = "chat"

[functions.null_chat.variants.variant]
type = "chat_completion"
model = "null"
max_tokens = 100

[functions.null_json]
type = "json"
output_schema = "../../../fixtures/config/functions/basic_test/output_schema.json"

[functions.null_json.variants.variant]
type = "chat_completion"
model = "null"
max_tokens = 100
json_mode = "strict"

[functions.weather_helper]
type = "chat"
system_schema = "../../../fixtures/config/functions/weather_helper/system_schema.json"
tools = ["get_temperature"]
tool_choice = "auto"

[functions.weather_helper.variants.anthropic-thinking]
type = "chat_completion"
weight = 0
model = "anthropic::claude-3-7-sonnet-20250219"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
thinking_budget_tokens = 1024
max_tokens = 1200

[functions.weather_helper.variants.gcp-vertex-anthropic-thinking]
type = "chat_completion"
weight = 0
model = "gcp-vertex-anthropic-claude-haiku-4-5@20251001-thinking"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
# This must be at least as large as `thinking.budget_tokens`
max_tokens = 1200

[functions.weather_helper.variants.gcp-vertex-anthropic-sonnet-thinking]
type = "chat_completion"
weight = 0
model = "gcp-vertex-anthropic-claude-sonnet-4-5@20250929-thinking"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
# This must be at least as large as `thinking.budget_tokens`
max_tokens = 1200

[functions.weather_helper.variants.variant]
type = "chat_completion"
weight = 1
model = "tool"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.split_tool_name]
type = "chat_completion"
model = "dummy::tool_split_name"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.bad_tool]
type = "chat_completion"
model = "bad_tool"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.aws-bedrock]
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.aws-bedrock-deepseek-r1]
type = "chat_completion"
model = "deepseek-r1-aws-bedrock"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 500
[functions.weather_helper.variants.anthropic]
type = "chat_completion"
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.azure]
type = "chat_completion"
model = "gpt-4o-mini-azure"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.fireworks]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/deepseek-v3p1"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-gemini-pro]
type = "chat_completion"
model = "gcp-gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 500

[functions.weather_helper.variants.google-ai-studio-gemini-flash-8b]
type = "chat_completion"
model = "gemini-2.0-flash-lite"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-gemini-flash-lite-tuned]
type = "chat_completion"
model = "gemini-2.0-flash-lite-tuned"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.google-ai-studio-gemini-2_5-pro]
type = "chat_completion"
model = "gemini-2.5-pro"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 500

[functions.weather_helper.variants.gcp-vertex-haiku]
type = "chat_completion"
model = "claude-3-haiku-20240307-gcp-vertex"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.groq]
type = "chat_completion"
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 500

[functions.weather_helper.variants.mistral]
type = "chat_completion"
model = "open-mistral-nemo-2407"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.openai-responses]
type = "chat_completion"
model = "responses-gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.openai-o1]
type = "chat_completion"
model = "o1-2024-12-17"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 1000

[functions.weather_helper.variants.openrouter]
type = "chat_completion"
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.together-tool]
type = "chat_completion"
model = "llama3.1-405b-instruct-turbo-together"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.sglang]
type = "chat_completion"
model = "Qwen/Qwen2.5-1.5B-Instruct"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.vllm]
type = "chat_completion"
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.xai]
type = "chat_completion"
model = "grok_2_1212"
system_template = "../../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper_parallel]
type = "chat"
system_schema = "../../../fixtures/config/functions/weather_helper_parallel/system_schema.json"
tools = ["get_temperature", "get_humidity"]
tool_choice = "auto"
# We use an inference-time parameter to set `parallel_tool_calls = true` for the test

[functions.weather_helper_parallel.variants.openai]
type = "chat_completion"
weight = 1
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper_parallel.variants.openai-responses]
type = "chat_completion"
weight = 1
model = "responses-gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper_parallel.variants.openai-o1]
type = "chat_completion"
weight = 1
model = "o1-2024-12-17"
system_template = "../../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"
max_tokens = 1000

[functions.weather_helper_parallel.variants.openrouter]
type = "chat_completion"
weight = 1
model = "gpt_4_o_mini_openrouter"
system_template = "../../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"

[functions.weather_helper_parallel.variants.anthropic]
type = "chat_completion"
weight = 1
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"

[functions.weather_helper_parallel.variants.groq]
type = "chat_completion"
weight = 1
model = "groq-qwen"
system_template = "../../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"

[functions.weather_helper_parallel.variants.together-tool]
type = "chat_completion"
weight = 1
model = "llama3.1-405b-instruct-turbo-together"
system_template = "../../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"

[functions.weather_helper_parallel.variants.vllm]
type = "chat_completion"
weight = 1
model = "qwen2.5-0.5b-instruct-vllm"
system_template = "../../../fixtures/config/functions/weather_helper_parallel/prompt/system_template.minijinja"


[functions.required_tool]
type = "chat"
tool_choice = "required"

[functions.required_tool.variants.variant]
type = "chat_completion"
model = "test"

[functions.specific_tool]
type = "chat"
tool_choice.specific = "get_temperature"

[functions.specific_tool.variants.variant]
type = "chat_completion"
model = "test"

[functions.best_of_n]
type = "chat"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"

[functions.best_of_n.variants.variant0]
type = "chat_completion"
weight = 0
model = "test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.best_of_n.variants.variant1]
type = "chat_completion"
weight = 0
model = "json"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.best_of_n.variants.best_of_n_variant]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant1"]

[functions.best_of_n.variants.best_of_n_variant.evaluator]
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.best_of_n.variants.best_of_n_variant_extra_body]
type = "experimental_best_of_n_sampling"
weight = 0
candidates = ["variant0", "variant1"]

[functions.best_of_n.variants.best_of_n_variant_extra_body.evaluator]
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
extra_body = [{ pointer = "/generationConfig/temperature", value = 0.123 }]

[functions.best_of_n.variants.flaky_best_of_n_variant]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant1"]

[functions.best_of_n.variants.flaky_best_of_n_variant.evaluator]
model = "flaky_best_of_n_judge"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
retries = { num_retries = 5, max_delay_s = 0.1 }

[functions.best_of_n_json]
type = "json"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../../fixtures/config/functions/best_of_n_json/output_schema.json"

[functions.best_of_n_json.variants.variant0]
type = "chat_completion"
weight = 0
model = "test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json.variants.variant1]
type = "chat_completion"
weight = 0
model = "json"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json.variants.variant2]
type = "chat_completion"
weight = 0
model = "json_goodbye"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json.variants.best_of_n_variant]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant1", "variant2"]

[functions.best_of_n_json.variants.best_of_n_variant.evaluator]
model = "gemini-2.0-flash-001"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json.variants.best_of_n_variant_implicit_tool]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant1", "variant2"]

[functions.best_of_n_json.variants.best_of_n_variant_implicit_tool.evaluator]
model = "claude-3-haiku-20240307-anthropic"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "implicit_tool"

[functions.best_of_n_json_repeated]
type = "json"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../../fixtures/config/functions/best_of_n_json/output_schema.json"

[functions.best_of_n_json_repeated.variants.variant0]
type = "chat_completion"
weight = 0
model = "dummy::random_answer"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json_repeated.variants.variant1]
type = "chat_completion"
weight = 0
model = "json"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.best_of_n_json_repeated.variants.best_of_n_variant]
type = "experimental_best_of_n_sampling"
weight = 1
candidates = ["variant0", "variant0", "variant1"]

[functions.best_of_n_json_repeated.variants.best_of_n_variant.evaluator]
model = "dummy::best_of_n_0"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_single_candidate]
type = "chat"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"

[functions.mixture_of_n_single_candidate.variants.variant0]
type = "chat_completion"
weight = 0
model = "test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n_single_candidate.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0"]

[functions.mixture_of_n_single_candidate.variants.mixture_of_n_variant.fuser]
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n]
type = "chat"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"

[functions.mixture_of_n.variants.variant0]
type = "chat_completion"
weight = 0
model = "test"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n.variants.variant1]
type = "chat_completion"
weight = 0
model = "alternate"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant1"]

[functions.mixture_of_n.variants.mixture_of_n_variant.fuser]
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n.variants.mixture_of_n_variant_bad_fuser]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant0"]

[functions.mixture_of_n.variants.mixture_of_n_variant_bad_fuser.fuser]
model = "dummy::error"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"


[functions.mixture_of_n_extra_body]
type = "chat"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"


[functions.mixture_of_n_extra_body.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant1"]

[functions.mixture_of_n_extra_body.variants.variant0]
type = "chat_completion"
model = "o1-2024-12-17"
weight = 0
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n_extra_body.variants.variant1]
type = "chat_completion"
model = "test"
weight = 0
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"

[functions.mixture_of_n_extra_body.variants.mixture_of_n_variant.fuser]
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
extra_body = [{ pointer = "/temperature", value = 0.123 }]

[functions.mixture_of_n_json]
type = "json"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"
output_schema = "../../../fixtures/config/functions/mixture_of_n_json/output_schema.json"

[functions.mixture_of_n_json.variants.variant0]
type = "chat_completion"
weight = 0
model = "json_beatles_1"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json.variants.variant1]
type = "chat_completion"
weight = 0
model = "json_beatles_2"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant1"]

[functions.mixture_of_n_json.variants.mixture_of_n_variant.fuser]
model = "gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.multi_hop_rag_agent]
type = "chat"
tools = ["think", "search_wikipedia", "load_wikipedia_page", "answer_question"]
tool_choice = "required"
parallel_tool_calls = false

[functions.multi_hop_rag_agent.variants.baseline]
type = "chat_completion"
model = "openai::gpt-4o-mini"
system_template = "../../../fixtures/config/functions/multi_hop_rag_agent/baseline/system_template.txt"

[functions.write_haiku]
type = "chat"
user_schema = "../../../fixtures/config/functions/write_haiku/user_schema.json"

[functions.write_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/write_haiku/initial_prompt/system_template.minijinja"
user_template = "../../../fixtures/config/functions/write_haiku/initial_prompt/user_template.minijinja"

[functions.write_haiku.variants.aws_bedrock]
weight = 0
type = "chat_completion"
model = "claude-3-haiku-20240307-aws-bedrock"
user_template = "../../../fixtures/config/functions/write_haiku/initial_prompt/user_template.minijinja"

[functions.extract_entities]
type = "json"
output_schema = "../../../fixtures/config/functions/extract_entities/output_schema.json"

[functions.extract_entities.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
system_template = "../../../fixtures/config/functions/extract_entities/initial_prompt/system_template.minijinja"
json_mode = "strict"

[functions.extract_entities.variants.dummy_error]
type = "chat_completion"
model = "dummy::error"
system_template = "../../../fixtures/config/functions/extract_entities/initial_prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json_repeated]
type = "json"
system_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"

[functions.mixture_of_n_json_repeated.variants.variant0]
type = "chat_completion"
weight = 0
model = "dummy::random_answer"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json_repeated.variants.variant1]
type = "chat_completion"
weight = 0
model = "json"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.mixture_of_n_json_repeated.variants.mixture_of_n_variant]
type = "experimental_mixture_of_n"
weight = 1
candidates = ["variant0", "variant0", "variant1"]

[functions.mixture_of_n_json_repeated.variants.mixture_of_n_variant.fuser]
model = "json"
system_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
json_mode = "strict"

[functions.image_judger]
type = "chat"

[functions.image_judger.variants.honest_answer]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"

[functions.openai_responses_gpt5]
type = "chat"

[functions.openai_responses_gpt5.variants.openai]
type = "chat_completion"
model = "responses-gpt-5"


[functions.openai_with_assistant_schema]
type = "chat"
assistant_schema = "../../../fixtures/config/functions/basic_test/system_schema.json"

[functions.openai_with_assistant_schema.variants.openai]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
assistant_template = "../../../fixtures/config/functions/basic_test/prompt/system_template.minijinja"
max_tokens = 100
