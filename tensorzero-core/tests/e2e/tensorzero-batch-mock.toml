# Configuration file for E2E batch testing with mock inference providers
# This config points all providers to the mock server running on localhost:3030

[gateway]
bind_address = "0.0.0.0:3000"
debug = true

[object_storage]
type = "disabled"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                   MODELS                                   │
# └────────────────────────────────────────────────────────────────────────────┘

# OpenAI model configured to use mock server
[models."gpt-4o-mini-2024-07-18"]
routing = ["openai"]

[models."gpt-4o-mini-2024-07-18".providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"
api_base = "http://localhost:3030/openai/"

# GCP Vertex Gemini model configured to use mock server
[models."gemini-2.0-flash-001"]
routing = ["gcp_vertex_gemini"]

[models."gemini-2.0-flash-001".providers.gcp_vertex_gemini]
type = "gcp_vertex_gemini"
model_id = "gemini-2.0-flash-001"
location = "us-central1"
project_id = "test-project"
credential_location = "none"
api_base = "http://localhost:3030/v1/"

# GCP Vertex Gemini batch configuration pointing to mock GCS and batch API
[provider_types.gcp_vertex_gemini.batch]
storage_type = "cloud_storage"
input_uri_prefix = "gs://mock-bucket/input/"
output_uri_prefix = "gs://mock-bucket/output/"
# This will be overridden to point to mock server via provider URL override

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                   TOOLS                                    │
# └────────────────────────────────────────────────────────────────────────────┘

[tools.get_temperature]
description = "Get the current temperature in a given location"
parameters = "../../fixtures/config/tools/get_temperature.json"

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                 FUNCTIONS                                  │
# └────────────────────────────────────────────────────────────────────────────┘

[functions.basic_test]
type = "chat"

[functions.basic_test.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "You are a helpful assistant."
max_tokens = 100

[functions.basic_test.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "You are a helpful assistant."
max_tokens = 100

[functions.json_success]
type = "json"
schemas.system.path = "../../fixtures/config/functions/basic_test/system_schema.json"
user_schema = "../../fixtures/config/functions/json_success/user_schema.json"
output_schema = "../../fixtures/config/functions/basic_test/output_schema.json"

[functions.json_success.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.json_success.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/json_success/prompt/system_template.minijinja"
user_template = "../../fixtures/config/functions/json_success/prompt/user_template.minijinja"
json_mode = "on"
max_tokens = 100

[functions.weather_helper]
type = "chat"
system_schema = "../../fixtures/config/functions/weather_helper/system_schema.json"
tools = ["get_temperature"]
tool_choice = "auto"

[functions.weather_helper.variants.openai]
type = "chat_completion"
model = "gpt-4o-mini-2024-07-18"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

[functions.weather_helper.variants.gcp-vertex-gemini-flash]
type = "chat_completion"
model = "gemini-2.0-flash-001"
system_template = "../../fixtures/config/functions/weather_helper/prompt/system_template.minijinja"
max_tokens = 100

# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                  METRICS                                   │
# └────────────────────────────────────────────────────────────────────────────┘

[metrics.test_metric]
type = "boolean"
optimize = "max"
level = "inference"
