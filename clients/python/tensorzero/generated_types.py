"""
Auto-generated Python dataclasses from JSON schemas.

This file is generated from JSON schemas in the tensorzero-core crate.
Do not edit this file manually - it will be overwritten.

Generated from schemas in: tensorzero-core/clients/schemas/

To regenerate, run:
    python generate_schema_types.py
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Literal

from .unset_type import UNSET, UnsetType

Model = Any


Detail = Literal["low", "high", "auto"]


System = str | dict[str, Any]


Role = Literal["user", "assistant"]


@dataclass(kw_only=True)
class StoredInputMessageContentText:
    text: str
    type: Literal["text"] = "text"


@dataclass(kw_only=True)
class StoredInputMessageContentTemplate:
    name: str
    arguments: dict[str, Any]
    type: Literal["template"] = "template"


@dataclass(kw_only=True)
class StoredInputMessageContentToolResult:
    name: str
    result: str
    id: str
    type: Literal["tool_result"] = "tool_result"


@dataclass(kw_only=True)
class StoredInputMessageContentRawText:
    value: str
    type: Literal["raw_text"] = "raw_text"


@dataclass(kw_only=True)
class StoredInputMessageContentUnknown:
    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    type: Literal["unknown"] = "unknown"
    model_provider_name: str | None = None
    """
    A fully-qualified name specifying when this content block should
    be included in the model provider input.
    """


@dataclass(kw_only=True)
class ToolCall:
    id: str
    name: str
    arguments: str


@dataclass(kw_only=True)
class ThoughtSummaryBlockSummaryText:
    text: str
    type: Literal["summary_text"] = "summary_text"


ThoughtSummaryBlock = ThoughtSummaryBlockSummaryText


@dataclass(kw_only=True)
class Thought:
    text: str | None = None
    signature: str | None = None
    """
    An optional signature - currently, this is only used with Anthropic,
    and is ignored by other providers.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    field_internal_provider_type: str | None = None
    """
    When set, this 'Thought' block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """


@dataclass(kw_only=True)
class StorageKindS3Compatible:
    type: Literal["s3_compatible"] = "s3_compatible"
    bucket_name: str | None = None
    region: str | None = None
    endpoint: str | None = None
    allow_http: bool | None = None
    prefix: str | None = ""
    """
    An extra prefix to prepend to the object key.
    This is only enabled in e2e tests, to prevent clashes between concurrent test runs.
    """


@dataclass(kw_only=True)
class StorageKindFilesystem:
    path: str
    type: Literal["filesystem"] = "filesystem"


@dataclass(kw_only=True)
class StorageKindDisabled:
    type: Literal["disabled"] = "disabled"


StorageKind = StorageKindS3Compatible | StorageKindFilesystem | StorageKindDisabled


@dataclass(kw_only=True)
class ContentBlockChatOutputText:
    text: str
    type: Literal["text"] = "text"


@dataclass(kw_only=True)
class ContentBlockChatOutputThought(Thought):
    type: Literal["thought"] = "thought"


@dataclass(kw_only=True)
class ContentBlockChatOutputUnknown:
    data: Any
    type: Literal["unknown"] = "unknown"
    model_provider_name: str | None = None


@dataclass(kw_only=True)
class InferenceResponseToolCall:
    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """
    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """


@dataclass(kw_only=True)
class ToolClientSideFunction:
    description: str
    parameters: Any
    name: str
    type: Literal["client_side_function"] = "client_side_function"
    strict: bool | None = False
    """
    `strict` here specifies that TensorZero should attempt to use any facilities
    available from the model provider to force the model to generate an accurate tool call,
    notably OpenAI's strict tool call mode (https://platform.openai.com/docs/guides/function-calling#strict-mode).
    This imposes additional restrictions on the JSON schema that may vary across providers
    so we allow it to be configurable.
    """


@dataclass(kw_only=True)
class OpenAICustomToolFormatText:
    type: Literal["text"] = "text"


OpenAIGrammarSyntax = Literal["lark", "regex"]


@dataclass(kw_only=True)
class FunctionTool:
    description: str
    parameters: Any
    name: str
    strict: bool | None = False
    """
    `strict` here specifies that TensorZero should attempt to use any facilities
    available from the model provider to force the model to generate an accurate tool call,
    notably OpenAI's strict tool call mode (https://platform.openai.com/docs/guides/function-calling#strict-mode).
    This imposes additional restrictions on the JSON schema that may vary across providers
    so we allow it to be configurable.
    """


@dataclass(kw_only=True)
class ToolChoiceSpecific:
    specific: str


ToolChoice = Literal["none", "auto", "required"] | ToolChoiceSpecific


@dataclass(kw_only=True)
class ProviderToolScopeModelProvider:
    model_name: str
    model_provider_name: str


ProviderToolScope = ProviderToolScopeModelProvider | None


@dataclass(kw_only=True)
class InputMessageContentText:
    text: str
    type: Literal["text"] = "text"


@dataclass(kw_only=True)
class InputMessageContentTemplate:
    name: str
    arguments: dict[str, Any]
    type: Literal["template"] = "template"


@dataclass(kw_only=True)
class InputMessageContentToolResult:
    name: str
    result: str
    id: str
    type: Literal["tool_result"] = "tool_result"


@dataclass(kw_only=True)
class InputMessageContentRawText:
    value: str
    type: Literal["raw_text"] = "raw_text"


@dataclass(kw_only=True)
class InputMessageContentThought(Thought):
    type: Literal["thought"] = "thought"


@dataclass(kw_only=True)
class InputMessageContentUnknown:
    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    type: Literal["unknown"] = "unknown"
    model_provider_name: str | None = None
    """
    A fully-qualified name specifying when this content block should
    be included in the model provider input.
    """


ToolCallWrapper = ToolCall | InferenceResponseToolCall


@dataclass(kw_only=True)
class UrlFile:
    url: str
    mime_type: str | None = None
    detail: Detail | None = None
    filename: str | None = None


@dataclass(kw_only=True)
class Base64File:
    mime_type: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None
    filename: str | None = None


@dataclass(kw_only=True)
class FileUrlFile(UrlFile):
    file_type: Literal["url"] = "url"


@dataclass(kw_only=True)
class FileBase64(Base64File):
    file_type: Literal["base64"] = "base64"


@dataclass(kw_only=True)
class JsonDatapointOutputUpdate:
    raw: str
    """
    The raw output of the datapoint. For valid JSON outputs, this should be a JSON-serialized string.
    """


FloatComparisonOperator = Literal["<", "<=", "=", ">", ">=", "!="]


@dataclass(kw_only=True)
class FloatMetricFilter:
    metric_name: str
    value: float
    comparison_operator: FloatComparisonOperator


@dataclass(kw_only=True)
class BooleanMetricFilter:
    metric_name: str
    value: bool


TagComparisonOperator = Literal["=", "!="]


@dataclass(kw_only=True)
class TagFilter:
    key: str
    value: str
    comparison_operator: TagComparisonOperator


TimeComparisonOperator = Literal["<", "<=", "=", ">", ">=", "!="]


@dataclass(kw_only=True)
class TimeFilter:
    time: str
    comparison_operator: TimeComparisonOperator


@dataclass(kw_only=True)
class JsonInferenceOutput:
    raw: str | None = None
    """
    This is never omitted from the response even if it's None. A `null` value indicates no output from the model.
    It's rare and unexpected from the model, but it's possible.
    """
    parsed: Any | None = None
    """
    This is never omitted from the response even if it's None.
    """


InferenceOutputSource = str


@dataclass(kw_only=True)
class TagDatapointFilter(TagFilter):
    type: Literal["tag"] = "tag"


@dataclass(kw_only=True)
class TimeDatapointFilter(TimeFilter):
    type: Literal["time"] = "time"


OrderDirection = Literal["ascending", "descending"]


@dataclass(kw_only=True)
class DatapointMetadataUpdate:
    name: str | None | UnsetType = UNSET
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """


@dataclass(kw_only=True)
class UpdateDatapointMetadataRequest:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    name: str | None | UnsetType = UNSET
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """


CreateDatapointsFromInferenceOutputSource = str


@dataclass(kw_only=True)
class CreateDatapointsFromInferenceRequestParamsInferenceIds:
    inference_ids: list[str]
    """
    The inference IDs to create datapoints from.
    """
    type: Literal["inference_ids"] = "inference_ids"


@dataclass(kw_only=True)
class CreateDatapointsResponse:
    ids: list[str]
    """
    The IDs of the newly-generated datapoints.
    """


@dataclass(kw_only=True)
class DeleteDatapointsRequest:
    ids: list[str]
    """
    The IDs of the datapoints to delete.
    """


@dataclass(kw_only=True)
class DeleteDatapointsResponse:
    num_deleted_datapoints: int
    """
    The number of deleted datapoints.
    """


@dataclass(kw_only=True)
class GetDatapointsRequest:
    ids: list[str]
    """
    The IDs of the datapoints to retrieve. Required.
    """


@dataclass(kw_only=True)
class GetInferencesRequest:
    ids: list[str]
    """
    The IDs of the inferences to retrieve. Required.
    """
    output_source: InferenceOutputSource
    """
    Source of the inference output.
    Determines whether to return the original inference output or demonstration feedback
    (manually-curated output) if available.
    """
    function_name: str | None = None
    """
    Optional function name to filter by.
    Including this improves query performance since `function_name` is the first column
    in the ClickHouse primary key.
    """


@dataclass(kw_only=True)
class RawText:
    value: str


@dataclass(kw_only=True)
class Template:
    name: str
    arguments: dict[str, Any]


@dataclass(kw_only=True)
class Text:
    text: str


@dataclass(kw_only=True)
class ToolResult:
    name: str
    result: str
    id: str


@dataclass(kw_only=True)
class Unknown:
    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    model_provider_name: str | None = None
    """
    A fully-qualified name specifying when this content block should
    be included in the model provider input.
    """


@dataclass(kw_only=True)
class UpdateDatapointsMetadataRequest:
    datapoints: list[UpdateDatapointMetadataRequest]
    """
    The datapoints to update metadata for.
    """


@dataclass(kw_only=True)
class UpdateDatapointsResponse:
    ids: list[str]
    """
    The IDs of the datapoints that were updated.
    These are newly generated IDs for UpdateDatapoint requests, and they are the same IDs for UpdateDatapointMetadata requests.
    """


@dataclass(kw_only=True)
class StoredInputMessageContentToolCall(ToolCall):
    type: Literal["tool_call"] = "tool_call"


@dataclass(kw_only=True)
class StoredInputMessageContentThought(Thought):
    type: Literal["thought"] = "thought"


@dataclass(kw_only=True)
class StoragePath:
    kind: StorageKind
    path: str


@dataclass(kw_only=True)
class ContentBlockChatOutputToolCall(InferenceResponseToolCall):
    type: Literal["tool_call"] = "tool_call"


ContentBlockChatOutput = (
    ContentBlockChatOutputText
    | ContentBlockChatOutputToolCall
    | ContentBlockChatOutputThought
    | ContentBlockChatOutputUnknown
)


@dataclass(kw_only=True)
class OpenAIGrammarDefinition:
    syntax: OpenAIGrammarSyntax
    definition: str


@dataclass(kw_only=True)
class ProviderTool:
    tool: Any
    scope: ProviderToolScope | None = None


@dataclass(kw_only=True)
class InputMessageContentToolCall:
    type: Literal["tool_call"] = "tool_call"


@dataclass(kw_only=True)
class ObjectStoragePointer:
    mime_type: str
    storage_path: StoragePath
    source_url: str | None = None
    detail: Detail | None = None
    filename: str | None = None


@dataclass(kw_only=True)
class ObjectStorageFile:
    mime_type: str
    storage_path: StoragePath
    data: str
    source_url: str | None = None
    detail: Detail | None = None
    filename: str | None = None


@dataclass(kw_only=True)
class ObjectStorageError:
    mime_type: str
    storage_path: StoragePath
    source_url: str | None = None
    detail: Detail | None = None
    filename: str | None = None
    error: str | None = None


@dataclass(kw_only=True)
class FileObjectStoragePointer(ObjectStoragePointer):
    file_type: Literal["object_storage_pointer"] = "object_storage_pointer"


@dataclass(kw_only=True)
class FileObjectStorage(ObjectStorageFile):
    file_type: Literal["object_storage"] = "object_storage"


@dataclass(kw_only=True)
class FileObjectStorageError(ObjectStorageError):
    file_type: Literal["object_storage_error"] = "object_storage_error"


File = FileUrlFile | FileBase64 | FileObjectStoragePointer | FileObjectStorage | FileObjectStorageError


@dataclass(kw_only=True)
class InferenceFilterFloatMetric(FloatMetricFilter):
    type: Literal["float_metric"] = "float_metric"


@dataclass(kw_only=True)
class InferenceFilterBooleanMetric(BooleanMetricFilter):
    type: Literal["boolean_metric"] = "boolean_metric"


@dataclass(kw_only=True)
class InferenceFilterTag(TagFilter):
    type: Literal["tag"] = "tag"


@dataclass(kw_only=True)
class InferenceFilterTime(TimeFilter):
    type: Literal["time"] = "time"


@dataclass(kw_only=True)
class OrderByTimestamp:
    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["timestamp"] = "timestamp"


@dataclass(kw_only=True)
class OrderByMetric:
    direction: OrderDirection
    """
    The ordering direction.
    """
    name: str
    """
    The name of the metric to order by.
    """
    by: Literal["metric"] = "metric"


@dataclass(kw_only=True)
class OrderBySearchRelevance:
    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["search_relevance"] = "search_relevance"


OrderBy = OrderByTimestamp | OrderByMetric | OrderBySearchRelevance


@dataclass(kw_only=True)
class StoredInputMessageContentFile:
    mime_type: str
    storage_path: StoragePath
    type: Literal["file"] = "file"
    source_url: str | None = None
    detail: Detail | None = None
    filename: str | None = None


StoredInputMessageContent = (
    StoredInputMessageContentText
    | StoredInputMessageContentTemplate
    | StoredInputMessageContentToolCall
    | StoredInputMessageContentToolResult
    | StoredInputMessageContentRawText
    | StoredInputMessageContentThought
    | StoredInputMessageContentFile
    | StoredInputMessageContentUnknown
)


@dataclass(kw_only=True)
class OpenAICustomToolFormatGrammar:
    grammar: OpenAIGrammarDefinition
    type: Literal["grammar"] = "grammar"


OpenAICustomToolFormat = OpenAICustomToolFormatText | OpenAICustomToolFormatGrammar


@dataclass(kw_only=True)
class InputMessageContentFile:
    type: Literal["file"] = "file"


InputMessageContent = (
    InputMessageContentText
    | InputMessageContentTemplate
    | InputMessageContentToolCall
    | InputMessageContentToolResult
    | InputMessageContentRawText
    | InputMessageContentThought
    | InputMessageContentFile
    | InputMessageContentUnknown
)


@dataclass(kw_only=True)
class OpenAICustomTool:
    name: str
    description: str | None = None
    format: OpenAICustomToolFormat | None = None


@dataclass(kw_only=True)
class StoredInputMessage:
    role: Role
    content: list[StoredInputMessageContent]


@dataclass(kw_only=True)
class ToolOpenAICustom:
    name: str
    type: Literal["openai_custom"] = "openai_custom"
    description: str | None = None
    format: OpenAICustomToolFormat | None = None


Tool = ToolClientSideFunction | ToolOpenAICustom


@dataclass(kw_only=True)
class InputMessage:
    role: Role
    content: list[InputMessageContent]


@dataclass(kw_only=True)
class StoredInput:
    system: System | None = None
    messages: list[StoredInputMessage] | None = field(default_factory=lambda: [])


DynamicTool = Tool | FunctionTool


@dataclass(kw_only=True)
class Input:
    system: System | None = None
    messages: list[InputMessage] | None = field(default_factory=lambda: [])


@dataclass(kw_only=True)
class CreateChatDatapointRequest:
    function_name: str
    """
    The function name for this datapoint. Required.
    """
    input: Input
    """
    Input to the function. Required.
    """
    episode_id: str | None = None
    """
    Episode ID that the datapoint belongs to. Optional.
    """
    output: list[ContentBlockChatOutput] | None = None
    """
    Chat datapoint output. Optional.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    additional_tools: list[DynamicTool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = field(default_factory=lambda: [])
    """
    Provider-specific tool configurations
    """
    tags: dict[str, Any] | None = None
    """
    Tags associated with this datapoint. Optional.
    """
    name: str | None = None
    """
    Human-readable name for the datapoint. Optional.
    """


@dataclass(kw_only=True)
class CreateJsonDatapointRequest:
    function_name: str
    """
    The function name for this datapoint. Required.
    """
    input: Input
    """
    Input to the function. Required.
    """
    episode_id: str | None = None
    """
    Episode ID that the datapoint belongs to. Optional.
    """
    output: JsonDatapointOutputUpdate | None = None
    """
    JSON datapoint output. Optional.
    If provided, it will be validated against the output_schema. Invalid raw outputs will be stored as-is (not parsed), because we allow
    invalid outputs in datapoints by design.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. Optional.
    If not provided, the function's output schema will be used. If provided, it will be validated.
    """
    tags: dict[str, Any] | None = None
    """
    Tags associated with this datapoint. Optional.
    """
    name: str | None = None
    """
    Human-readable name for the datapoint. Optional.
    """


@dataclass(kw_only=True)
class CreateDatapointRequestChat(CreateChatDatapointRequest):
    type: Literal["chat"] = "chat"


@dataclass(kw_only=True)
class CreateDatapointRequestJson(CreateJsonDatapointRequest):
    type: Literal["json"] = "json"


CreateDatapointRequest = CreateDatapointRequestChat | CreateDatapointRequestJson


@dataclass(kw_only=True)
class ChatInferenceDatapoint:
    dataset_name: str
    function_name: str
    id: str
    input: StoredInput
    is_deleted: bool
    updated_at: str
    episode_id: str | None = None
    output: list[ContentBlockChatOutput] | None = None
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    additional_tools: list[DynamicTool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = field(default_factory=lambda: [])
    """
    Provider-specific tool configurations
    """
    tags: dict[str, Any] | None = None
    auxiliary: str | None = None
    is_custom: bool | None = False
    source_inference_id: str | None = None
    staled_at: str | None = None
    name: str | None = None


@dataclass(kw_only=True)
class JsonInferenceDatapoint:
    dataset_name: str
    function_name: str
    id: str
    input: StoredInput
    output_schema: Any
    is_deleted: bool
    updated_at: str
    episode_id: str | None = None
    output: JsonInferenceOutput | None = None
    tags: dict[str, Any] | None = None
    auxiliary: str | None = None
    is_custom: bool | None = False
    source_inference_id: str | None = None
    staled_at: str | None = None
    name: str | None = None


@dataclass(kw_only=True)
class DatapointChat(ChatInferenceDatapoint):
    type: Literal["chat"] = "chat"


@dataclass(kw_only=True)
class DatapointJson(JsonInferenceDatapoint):
    type: Literal["json"] = "json"


Datapoint = DatapointChat | DatapointJson


@dataclass(kw_only=True)
class StoredChatInference:
    function_name: str
    variant_name: str
    input: StoredInput
    output: list[ContentBlockChatOutput]
    timestamp: str
    episode_id: str
    inference_id: str
    dispreferred_outputs: list[list[ContentBlockChatOutput]] | None = field(default_factory=lambda: [])
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    additional_tools: list[DynamicTool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = field(default_factory=lambda: [])
    """
    Provider-specific tool configurations
    """
    tags: dict[str, str] | None = field(default_factory=lambda: {})


@dataclass(kw_only=True)
class StoredJsonInference:
    function_name: str
    variant_name: str
    input: StoredInput
    output: JsonInferenceOutput
    timestamp: str
    episode_id: str
    inference_id: str
    output_schema: Any
    dispreferred_outputs: list[JsonInferenceOutput] | None = field(default_factory=lambda: [])
    tags: dict[str, str] | None = field(default_factory=lambda: {})


@dataclass(kw_only=True)
class UpdateDynamicToolParamsRequest:
    allowed_tools: list[str] | None | UnsetType = UNSET
    """
    A subset of static tools configured for the function that the inference is explicitly allowed to use.
    If omitted, it will be left unchanged. If specified as `null`, it will be cleared (we allow function-configured tools plus additional tools
    provided at inference time). If specified as a value, it will be set to the provided value.
    """
    additional_tools: list[DynamicTool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured tools, that are also allowed.
    Modifying `additional_tools` DOES NOT automatically modify `allowed_tools`; `allowed_tools` must be explicitly updated to include
    new tools or exclude removed tools.
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    tool_choice: ToolChoice | None | UnsetType = UNSET
    """
    User-specified tool choice strategy.
    If omitted, it will be left unchanged. If specified as `null`, we will clear the dynamic tool choice and use function-configured tool choice.
    """
    parallel_tool_calls: bool | None | UnsetType = UNSET
    """
    Whether to use parallel tool calls in the inference.
    If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """


@dataclass(kw_only=True)
class UpdateChatDatapointRequestInternal:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    output: list[ContentBlockChatOutput] | None = None
    """
    Chat datapoint output. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will overwrite the existing output.
    """
    allowed_tools: list[str] | None | UnsetType = UNSET
    """
    A subset of static tools configured for the function that the inference is explicitly allowed to use.
    If omitted, it will be left unchanged. If specified as `null`, it will be cleared (we allow function-configured tools plus additional tools
    provided at inference time). If specified as a value, it will be set to the provided value.
    """
    additional_tools: list[DynamicTool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured tools, that are also allowed.
    Modifying `additional_tools` DOES NOT automatically modify `allowed_tools`; `allowed_tools` must be explicitly updated to include
    new tools or exclude removed tools.
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    tool_choice: ToolChoice | None | UnsetType = UNSET
    """
    User-specified tool choice strategy.
    If omitted, it will be left unchanged. If specified as `null`, we will clear the dynamic tool choice and use function-configured tool choice.
    """
    parallel_tool_calls: bool | None | UnsetType = UNSET
    """
    Whether to use parallel tool calls in the inference.
    If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    tool_params: UpdateDynamicToolParamsRequest | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Datapoint tool parameters.
    Moving forward, don't nest these fields.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """
    name: str | None | UnsetType = UNSET
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Metadata fields to update.
    Moving forward, don't nest these fields.
    """


@dataclass(kw_only=True)
class UpdateJsonDatapointRequestInternal:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    output: JsonDatapointOutputUpdate | None | UnsetType = UNSET
    """
    JSON datapoint output. If omitted, it will be left unchanged. If `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    This will be parsed and validated against output_schema, and valid `raw` values will be parsed and stored as `parsed`. Invalid `raw` values will
    also be stored, because we allow invalid outputs in datapoints by design.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    If not provided, the function's output schema will be used.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """
    name: str | None | UnsetType = UNSET
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Metadata fields to update.
    Moving forward, don't nest these fields.
    """


@dataclass(kw_only=True)
class UpdateChatDatapointRequest(UpdateChatDatapointRequestInternal):
    type: Literal["chat"] = "chat"


@dataclass(kw_only=True)
class UpdateJsonDatapointRequest(UpdateJsonDatapointRequestInternal):
    type: Literal["json"] = "json"


UpdateDatapointRequest = UpdateChatDatapointRequest | UpdateJsonDatapointRequest


@dataclass(kw_only=True)
class CreateDatapointsRequest:
    datapoints: list[CreateDatapointRequest]
    """
    The datapoints to create.
    """


@dataclass(kw_only=True)
class DynamicToolParams:
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    additional_tools: list[DynamicTool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = field(default_factory=lambda: [])
    """
    Provider-specific tool configurations
    """


@dataclass(kw_only=True)
class GetDatapointsResponse:
    datapoints: list[Datapoint]
    """
    The retrieved datapoints.
    """


@dataclass(kw_only=True)
class UpdateDatapointsRequest:
    datapoints: list[UpdateDatapointRequest]
    """
    The datapoints to update.
    """


@dataclass(kw_only=True)
class StoredInferenceChat(StoredChatInference):
    type: Literal["chat"] = "chat"


@dataclass(kw_only=True)
class StoredInferenceJson(StoredJsonInference):
    type: Literal["json"] = "json"


StoredInference = StoredInferenceChat | StoredInferenceJson


@dataclass(kw_only=True)
class GetInferencesResponse:
    inferences: list[StoredInference]
    """
    The retrieved inferences.
    """


@dataclass(kw_only=True)
class InferenceFilterAnd:
    children: list[InferenceFilter]
    type: Literal["and"] = "and"


@dataclass(kw_only=True)
class InferenceFilterOr:
    children: list[InferenceFilter]
    type: Literal["or"] = "or"


@dataclass(kw_only=True)
class InferenceFilterNot:
    child: InferenceFilter
    type: Literal["not"] = "not"


InferenceFilter = (
    InferenceFilterFloatMetric
    | InferenceFilterBooleanMetric
    | InferenceFilterTag
    | InferenceFilterTime
    | InferenceFilterAnd
    | InferenceFilterOr
    | InferenceFilterNot
)


@dataclass(kw_only=True)
class AndDatapointFilter:
    children: list[DatapointFilter]
    type: Literal["and"] = "and"


@dataclass(kw_only=True)
class OrDatapointFilter:
    children: list[DatapointFilter]
    type: Literal["or"] = "or"


@dataclass(kw_only=True)
class NotDatapointFilter:
    child: DatapointFilter
    type: Literal["not"] = "not"


DatapointFilter = TagDatapointFilter | TimeDatapointFilter | AndDatapointFilter | OrDatapointFilter | NotDatapointFilter


@dataclass(kw_only=True)
class CreateDatapointsFromInferenceRequestParamsInferenceQuery:
    function_name: str
    """
    The function name to filter inferences by.
    """
    type: Literal["inference_query"] = "inference_query"
    variant_name: str | None = None
    """
    Variant name to filter inferences by, optional.
    """
    filters: InferenceFilter | None = None
    """
    Filters to apply when querying inferences, optional.
    """


CreateDatapointsFromInferenceRequestParams = (
    CreateDatapointsFromInferenceRequestParamsInferenceIds | CreateDatapointsFromInferenceRequestParamsInferenceQuery
)


@dataclass(kw_only=True)
class ListDatapointsRequest:
    function_name: str | None = None
    """
    Optional function name to filter datapoints by.
    If provided, only datapoints from this function will be returned.
    """
    limit: int | None = None
    """
    The maximum number of datapoints to return.
    Defaults to 20.
    """
    page_size: int | None = None
    """
    The maximum number of datapoints to return. Defaults to 20.
    Deprecated: please use `limit`. If `limit` is provided, `page_size` is ignored.
    """
    offset: int | None = None
    """
    The number of datapoints to skip before starting to return results.
    Defaults to 0.
    """
    filter: DatapointFilter | None = None
    """
    Optional filter to apply when querying datapoints.
    Supports filtering by tags, time, and logical combinations (AND/OR/NOT).
    """


@dataclass(kw_only=True)
class ListInferencesRequest:
    output_source: InferenceOutputSource
    """
    Source of the inference output. Determines whether to return the original
    inference output or demonstration feedback (manually-curated output) if available.
    """
    function_name: str | None = None
    """
    Optional function name to filter inferences by.
    If provided, only inferences from this function will be returned.
    """
    variant_name: str | None = None
    """
    Optional variant name to filter inferences by.
    If provided, only inferences from this variant will be returned.
    """
    episode_id: str | None = None
    """
    Optional episode ID to filter inferences by.
    If provided, only inferences from this episode will be returned.
    """
    limit: int | None = None
    """
    The maximum number of inferences to return.
    Defaults to 20.
    """
    offset: int | None = None
    """
    The number of inferences to skip before starting to return results.
    Defaults to 0.
    """
    filter: InferenceFilter | None = None
    """
    Optional filter to apply when querying inferences.
    Supports filtering by metrics, tags, time, and logical combinations (AND/OR/NOT).
    """
    order_by: list[OrderBy] | None = None
    """
    Optional ordering criteria for the results.
    Supports multiple sort criteria (e.g., sort by timestamp then by metric).
    """
    search_query_experimental: str | None = None
    """
    Text query to filter. Case-insensitive substring search over the inferences' input and output.

    THIS FEATURE IS EXPERIMENTAL, and we may change or remove it at any time.
    We recommend against depending on this feature for critical use cases.

    Important limitations:
    - This requires an exact substring match; we do not tokenize this query string.
    - This doesn't search for any content in the template itself.
    - Quality is based on term frequency > 0, without any relevance scoring.
    - There are no performance guarantees (it's best effort only). Today, with no other
      filters, it will perform a full table scan, which may be extremely slow depending
      on the data volume.
    """
