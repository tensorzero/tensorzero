"""
Auto-generated Python dataclasses from JSON schemas.

This file is generated from JSON schemas in the tensorzero-core crate.
Do not edit this file manually - it will be overwritten.

Generated from schemas in: tensorzero-core/clients/schemas/

To regenerate, run:
    python generate_schema_types.py
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Literal

from .omit_type import OMIT, OmitType

Model = Any


@dataclass(kw_only=True)
class BooleanMetricFilter:
    metric_name: str
    value: bool


@dataclass(kw_only=True)
class ContentBlockChatOutputText:
    """
    InputMessages are validated against the input schema of the Function
    and then templated and transformed into RequestMessages for a particular Variant.
    They might contain tool calls or tool results along with text.
    The abstraction we use to represent this is ContentBlock, which is a union of Text, ToolCall, and ToolResult.
    ContentBlocks are collected into RequestMessages.
    These RequestMessages are collected into a ModelInferenceRequest,
    which should contain all information needed by a ModelProvider to perform the
    inference that is called for.
    """

    text: str
    type: Literal["text"] = "text"


@dataclass(kw_only=True)
class CreateDatapointsResponse:
    """
    Response from creating datapoints.
    """

    ids: list[str]
    """
    The IDs of the newly-generated datapoints.
    """


@dataclass(kw_only=True)
class DatapointMetadataUpdate:
    """
    A request to update the metadata of a datapoint.
    """

    name: str | None | OmitType = OMIT
    """
    Datapoint name.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared. If specified as a value, it will
    be set to the provided value.
    """


@dataclass(kw_only=True)
class DatasetMetadata:
    """
    Metadata for a single dataset.
    """

    datapoint_count: int
    """
    The total number of datapoints in the dataset.
    """
    dataset_name: str
    """
    The name of the dataset.
    """
    last_updated: str
    """
    The timestamp of the last update (ISO 8601 format).
    """


@dataclass(kw_only=True)
class DeleteDatapointsRequest:
    """
    Request to delete datapoints from a dataset.
    """

    ids: list[str]
    """
    The IDs of the datapoints to delete.
    """


@dataclass(kw_only=True)
class DeleteDatapointsResponse:
    """
    Response containing the number of deleted datapoints.
    """

    num_deleted_datapoints: int
    """
    The number of deleted datapoints.
    """


@dataclass(kw_only=True)
class DemonstrationFeedbackFilter:
    """
    Filter by whether an inference has a demonstration.
    """

    has_demonstration: bool


Detail = Literal["low", "high", "auto"]


@dataclass(kw_only=True)
class ProviderExtraBody:
    """
    DEPRECATED: Use `ModelProvider` instead.
    """

    model_provider_name: str
    """
    A fully-qualified model provider name in your configuration (e.g. `tensorzero::model_name::my_model::provider_name::my_provider`)
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    value: Any
    """
    The value to set the field to
    """


@dataclass(kw_only=True)
class ProviderExtraBodyDelete:
    """
    DEPRECATED: Use `ModelProviderDelete` instead.
    """

    delete: Literal[True] = True
    """
    Set to true to remove the field from the model provider request's body
    """
    model_provider_name: str
    """
    A fully-qualified model provider name in your configuration (e.g. `tensorzero::model_name::my_model::provider_name::my_provider`)
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """


@dataclass(kw_only=True)
class VariantExtraBody:
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    value: Any
    """
    The value to set the field to
    """
    variant_name: str
    """
    A variant name in your configuration (e.g. `my_variant`)
    """


@dataclass(kw_only=True)
class VariantExtraBodyDelete:
    delete: Literal[True] = True
    """
    Set to true to remove the field from the model provider request's body
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    variant_name: str
    """
    A variant name in your configuration (e.g. `my_variant`)
    """


@dataclass(kw_only=True)
class ModelProviderExtraBody:
    model_name: str
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    value: Any
    """
    The value to set the field to
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`)
    """


@dataclass(kw_only=True)
class ModelProviderExtraBodyDelete:
    delete: Literal[True] = True
    """
    Set to true to remove the field from the model provider request's body
    """
    model_name: str
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`)
    """


@dataclass(kw_only=True)
class AlwaysExtraBody:
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    value: Any
    """
    The value to set the field to
    """


@dataclass(kw_only=True)
class AlwaysExtraBodyDelete:
    delete: Literal[True] = True
    """
    Set to true to remove the field from the model provider request's body
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """


ExtraBody = (
    ProviderExtraBody
    | ProviderExtraBodyDelete
    | VariantExtraBody
    | VariantExtraBodyDelete
    | ModelProviderExtraBody
    | ModelProviderExtraBodyDelete
    | AlwaysExtraBody
    | AlwaysExtraBodyDelete
)


@dataclass(kw_only=True)
class ExtraBodyReplacementKindValue:
    value: Any


ExtraBodyReplacementKind = Literal["delete"] | ExtraBodyReplacementKindValue


@dataclass(kw_only=True)
class ProviderExtraHeader:
    """
    DEPRECATED: Use `ModelProvider` instead.
    """

    model_provider_name: str
    """
    A fully-qualified model provider name in your configuration (e.g. `tensorzero::model_name::my_model::provider_name::my_provider`)
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    value: str
    """
    The value of the HTTP header (e.g. `feature1,feature2,feature3`)
    """


@dataclass(kw_only=True)
class ProviderExtraHeaderDelete:
    """
    DEPRECATED: Use `ModelProviderDelete` instead.
    """

    delete: Literal[True] = True
    """
    Set to true to remove the header from the model provider request
    """
    model_provider_name: str
    """
    A fully-qualified model provider name in your configuration (e.g. `tensorzero::model_name::my_model::provider_name::my_provider`)
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """


@dataclass(kw_only=True)
class VariantExtraHeader:
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    value: str
    """
    The value of the HTTP header (e.g. `feature1,feature2,feature3`)
    """
    variant_name: str
    """
    A variant name in your configuration (e.g. `my_variant`)
    """


@dataclass(kw_only=True)
class VariantExtraHeaderDelete:
    delete: Literal[True] = True
    """
    Set to true to remove the header from the model provider request
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    variant_name: str
    """
    A variant name in your configuration (e.g. `my_variant`)
    """


@dataclass(kw_only=True)
class ModelProviderExtraHeader:
    model_name: str
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    value: str
    """
    The value of the HTTP header (e.g. `feature1,feature2,feature3`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`).
    """


@dataclass(kw_only=True)
class ModelProviderExtraHeaderDelete:
    delete: Literal[True] = True
    """
    Set to true to remove the header from the model provider request
    """
    model_name: str
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`)
    """


@dataclass(kw_only=True)
class AlwaysExtraHeader:
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    value: str
    """
    The value of the HTTP header (e.g. `feature1,feature2,feature3`)
    """


@dataclass(kw_only=True)
class AlwaysExtraHeaderDelete:
    delete: Literal[True] = True
    """
    Set to true to remove the header from the model provider request
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """


ExtraHeader = (
    ProviderExtraHeader
    | ProviderExtraHeaderDelete
    | VariantExtraHeader
    | VariantExtraHeaderDelete
    | ModelProviderExtraHeader
    | ModelProviderExtraHeaderDelete
    | AlwaysExtraHeader
    | AlwaysExtraHeaderDelete
)


FloatComparisonOperator = Literal["<", "<=", "=", ">", ">=", "!="]


@dataclass(kw_only=True)
class FloatMetricFilter:
    comparison_operator: FloatComparisonOperator
    metric_name: str
    value: float


@dataclass(kw_only=True)
class GetDatapointsRequest:
    """
    Request to get specific datapoints by their IDs.
    Used by the `POST /v1/datasets/{dataset_name}/get_datapoints` endpoint.
    """

    ids: list[str]
    """
    The IDs of the datapoints to retrieve. Required.
    """


@dataclass(kw_only=True)
class InferenceFilterFloatMetric(FloatMetricFilter):
    """
    Filter by the value of a float metric
    """

    type: Literal["float_metric"] = "float_metric"


@dataclass(kw_only=True)
class InferenceFilterBooleanMetric(BooleanMetricFilter):
    """
    Filter by the value of a boolean metric
    """

    type: Literal["boolean_metric"] = "boolean_metric"


@dataclass(kw_only=True)
class InferenceFilterDemonstrationFeedback(DemonstrationFeedbackFilter):
    """
    Filter by whether an inference has a demonstration.
    """

    type: Literal["demonstration_feedback"] = "demonstration_feedback"


InferenceOutputSource = str


@dataclass(kw_only=True)
class InferenceResponseToolCall:
    """
    An InferenceResponseToolCall is a request by a model to call a Tool
    in the form that we return to the client / ClickHouse
    This includes some synactic sugar (parsing / validation of the tool arguments)
    in the `arguments` field and the name in the `name` field.
    We support looping this back through the TensorZero inference API via the ToolCallWrapper
    """

    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """
    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """


@dataclass(kw_only=True)
class InputMessageContentText:
    """
    InputMessages are validated against the input schema of the Function
    and then templated and transformed into RequestMessages for a particular Variant.
    They might contain tool calls or tool results along with text.
    The abstraction we use to represent this is ContentBlock, which is a union of Text, ToolCall, and ToolResult.
    ContentBlocks are collected into RequestMessages.
    These RequestMessages are collected into a ModelInferenceRequest,
    which should contain all information needed by a ModelProvider to perform the
    inference that is called for.
    """

    text: str
    type: Literal["text"] = "text"


@dataclass(kw_only=True)
class InputMessageContentTemplate:
    arguments: dict[str, Any]
    name: str
    type: Literal["template"] = "template"


@dataclass(kw_only=True)
class InputMessageContentInferenceResponseToolCall(InferenceResponseToolCall):
    """
    `ToolCallWrapper` helps us disambiguate between `ToolCall` (no `raw_*`) and `InferenceResponseToolCall` (has `raw_*`).
    Typically tool calls come from previous inferences and are therefore outputs of TensorZero (`InferenceResponseToolCall`)
    but they may also be constructed client side or through the OpenAI endpoint `ToolCall` so we support both via this wrapper.
    """

    type: Literal["tool_call"] = "tool_call"


@dataclass(kw_only=True)
class InputMessageContentToolResult:
    """
    A ToolResult is the outcome of a ToolCall, which we may want to present back to the model
    """

    id: str
    name: str
    result: str
    type: Literal["tool_result"] = "tool_result"


@dataclass(kw_only=True)
class InputMessageContentRawText:
    """
    Struct that represents raw text content that should be passed directly to the model
    without any template processing or validation
    """

    type: Literal["raw_text"] = "raw_text"
    value: str


@dataclass(kw_only=True)
class JsonDatapointOutputUpdate:
    """
    A request to update the output of a JSON datapoint.

    We intentionally only accept the `raw` field, because JSON datapoints can contain invalid or malformed JSON for eval purposes.
    """

    raw: str | None = None
    """
    The raw output of the datapoint. For valid JSON outputs, this should be a JSON-serialized string.

    This will be parsed and validated against the datapoint's `output_schema`. Valid `raw` values will be parsed and stored as `parsed`, and
    invalid `raw` values will be stored as-is, because we allow invalid outputs in datapoints by design.
    """


@dataclass(kw_only=True)
class JsonInferenceOutput:
    parsed: Any | None = None
    """
    This is never omitted from the response even if it's None.
    """
    raw: str | None = None
    """
    This is never omitted from the response even if it's None. A `null` value indicates no output from the model.
    It's rare and unexpected from the model, but it's possible.
    """


JsonMode = Literal["off", "on", "strict", "tool"]


@dataclass(kw_only=True)
class ListDatasetsRequest:
    """
    Request to list datasets with optional filtering and pagination.
    Used by the `GET /internal/datasets` endpoint.
    """

    function_name: str | None = None
    """
    Optional function name to filter datasets by.
    If provided, only datasets with datapoints for this function will be returned.
    """
    limit: int | None = None
    """
    The maximum number of datasets to return.
    """
    offset: int | None = None
    """
    The number of datasets to skip before starting to return results.
    """


@dataclass(kw_only=True)
class ListDatasetsResponse:
    """
    Response containing a list of datasets.
    """

    datasets: list[DatasetMetadata]
    """
    List of dataset metadata.
    """


@dataclass(kw_only=True)
class OpenAICustomToolFormatText:
    type: Literal["text"] = "text"


OpenAIGrammarSyntax = Literal["lark", "regex"]


OrderDirection = Literal["ascending", "descending"]


@dataclass(kw_only=True)
class ProviderToolScopeModelProvider:
    model_name: str
    provider_name: str | None = None


@dataclass(kw_only=True)
class RawText:
    """
    Struct that represents raw text content that should be passed directly to the model
    without any template processing or validation
    """

    value: str


Role = Literal["user", "assistant"]


ServiceTier = Literal["auto", "default", "priority", "flex"]


@dataclass(kw_only=True)
class StorageKindS3Compatible:
    """
    Configuration for the object storage backend
    Currently, we only support S3-compatible object storage and local filesystem storage
    We test against Amazon S3, GCS, Cloudflare R2, and Minio
    """

    type: Literal["s3_compatible"] = "s3_compatible"
    allow_http: bool | None = None
    bucket_name: str | None = None
    endpoint: str | None = None
    prefix: str | None = ""
    """
    An extra prefix to prepend to the object key.
    This is only enabled in e2e tests, to prevent clashes between concurrent test runs.
    """
    region: str | None = None


@dataclass(kw_only=True)
class StorageKindFilesystem:
    """
    Configuration for the object storage backend
    Currently, we only support S3-compatible object storage and local filesystem storage
    We test against Amazon S3, GCS, Cloudflare R2, and Minio
    """

    path: str
    type: Literal["filesystem"] = "filesystem"


@dataclass(kw_only=True)
class StorageKindDisabled:
    """
    Configuration for the object storage backend
    Currently, we only support S3-compatible object storage and local filesystem storage
    We test against Amazon S3, GCS, Cloudflare R2, and Minio
    """

    type: Literal["disabled"] = "disabled"


StorageKind = StorageKindS3Compatible | StorageKindFilesystem | StorageKindDisabled


@dataclass(kw_only=True)
class StoragePath:
    """
    Path to a file in an object storage backend.
    This is part of the public API for `File`s. In particular, this is useful for roundtripping
    unresolved inputs from stored inferences or datapoints, without requiring clients to fetch
    file data first.
    """

    kind: StorageKind
    path: str


@dataclass(kw_only=True)
class StoredInputMessageContentText:
    """
    InputMessages are validated against the input schema of the Function
    and then templated and transformed into RequestMessages for a particular Variant.
    They might contain tool calls or tool results along with text.
    The abstraction we use to represent this is ContentBlock, which is a union of Text, ToolCall, and ToolResult.
    ContentBlocks are collected into RequestMessages.
    These RequestMessages are collected into a ModelInferenceRequest,
    which should contain all information needed by a ModelProvider to perform the
    inference that is called for.
    """

    text: str
    type: Literal["text"] = "text"


@dataclass(kw_only=True)
class StoredInputMessageContentTemplate:
    arguments: dict[str, Any]
    name: str
    type: Literal["template"] = "template"


@dataclass(kw_only=True)
class StoredInputMessageContentToolResult:
    """
    A ToolResult is the outcome of a ToolCall, which we may want to present back to the model
    """

    id: str
    name: str
    result: str
    type: Literal["tool_result"] = "tool_result"


@dataclass(kw_only=True)
class StoredInputMessageContentRawText:
    """
    Struct that represents raw text content that should be passed directly to the model
    without any template processing or validation
    """

    type: Literal["raw_text"] = "raw_text"
    value: str


@dataclass(kw_only=True)
class StoredInputMessageContentFile:
    """
    A file stored in an object storage backend, without data.
    This struct can be stored in the database. It's used by `StoredFile` (`StoredInput`).
    Note: `File` supports both `ObjectStorageFilePointer` and `ObjectStorageFile`.
    """

    mime_type: str
    storage_path: StoragePath
    type: Literal["file"] = "file"
    detail: Detail | None = None
    filename: str | None = None
    source_url: str | None = None


System = str | dict[str, Any]


TagComparisonOperator = Literal["=", "!="]


@dataclass(kw_only=True)
class TagFilter:
    """
    Filter by tag key-value pair.
    """

    comparison_operator: TagComparisonOperator
    key: str
    value: str


@dataclass(kw_only=True)
class Template:
    arguments: dict[str, Any]
    name: str


@dataclass(kw_only=True)
class Text:
    """
    InputMessages are validated against the input schema of the Function
    and then templated and transformed into RequestMessages for a particular Variant.
    They might contain tool calls or tool results along with text.
    The abstraction we use to represent this is ContentBlock, which is a union of Text, ToolCall, and ToolResult.
    ContentBlocks are collected into RequestMessages.
    These RequestMessages are collected into a ModelInferenceRequest,
    which should contain all information needed by a ModelProvider to perform the
    inference that is called for.
    """

    text: str


@dataclass(kw_only=True)
class ThoughtSummaryBlockSummaryText:
    text: str
    type: Literal["summary_text"] = "summary_text"


ThoughtSummaryBlock = ThoughtSummaryBlockSummaryText


TimeComparisonOperator = Literal["<", "<=", "=", ">", ">=", "!="]


@dataclass(kw_only=True)
class TimeFilter:
    """
    Filter by timestamp.
    """

    comparison_operator: TimeComparisonOperator
    time: str


@dataclass(kw_only=True)
class FunctionTool:
    """
    `FunctionTool` is a particular kind of tool that relies
    on the client to execute a function on their side (a ToolCall content block)
    and return the result on the next turn (a ToolCallResult).
    Notably, we assume there is a JSON schema `parameters` that specifies the
    set of arguments that the tool will accept.
    """

    description: str
    name: str
    parameters: Any
    type: Literal["function"] = "function"
    strict: bool | None = False
    """
    `strict` here specifies that TensorZero should attempt to use any facilities
    available from the model provider to force the model to generate an accurate tool call,
    notably OpenAI's strict tool call mode (https://platform.openai.com/docs/guides/function-calling#strict-mode).
    This imposes additional restrictions on the JSON schema that may vary across providers
    so we allow it to be configurable.
    """


@dataclass(kw_only=True)
class ToolCall:
    arguments: str
    id: str
    name: str


ToolCallWrapper = ToolCall | InferenceResponseToolCall


@dataclass(kw_only=True)
class ToolChoiceSpecific:
    """
    Forces the LLM to call a specific tool. The String is the name of the tool.
    """

    specific: str


ToolChoice = Literal["none", "auto", "required"] | ToolChoiceSpecific


@dataclass(kw_only=True)
class ToolResult:
    """
    A ToolResult is the outcome of a ToolCall, which we may want to present back to the model
    """

    id: str
    name: str
    result: str


UnfilteredInferenceExtraBody = list[ExtraBody]


@dataclass(kw_only=True)
class Unknown:
    """
    Struct that represents an unknown provider-specific content block.
    We pass this along as-is without any validation or transformation.
    """

    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    model_name: str | None = None
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`)
    """


@dataclass(kw_only=True)
class UpdateDatapointMetadataRequest:
    """
    A request to update the metadata of a single datapoint.
    """

    id: str
    """
    The ID of the datapoint to update. Required.
    """
    name: str | None | OmitType = OMIT
    """
    Datapoint name.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared. If specified as a value, it will
    be set to the provided value.
    """


@dataclass(kw_only=True)
class UpdateDatapointsMetadataRequest:
    """
    Request to update metadata for one or more datapoints in a dataset.
    Used by the `PATCH /v1/datasets/{dataset_id}/datapoints/metadata` endpoint.
    """

    datapoints: list[UpdateDatapointMetadataRequest]
    """
    The datapoints to update metadata for.
    """


@dataclass(kw_only=True)
class UpdateDatapointsResponse:
    """
    A response to a request to update one or more datapoints in a dataset.
    """

    ids: list[str]
    """
    The IDs of the datapoints that were updated.
    These are newly generated IDs for UpdateDatapoint requests, and they are the same IDs for UpdateDatapointMetadata requests.
    """


@dataclass(kw_only=True)
class UrlFile:
    """
    A file that can be located at a URL
    """

    url: str
    detail: Detail | None = None
    filename: str | None = None
    mime_type: str | None = None


@dataclass(kw_only=True)
class Base64File:
    """
    A file already encoded as base64
    """

    data: str
    mime_type: str
    detail: Detail | None = None
    filename: str | None = None
    source_url: str | None = None


@dataclass(kw_only=True)
class ChatCompletionInferenceParams:
    frequency_penalty: float | None = None
    json_mode: JsonMode | None = None
    max_tokens: int | None = None
    presence_penalty: float | None = None
    reasoning_effort: str | None = None
    seed: int | None = None
    service_tier: ServiceTier | None = None
    stop_sequences: list[str] | None = None
    temperature: float | None = None
    thinking_budget_tokens: int | None = None
    top_p: float | None = None
    verbosity: str | None = None


@dataclass(kw_only=True)
class ContentBlockChatOutputToolCall(InferenceResponseToolCall):
    """
    Defines the types of content block that can come from a `chat` function
    """

    type: Literal["tool_call"] = "tool_call"


@dataclass(kw_only=True)
class ContentBlockChatOutputUnknown(Unknown):
    """
    Defines the types of content block that can come from a `chat` function
    """

    type: Literal["unknown"] = "unknown"


@dataclass(kw_only=True)
class CreateDatapointsFromInferenceRequestParamsInferenceIds:
    """
    Create datapoints from specific inference IDs.
    """

    inference_ids: list[str]
    """
    The inference IDs to create datapoints from.
    """
    type: Literal["inference_ids"] = "inference_ids"
    output_source: InferenceOutputSource | None = None
    """
    When creating the datapoint, this specifies the source of the output for the datapoint.
    If not provided, by default we will use the original inference output as the datapoint's output
    (equivalent to `inference`).
    """


@dataclass(kw_only=True)
class TagDatapointFilter(TagFilter):
    """
    Filter by tag key-value pair
    """

    type: Literal["tag"] = "tag"


@dataclass(kw_only=True)
class TimeDatapointFilter(TimeFilter):
    """
    Filter by datapoint update time
    """

    type: Literal["time"] = "time"


@dataclass(kw_only=True)
class DatapointOrderByTimestamp:
    """
    Creation timestamp of the datapoint.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["timestamp"] = "timestamp"


@dataclass(kw_only=True)
class DatapointOrderBySearchRelevance:
    """
    Relevance score of the search query in the input and output of the datapoint.
    Requires a search query (experimental). If it's not provided, we return an error.

    Current relevance metric is very rudimentary (just term frequency), but we plan
    to improve it in the future.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["search_relevance"] = "search_relevance"


DatapointOrderBy = DatapointOrderByTimestamp | DatapointOrderBySearchRelevance


@dataclass(kw_only=True)
class FileUrlFile(UrlFile):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["url"] = "url"


@dataclass(kw_only=True)
class FileBase64(Base64File):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["base64"] = "base64"


@dataclass(kw_only=True)
class GetInferencesRequest:
    """
    Request to get specific inferences by their IDs.
    Used by the `POST /v1/inferences/get_inferences` endpoint.
    """

    ids: list[str]
    """
    The IDs of the inferences to retrieve. Required.
    """
    function_name: str | None = None
    """
    Optional function name to filter by.
    Including this improves query performance since `function_name` is the first column
    in the ClickHouse primary key.
    """
    output_source: InferenceOutputSource | None = "inference"
    """
    Source of the inference output.
    Determines whether to return the original inference output or demonstration feedback
    (manually-curated output) if available.
    Defaults to `Inference` if not specified.
    """


@dataclass(kw_only=True)
class InferenceFilterTag(TagFilter):
    """
    Filter by tag key-value pair
    """

    type: Literal["tag"] = "tag"


@dataclass(kw_only=True)
class InferenceFilterTime(TimeFilter):
    """
    Filter by the timestamp of an inference.
    """

    type: Literal["time"] = "time"


@dataclass(kw_only=True)
class InferenceParams:
    """
    InferenceParams is the top-level struct for inference parameters.
    We backfill these from the configs given in the variants used and ultimately write them to the database.
    """

    chat_completion: ChatCompletionInferenceParams


@dataclass(kw_only=True)
class InputMessageContentToolCall(ToolCall):
    """
    `ToolCallWrapper` helps us disambiguate between `ToolCall` (no `raw_*`) and `InferenceResponseToolCall` (has `raw_*`).
    Typically tool calls come from previous inferences and are therefore outputs of TensorZero (`InferenceResponseToolCall`)
    but they may also be constructed client side or through the OpenAI endpoint `ToolCall` so we support both via this wrapper.
    """

    type: Literal["tool_call"] = "tool_call"


@dataclass(kw_only=True)
class InputMessageContentUnknown(Unknown):
    """
    An unknown content block type, used to allow passing provider-specific
    content blocks (e.g. Anthropic's `redacted_thinking`) in and out
    of TensorZero.
    The `data` field holds the original content block from the provider,
    without any validation or transformation by TensorZero.
    """

    type: Literal["unknown"] = "unknown"


@dataclass(kw_only=True)
class ObjectStorageError:
    """
    A file that we failed to read from object storage.
    This struct can NOT be stored in the database.
    """

    mime_type: str
    storage_path: StoragePath
    detail: Detail | None = None
    error: str | None = None
    filename: str | None = None
    source_url: str | None = None


@dataclass(kw_only=True)
class ObjectStorageFile:
    """
    A file stored in an object storage backend, with data.
    This struct can NOT be stored in the database.
    Note: `File` supports both `ObjectStorageFilePointer` and `ObjectStorageFile`.
    """

    data: str
    mime_type: str
    storage_path: StoragePath
    detail: Detail | None = None
    filename: str | None = None
    source_url: str | None = None


@dataclass(kw_only=True)
class ObjectStoragePointer:
    """
    A file stored in an object storage backend, without data.
    This struct can be stored in the database. It's used by `StoredFile` (`StoredInput`).
    Note: `File` supports both `ObjectStorageFilePointer` and `ObjectStorageFile`.
    """

    mime_type: str
    storage_path: StoragePath
    detail: Detail | None = None
    filename: str | None = None
    source_url: str | None = None


@dataclass(kw_only=True)
class OpenAIGrammarDefinition:
    definition: str
    syntax: OpenAIGrammarSyntax


@dataclass(kw_only=True)
class OrderByTimestamp:
    """
    Creation timestamp of the item.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["timestamp"] = "timestamp"


@dataclass(kw_only=True)
class OrderByMetric:
    """
    Value of a metric.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["metric"] = "metric"
    name: str
    """
    The name of the metric to order by.
    """


@dataclass(kw_only=True)
class OrderBySearchRelevance:
    """
    Relevance score of the search query in the input and output of the item.
    Requires a search query (experimental). If it's not provided, we return an error.

    Current relevance metric is very rudimentary (just term frequency), but we plan
    to improve it in the future.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["search_relevance"] = "search_relevance"


OrderBy = OrderByTimestamp | OrderByMetric | OrderBySearchRelevance


ProviderToolScope = ProviderToolScopeModelProvider | None


@dataclass(kw_only=True)
class StoredInputMessageContentToolCall(ToolCall):
    type: Literal["tool_call"] = "tool_call"


@dataclass(kw_only=True)
class StoredInputMessageContentUnknown(Unknown):
    type: Literal["unknown"] = "unknown"


@dataclass(kw_only=True)
class Thought:
    """
    Struct that represents a model's reasoning
    """

    extra_data: Any | None = None
    """
    Provider-specific opaque data for multi-turn reasoning support.
    For example, OpenRouter stores encrypted reasoning blocks with `{"format": "...", "encrypted": true}` structure.
    Note: Not exposed to Python because `Value` doesn't implement `IntoPyObject`.
    """
    provider_type: str | None = None
    """
    When set, this `Thought` block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """
    signature: str | None = None
    """
    An optional signature - used with Anthropic and OpenRouter for multi-turn
    reasoning conversations. Other providers will ignore this field.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    text: str | None = None


@dataclass(kw_only=True)
class ContentBlockChatOutputThought(Thought):
    """
    Defines the types of content block that can come from a `chat` function
    """

    type: Literal["thought"] = "thought"


ContentBlockChatOutput = (
    ContentBlockChatOutputText
    | ContentBlockChatOutputToolCall
    | ContentBlockChatOutputThought
    | ContentBlockChatOutputUnknown
)


@dataclass(kw_only=True)
class FileObjectStoragePointer(ObjectStoragePointer):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["object_storage_pointer"] = "object_storage_pointer"


@dataclass(kw_only=True)
class FileObjectStorage(ObjectStorageFile):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["object_storage"] = "object_storage"


@dataclass(kw_only=True)
class FileObjectStorageError(ObjectStorageError):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["object_storage_error"] = "object_storage_error"


File = FileUrlFile | FileBase64 | FileObjectStoragePointer | FileObjectStorage | FileObjectStorageError


@dataclass(kw_only=True)
class InputMessageContentThought(Thought):
    type: Literal["thought"] = "thought"


@dataclass(kw_only=True)
class InputMessageContentFile:
    type: Literal["file"] = "file"


InputMessageContent = (
    InputMessageContentText
    | InputMessageContentTemplate
    | InputMessageContentToolCall
    | InputMessageContentInferenceResponseToolCall
    | InputMessageContentToolResult
    | InputMessageContentRawText
    | InputMessageContentThought
    | InputMessageContentFile
    | InputMessageContentUnknown
)


@dataclass(kw_only=True)
class OpenAICustomToolFormatGrammar:
    grammar: OpenAIGrammarDefinition
    type: Literal["grammar"] = "grammar"


OpenAICustomToolFormat = OpenAICustomToolFormatText | OpenAICustomToolFormatGrammar


@dataclass(kw_only=True)
class ProviderTool:
    tool: Any
    scope: ProviderToolScope | None = None


@dataclass(kw_only=True)
class StoredInputMessageContentThought(Thought):
    type: Literal["thought"] = "thought"


StoredInputMessageContent = (
    StoredInputMessageContentText
    | StoredInputMessageContentTemplate
    | StoredInputMessageContentToolCall
    | StoredInputMessageContentToolResult
    | StoredInputMessageContentRawText
    | StoredInputMessageContentThought
    | StoredInputMessageContentFile
    | StoredInputMessageContentUnknown
)


@dataclass(kw_only=True)
class OpenAICustomTool:
    """
    `OpenAICustomTool` represents OpenAI's custom tool format, which allows
    for text or grammar-based tool definitions beyond standard function calling.
    Currently, this type is a wire + outbound + storage type so it forces a consistent format.
    This only applies to the Chat Completions API. The Responses API has a slightly different request
    shape so we implement a conversion in `responses.rs`.
    """

    name: str
    type: Literal["openai_custom"] = "openai_custom"
    description: str | None = None
    format: OpenAICustomToolFormat | None = None


Tool = FunctionTool | OpenAICustomTool


@dataclass(kw_only=True)
class UpdateDynamicToolParamsRequest:
    """
    A request to update the dynamic tool parameters of a datapoint.
    """

    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured tools, that are also allowed.
    Modifying `additional_tools` DOES NOT automatically modify `allowed_tools`; `allowed_tools` must be explicitly updated to include
    new tools or exclude removed tools.
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    allowed_tools: list[str] | None | OmitType = OMIT
    """
    A subset of static tools configured for the function that the inference is explicitly allowed to use.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared (we allow function-configured tools
    plus additional tools provided at inference time). If specified as a value, it will be set to the provided value.
    """
    parallel_tool_calls: bool | None | OmitType = OMIT
    """
    Whether to use parallel tool calls in the inference.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared (we will use function-configured
    parallel tool calls). If specified as a value, it will be set to the provided value.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    tool_choice: ToolChoice | None | OmitType = OMIT
    """
    User-specified tool choice strategy.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared (we will use function-configured
    tool choice). If specified as a value, it will be set to the provided value.
    """


@dataclass(kw_only=True)
class DynamicToolParams:
    """
    Wire/API representation of dynamic tool parameters for inference requests.

    This type is the **wire format** for tool configurations used in API requests and responses.
    It distinguishes between static tools (configured in the function) and dynamic tools
    (provided at runtime), allowing clients to reference pre-configured tools by name or
    provide new tools on-the-fly.

    # Purpose
    - Accept tool parameters in inference API requests (e.g., `/inference/{function_name}`)
    - Expose tool configurations in API responses for stored inferences
    - Support Python and TypeScript client bindings
    - Allow runtime customization of tool behavior

    # Fields
    - `allowed_tools`: Names of static tools from function config to use (subset selection)
    - `additional_tools`: New tools defined at runtime (not in static config)
    - `tool_choice`: Override the function's default tool choice strategy
    - `parallel_tool_calls`: Override whether parallel tool calls are enabled
    - `provider_tools`: Provider-specific tool configurations (not persisted to database)

    # Key Differences from ToolCallConfigDatabaseInsert
    - **Separate lists**: Maintains distinction between static (`allowed_tools`) and dynamic (`additional_tools`) tools
    - **By reference**: Static tools referenced by name, not duplicated
    - **Has provider_tools**: Can specify provider-specific tool configurations
    - **Has bindings**: Exposed to Python/TypeScript via `pyo3` and `ts_rs`

    # Conversion to Storage Format
    Converting from `DynamicToolParams` to `ToolCallConfigDatabaseInsert` is a **lossy** operation:
    1. Static tools (from `allowed_tools` names) are resolved from function config
    2. Dynamic tools (from `additional_tools`) are included as-is
    3. Both lists are merged into a single `tools_available` list
    4. The distinction between static and dynamic tools is lost
    5. `provider_tools` are dropped (not stored)

    Use `FunctionConfig::dynamic_tool_params_to_database_insert()` for this conversion.

    # Conversion from Storage Format
    Converting from `ToolCallConfigDatabaseInsert` back to `DynamicToolParams` reconstructs the original:
    1. `dynamic_tools` -> `additional_tools`
    2. `allowed_tools` -> `allowed_tools` (based on choice enum)
    3. Other fields copied directly

    Use `From<ToolCallConfigDatabaseInsert> for DynamicToolParams` for this conversion.

    # Example
    ```rust,ignore
    // API request with dynamic tool params
    let params = DynamicToolParams {
        allowed_tools: Some(vec!["calculator".to_string()]),  // Use only the calculator tool from config
        additional_tools: Some(vec![Tool {  runtime tool  }]),  // Add a new tool
        tool_choice: Some(ToolChoice::Required),
        parallel_tool_calls: Some(true),
        provider_tools: vec![],
    };

    // Convert to storage format
    let db_insert = function_config
        .dynamic_tool_params_to_database_insert(params, &static_tools)?
        .unwrap_or_default();

    // db_insert.tools_available now contains both the calculator tool (from config)
    // and the runtime tool (from additional_tools), merged together
    ```

    See also: [`ToolCallConfigDatabaseInsert`] for the storage/database format
    """

    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = field(default_factory=lambda: [])
    """
    Provider-specific tool configurations
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """


@dataclass(kw_only=True)
class InputMessage:
    """
    InputMessage and Role are our representation of the input sent by the client
    prior to any processing into LLM representations below.
    `InputMessage` has a custom deserializer that addresses legacy data formats that we used to support (see input_message.rs).
    """

    content: list[InputMessageContent]
    role: Role


@dataclass(kw_only=True)
class StoredInputMessage:
    """
    `StoredInputMessage` has a custom deserializer that addresses legacy data formats in the database (see below).
    """

    content: list[StoredInputMessageContent]
    role: Role


@dataclass(kw_only=True)
class Input:
    """
    API representation of an input to a model.
    """

    messages: list[InputMessage] | None = field(default_factory=lambda: [])
    """
    Messages in the input.
    """
    system: System | None = None
    """
    System prompt of the input.
    """


@dataclass(kw_only=True)
class JsonInferenceDatapoint:
    dataset_name: str
    function_name: str
    id: str
    input: Input
    is_deleted: bool
    output_schema: Any
    updated_at: str
    auxiliary: str | None = None
    episode_id: str | None = None
    is_custom: bool | None = False
    name: str | None = None
    output: JsonInferenceOutput | None = None
    source_inference_id: str | None = None
    staled_at: str | None = None
    tags: dict[str, Any] | None = None


@dataclass(kw_only=True)
class StoredInput:
    """
    The input type that we directly store in ClickHouse.
    This is almost identical to `ResolvedInput`, but without `File` data.
    Only the object-storage path is actually stored in clickhouse
    (which can be used to re-fetch the file and produce a `ResolvedInput`).

    `StoredInputMessage` has a custom deserializer that addresses legacy data formats in the database.
    """

    messages: list[StoredInputMessage] | None = field(default_factory=lambda: [])
    system: System | None = None


@dataclass(kw_only=True)
class StoredJsonInference:
    episode_id: str
    function_name: str
    inference_id: str
    inference_params: InferenceParams
    input: StoredInput
    output: JsonInferenceOutput
    output_schema: Any
    timestamp: str
    variant_name: str
    dispreferred_outputs: list[JsonInferenceOutput] | None = field(default_factory=lambda: [])
    extra_body: UnfilteredInferenceExtraBody | None = field(default_factory=lambda: [])
    processing_time_ms: int | None = None
    tags: dict[str, str] | None = field(default_factory=lambda: {})
    ttft_ms: int | None = None


@dataclass(kw_only=True)
class UpdateChatDatapointRequestInternal:
    """
    An update request for a chat datapoint.
    """

    id: str
    """
    The ID of the datapoint to update. Required.
    """
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured tools, that are also allowed.
    Modifying `additional_tools` DOES NOT automatically modify `allowed_tools`; `allowed_tools` must be explicitly updated to include
    new tools or exclude removed tools.
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    allowed_tools: list[str] | None | OmitType = OMIT
    """
    A subset of static tools configured for the function that the inference is explicitly allowed to use.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared (we allow function-configured tools
    plus additional tools provided at inference time). If specified as a value, it will be set to the provided value.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Metadata fields to update.
    Moving forward, don't nest these fields.
    """
    name: str | None | OmitType = OMIT
    """
    Datapoint name.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared. If specified as a value, it will
    be set to the provided value.
    """
    output: list[ContentBlockChatOutput] | None | OmitType = OMIT
    """
    Chat datapoint output.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared.
    Otherwise, it will overwrite the existing output (and can be an empty list).
    """
    parallel_tool_calls: bool | None | OmitType = OMIT
    """
    Whether to use parallel tool calls in the inference.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared (we will use function-configured
    parallel tool calls). If specified as a value, it will be set to the provided value.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared.
    Otherwise, it will overwrite the existing tags.
    """
    tool_choice: ToolChoice | None | OmitType = OMIT
    """
    User-specified tool choice strategy.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared (we will use function-configured
    tool choice). If specified as a value, it will be set to the provided value.
    """
    tool_params: UpdateDynamicToolParamsRequest | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Datapoint tool parameters.
    Moving forward, don't nest these fields.
    """


@dataclass(kw_only=True)
class UpdateChatDatapointRequest(UpdateChatDatapointRequestInternal):
    """
    Request to update a chat datapoint.
    """

    type: Literal["chat"] = "chat"


@dataclass(kw_only=True)
class UpdateJsonDatapointRequestInternal:
    """
    An update request for a JSON datapoint.
    """

    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Metadata fields to update.
    Moving forward, don't nest these fields.
    """
    name: str | None | OmitType = OMIT
    """
    Datapoint name.

    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared. If specified as a value, it will
    be set to the provided value.
    """
    output: JsonDatapointOutputUpdate | None | OmitType = OMIT
    """
    JSON datapoint output.
    If omitted (which uses the default value `OMIT`), it will be left unchanged. If set to `None`, it will be cleared (represents edge case where
    inference succeeded but model didn't output relevant content blocks). Otherwise, it will overwrite the existing output.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    If not provided, the function's output schema will be used.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """


@dataclass(kw_only=True)
class ChatInferenceDatapoint:
    """
    Wire variant of ChatInferenceDatapoint for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces.
    """

    dataset_name: str
    function_name: str
    id: str
    input: Input
    is_deleted: bool
    updated_at: str
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    auxiliary: str | None = None
    episode_id: str | None = None
    is_custom: bool | None = False
    name: str | None = None
    output: list[ContentBlockChatOutput] | None = None
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = field(default_factory=lambda: [])
    """
    Provider-specific tool configurations
    """
    source_inference_id: str | None = None
    staled_at: str | None = None
    tags: dict[str, Any] | None = None
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """


@dataclass(kw_only=True)
class CreateChatDatapointRequest:
    """
    A request to create a chat datapoint.
    """

    function_name: str
    """
    The function name for this datapoint. Required.
    """
    input: Input
    """
    Input to the function. Required.
    """
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    episode_id: str | None = None
    """
    Episode ID that the datapoint belongs to. Optional.
    """
    name: str | None = None
    """
    Human-readable name for the datapoint. Optional.
    """
    output: list[ContentBlockChatOutput] | None = None
    """
    Chat datapoint output. Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = field(default_factory=lambda: [])
    """
    Provider-specific tool configurations
    """
    tags: dict[str, Any] | None = None
    """
    Tags associated with this datapoint. Optional.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """


@dataclass(kw_only=True)
class CreateDatapointRequestChat(CreateChatDatapointRequest):
    """
    Request to create a chat datapoint.
    """

    type: Literal["chat"] = "chat"


@dataclass(kw_only=True)
class CreateJsonDatapointRequest:
    """
    A request to create a JSON datapoint.
    """

    function_name: str
    """
    The function name for this datapoint. Required.
    """
    input: Input
    """
    Input to the function. Required.
    """
    episode_id: str | None = None
    """
    Episode ID that the datapoint belongs to. Optional.
    """
    name: str | None = None
    """
    Human-readable name for the datapoint. Optional.
    """
    output: JsonDatapointOutputUpdate | None = None
    """
    JSON datapoint output. Optional.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. Optional.
    If not provided, the function's output schema will be used. If provided, it will be validated.
    """
    tags: dict[str, Any] | None = None
    """
    Tags associated with this datapoint. Optional.
    """


@dataclass(kw_only=True)
class DatapointChat(ChatInferenceDatapoint):
    """
    Wire variant of Datapoint enum for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces.
    """

    type: Literal["chat"] = "chat"


@dataclass(kw_only=True)
class DatapointJson(JsonInferenceDatapoint):
    """
    Wire variant of Datapoint enum for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces.
    """

    type: Literal["json"] = "json"


Datapoint = DatapointChat | DatapointJson


@dataclass(kw_only=True)
class GetDatapointsResponse:
    """
    Response containing the requested datapoints.
    """

    datapoints: list[Datapoint]
    """
    The retrieved datapoints.
    """


@dataclass(kw_only=True)
class StoredChatInference:
    """
    Wire variant of StoredChatInference for API responses with Python/TypeScript bindings
    """

    episode_id: str
    function_name: str
    inference_id: str
    inference_params: InferenceParams
    input: StoredInput
    output: list[ContentBlockChatOutput]
    timestamp: str
    variant_name: str
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    dispreferred_outputs: list[list[ContentBlockChatOutput]] | None = field(default_factory=lambda: [])
    extra_body: UnfilteredInferenceExtraBody | None = field(default_factory=lambda: [])
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    processing_time_ms: int | None = None
    provider_tools: list[ProviderTool] | None = field(default_factory=lambda: [])
    """
    Provider-specific tool configurations
    """
    tags: dict[str, str] | None = field(default_factory=lambda: {})
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    ttft_ms: int | None = None


@dataclass(kw_only=True)
class StoredInferenceChat(StoredChatInference):
    """
    Wire variant of StoredInference for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces
    """

    type: Literal["chat"] = "chat"


@dataclass(kw_only=True)
class StoredInferenceJson(StoredJsonInference):
    """
    Wire variant of StoredInference for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces
    """

    type: Literal["json"] = "json"


StoredInference = StoredInferenceChat | StoredInferenceJson


@dataclass(kw_only=True)
class UpdateJsonDatapointRequest(UpdateJsonDatapointRequestInternal):
    """
    Request to update a JSON datapoint.
    """

    type: Literal["json"] = "json"


UpdateDatapointRequest = UpdateChatDatapointRequest | UpdateJsonDatapointRequest


@dataclass(kw_only=True)
class UpdateDatapointsRequest:
    """
    Request to update one or more datapoints in a dataset.
    """

    datapoints: list[UpdateDatapointRequest]
    """
    The datapoints to update.
    """


@dataclass(kw_only=True)
class CreateDatapointRequestJson(CreateJsonDatapointRequest):
    """
    Request to create a JSON datapoint.
    """

    type: Literal["json"] = "json"


CreateDatapointRequest = CreateDatapointRequestChat | CreateDatapointRequestJson


@dataclass(kw_only=True)
class CreateDatapointsRequest:
    """
    Request to create datapoints manually.
    Used by the `POST /v1/datasets/{dataset_id}/datapoints` endpoint.
    """

    datapoints: list[CreateDatapointRequest]
    """
    The datapoints to create.
    """


@dataclass(kw_only=True)
class GetInferencesResponse:
    """
    Response containing the requested inferences.
    """

    inferences: list[StoredInference]
    """
    The retrieved inferences.
    """


@dataclass(kw_only=True)
class CreateDatapointsFromInferenceRequestParamsInferenceQuery:
    """
    Create datapoints from an inference query.
    """

    type: Literal["inference_query"] = "inference_query"
    after: str | None = None
    """
    Optional inference ID to paginate after (exclusive).
    Returns inferences with IDs after this one (later in time).
    Cannot be used together with `before` or `offset`.
    """
    before: str | None = None
    """
    Optional inference ID to paginate before (exclusive).
    Returns inferences with IDs before this one (earlier in time).
    Cannot be used together with `after` or `offset`.
    """
    episode_id: str | None = None
    """
    Optional episode ID to filter inferences by.
    If provided, only inferences from this episode will be returned.
    """
    filter: InferenceFilter | None = None
    """
    **Deprecated:** Use `filters` instead. This field will be removed in a future release.
    """
    filters: InferenceFilter | None = None
    """
    Optional filter to apply when querying inferences.
    Supports filtering by metrics, tags, time, and logical combinations (AND/OR/NOT).
    """
    function_name: str | None = None
    """
    Optional function name to filter inferences by.
    If provided, only inferences from this function will be returned.
    """
    limit: int | None = None
    """
    The maximum number of inferences to return.
    Defaults to 20.
    """
    offset: int | None = None
    """
    The number of inferences to skip before starting to return results.
    Defaults to 0.
    """
    order_by: list[OrderBy] | None = None
    """
    Optional ordering criteria for the results.
    Supports multiple sort criteria (e.g., sort by timestamp then by metric).
    """
    output_source: InferenceOutputSource | None = "inference"
    """
    Source of the inference output. Determines whether to return the original
    inference output or demonstration feedback (manually-curated output) if available.
    Defaults to `Inference` if not specified.
    """
    search_query_experimental: str | None = None
    """
    Text query to filter. Case-insensitive substring search over the inferences' input and output.

    THIS FEATURE IS EXPERIMENTAL, and we may change or remove it at any time.
    We recommend against depending on this feature for critical use cases.

    Important limitations:
    - This requires an exact substring match; we do not tokenize this query string.
    - This doesn't search for any content in the template itself.
    - Quality is based on term frequency > 0, without any relevance scoring.
    - There are no performance guarantees (it's best effort only). Today, with no other
      filters, it will perform a full table scan, which may be extremely slow depending
      on the data volume.
    """
    variant_name: str | None = None
    """
    Optional variant name to filter inferences by.
    If provided, only inferences from this variant will be returned.
    """


CreateDatapointsFromInferenceRequestParams = (
    CreateDatapointsFromInferenceRequestParamsInferenceIds | CreateDatapointsFromInferenceRequestParamsInferenceQuery
)


@dataclass(kw_only=True)
class AndDatapointFilter:
    """
    Logical AND of multiple filters
    """

    children: list[DatapointFilter]
    type: Literal["and"] = "and"


@dataclass(kw_only=True)
class OrDatapointFilter:
    """
    Logical OR of multiple filters
    """

    children: list[DatapointFilter]
    type: Literal["or"] = "or"


@dataclass(kw_only=True)
class NotDatapointFilter:
    """
    Logical NOT of a filter
    """

    child: DatapointFilter
    type: Literal["not"] = "not"


DatapointFilter = TagDatapointFilter | TimeDatapointFilter | AndDatapointFilter | OrDatapointFilter | NotDatapointFilter


@dataclass(kw_only=True)
class InferenceFilterAnd:
    """
    Logical AND of multiple filters
    """

    children: list[InferenceFilter]
    type: Literal["and"] = "and"


@dataclass(kw_only=True)
class InferenceFilterOr:
    """
    Logical OR of multiple filters
    """

    children: list[InferenceFilter]
    type: Literal["or"] = "or"


@dataclass(kw_only=True)
class InferenceFilterNot:
    """
    Logical NOT of a filter
    """

    child: InferenceFilter
    type: Literal["not"] = "not"


InferenceFilter = (
    InferenceFilterFloatMetric
    | InferenceFilterBooleanMetric
    | InferenceFilterDemonstrationFeedback
    | InferenceFilterTag
    | InferenceFilterTime
    | InferenceFilterAnd
    | InferenceFilterOr
    | InferenceFilterNot
)


@dataclass(kw_only=True)
class ListDatapointsRequest:
    """
    Request to list datapoints from a dataset with pagination and filters.
    Used by the `POST /v1/datasets/{dataset_id}/list_datapoints` endpoint.
    """

    filter: DatapointFilter | None = None
    """
    Optional filter to apply when querying datapoints.
    Supports filtering by tags, time, and logical combinations (AND/OR/NOT).
    """
    function_name: str | None = None
    """
    Optional function name to filter datapoints by.
    If provided, only datapoints from this function will be returned.
    """
    limit: int | None = None
    """
    The maximum number of datapoints to return.
    Defaults to 20.
    """
    offset: int | None = None
    """
    The number of datapoints to skip before starting to return results.
    Defaults to 0.
    """
    order_by: list[DatapointOrderBy] | None = None
    """
    Optional ordering criteria for the results.
    Supports multiple sort criteria (e.g., sort by timestamp then by search relevance).
    """
    page_size: int | None = None
    """
    The maximum number of datapoints to return. Defaults to 20.
    Deprecated: please use `limit`. If `limit` is provided, `page_size` is ignored.
    """
    search_query_experimental: str | None = None
    """
    Text query to filter. Case-insensitive substring search over the datapoints' input and output.

    THIS FEATURE IS EXPERIMENTAL, and we may change or remove it at any time.
    We recommend against depending on this feature for critical use cases.

    Important limitations:
    - This requires an exact substring match; we do not tokenize this query string.
    - This doesn't search for any content in the template itself.
    - Quality is based on term frequency > 0, without any relevance scoring.
    - There are no performance guarantees (it's best effort only). Today, with no other
      filters, it will perform a full table scan, which may be extremely slow depending
      on the data volume.
    """


@dataclass(kw_only=True)
class ListInferencesRequest:
    """
    Request to list inferences with pagination and filters.
    Used by the `POST /v1/inferences/list_inferences` endpoint.
    """

    after: str | None = None
    """
    Optional inference ID to paginate after (exclusive).
    Returns inferences with IDs after this one (later in time).
    Cannot be used together with `before` or `offset`.
    """
    before: str | None = None
    """
    Optional inference ID to paginate before (exclusive).
    Returns inferences with IDs before this one (earlier in time).
    Cannot be used together with `after` or `offset`.
    """
    episode_id: str | None = None
    """
    Optional episode ID to filter inferences by.
    If provided, only inferences from this episode will be returned.
    """
    filter: InferenceFilter | None = None
    """
    **Deprecated:** Use `filters` instead. This field will be removed in a future release.
    """
    filters: InferenceFilter | None = None
    """
    Optional filter to apply when querying inferences.
    Supports filtering by metrics, tags, time, and logical combinations (AND/OR/NOT).
    """
    function_name: str | None = None
    """
    Optional function name to filter inferences by.
    If provided, only inferences from this function will be returned.
    """
    limit: int | None = None
    """
    The maximum number of inferences to return.
    Defaults to 20.
    """
    offset: int | None = None
    """
    The number of inferences to skip before starting to return results.
    Defaults to 0.
    """
    order_by: list[OrderBy] | None = None
    """
    Optional ordering criteria for the results.
    Supports multiple sort criteria (e.g., sort by timestamp then by metric).
    """
    output_source: InferenceOutputSource | None = "inference"
    """
    Source of the inference output. Determines whether to return the original
    inference output or demonstration feedback (manually-curated output) if available.
    Defaults to `Inference` if not specified.
    """
    search_query_experimental: str | None = None
    """
    Text query to filter. Case-insensitive substring search over the inferences' input and output.

    THIS FEATURE IS EXPERIMENTAL, and we may change or remove it at any time.
    We recommend against depending on this feature for critical use cases.

    Important limitations:
    - This requires an exact substring match; we do not tokenize this query string.
    - This doesn't search for any content in the template itself.
    - Quality is based on term frequency > 0, without any relevance scoring.
    - There are no performance guarantees (it's best effort only). Today, with no other
      filters, it will perform a full table scan, which may be extremely slow depending
      on the data volume.
    """
    variant_name: str | None = None
    """
    Optional variant name to filter inferences by.
    If provided, only inferences from this variant will be returned.
    """
