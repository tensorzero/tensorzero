"""
Auto-generated Python dataclasses from JSON schemas.

This file is generated from JSON schemas in the tensorzero-core crate.
Do not edit this file manually - it will be overwritten.

Generated from schemas in: tensorzero-core/schemas/

To regenerate, run:
    python generate_schema_types.py
"""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Literal
from uuid import UUID



# generated by datamodel-codegen:
#   filename:  Arguments.json




@dataclass
class Arguments:
    pass

# generated by datamodel-codegen:
#   filename:  ContentBlockChatOutput.json




@dataclass
class ContentBlockChatOutput1:
    text: str
    type: Literal['text']


@dataclass
class ContentBlockChatOutput2:
    data: Any
    type: Literal['unknown']
    model_provider_name: str | None = None


@dataclass
class InferenceResponseToolCall:
    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """
    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """


@dataclass
class ThoughtSummaryBlock1:
    text: str
    type: Literal['summary_text']


ThoughtSummaryBlock = ThoughtSummaryBlock1


@dataclass
class Thought:
    text: str | None = None
    signature: str | None = None
    """
    An optional signature - currently, this is only used with Anthropic,
    and is ignored by other providers.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    field_internal_provider_type: str | None = None
    """
    When set, this 'Thought' block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """


ContentBlockChatOutput = (
    ContentBlockChatOutput1
    | InferenceResponseToolCall
    | Thought
    | ContentBlockChatOutput2
)

# generated by datamodel-codegen:
#   filename:  DatapointMetadataUpdate.json




@dataclass
class DatapointMetadataUpdate:
    name: str | None = None
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """

# generated by datamodel-codegen:
#   filename:  DynamicToolParams.json




@dataclass
class Tool:
    description: str
    parameters: Any
    name: str
    strict: bool | None = False


class ToolChoice1(Enum):
    none = 'none'
    auto = 'auto'
    required = 'required'


@dataclass
class ToolChoice2:
    specific: str


ToolChoice = ToolChoice1 | ToolChoice2


@dataclass
class ProviderToolScope1:
    model_name: str
    model_provider_name: str


ProviderToolScope = ProviderToolScope1 | None


@dataclass
class ProviderTool:
    tool: Any
    scope: ProviderToolScope | None = None


@dataclass
class DynamicToolParams:
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations (not persisted to database)
    """

# generated by datamodel-codegen:
#   filename:  Input.json




@dataclass
class Arguments:
    pass


class Role(Enum):
    user = 'user'
    assistant = 'assistant'


@dataclass
class InputMessageContent1:
    text: str
    type: Literal['text']


@dataclass
class InputMessageContent2:
    name: str
    arguments: Arguments
    type: Literal['template']


@dataclass
class InputMessageContent3:
    name: str
    result: str
    id: str
    type: Literal['tool_result']


@dataclass
class InputMessageContent4:
    value: str
    type: Literal['raw_text']


@dataclass
class InputMessageContent5:
    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    type: Literal['unknown']
    model_provider_name: str | None = None
    """
    A fully-qualified name specifying when this content block should
    be included in the model provider input.
    """


@dataclass
class ToolCall:
    id: str
    name: str
    arguments: str


@dataclass
class InferenceResponseToolCall:
    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """
    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """


ToolCallWrapper = ToolCall | InferenceResponseToolCall


@dataclass
class ThoughtSummaryBlock1:
    text: str
    type: Literal['summary_text']


ThoughtSummaryBlock = ThoughtSummaryBlock1


@dataclass
class Thought:
    text: str | None = None
    signature: str | None = None
    """
    An optional signature - currently, this is only used with Anthropic,
    and is ignored by other providers.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    field_internal_provider_type: str | None = None
    """
    When set, this 'Thought' block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """


class Detail(Enum):
    low = 'low'
    high = 'high'
    auto = 'auto'


@dataclass
class UrlFile:
    url: str
    mime_type: str | None = None
    detail: Detail | None = None


@dataclass
class Base64File:
    mime_type: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStoragePointer:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageFile:
    mime_type: str
    storage_path: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageError:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None
    error: str | None = None


File = (
    UrlFile | Base64File | ObjectStoragePointer | ObjectStorageFile | ObjectStorageError
)


System = str | Arguments


InputMessageContent = (
    InputMessageContent1
    | InputMessageContent2
    | ToolCallWrapper
    | InputMessageContent3
    | InputMessageContent4
    | Thought
    | File
    | InputMessageContent5
)


@dataclass
class InputMessage:
    role: Role
    content: list[InputMessageContent]


@dataclass
class Input:
    system: System | None = None
    messages: list[InputMessage] | None = field(default_factory=lambda: [])

# generated by datamodel-codegen:
#   filename:  JsonDatapointOutputUpdate.json




@dataclass
class JsonDatapointOutputUpdate:
    raw: str
    """
    The raw output of the datapoint. For valid JSON outputs, this should be a JSON-serialized string.
    """

# generated by datamodel-codegen:
#   filename:  System.json




@dataclass
class Arguments:
    pass


System = str | Arguments

# generated by datamodel-codegen:
#   filename:  ToolCallChunk.json




@dataclass
class ToolCallChunk:
    id: str
    raw_arguments: str
    raw_name: str | None = None

# generated by datamodel-codegen:
#   filename:  ToolChoice.json




class ToolChoice1(Enum):
    none = 'none'
    auto = 'auto'
    required = 'required'


@dataclass
class ToolChoice2:
    specific: str


ToolChoice = ToolChoice1 | ToolChoice2

# generated by datamodel-codegen:
#   filename:  UpdateChatDatapointRequest.json




@dataclass
class Arguments:
    pass


class Role(Enum):
    user = 'user'
    assistant = 'assistant'


@dataclass
class InputMessageContent1:
    text: str
    type: Literal['text']


@dataclass
class InputMessageContent2:
    name: str
    arguments: Arguments
    type: Literal['template']


@dataclass
class InputMessageContent3:
    name: str
    result: str
    id: str
    type: Literal['tool_result']


@dataclass
class InputMessageContent4:
    value: str
    type: Literal['raw_text']


@dataclass
class InputMessageContent5:
    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    type: Literal['unknown']
    model_provider_name: str | None = None
    """
    A fully-qualified name specifying when this content block should
    be included in the model provider input.
    """


@dataclass
class ToolCall:
    id: str
    name: str
    arguments: str


@dataclass
class InferenceResponseToolCall:
    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """
    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """


ToolCallWrapper = ToolCall | InferenceResponseToolCall


@dataclass
class ThoughtSummaryBlock1:
    text: str
    type: Literal['summary_text']


ThoughtSummaryBlock = ThoughtSummaryBlock1


@dataclass
class Thought:
    text: str | None = None
    signature: str | None = None
    """
    An optional signature - currently, this is only used with Anthropic,
    and is ignored by other providers.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    field_internal_provider_type: str | None = None
    """
    When set, this 'Thought' block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """


class Detail(Enum):
    low = 'low'
    high = 'high'
    auto = 'auto'


@dataclass
class UrlFile:
    url: str
    mime_type: str | None = None
    detail: Detail | None = None


@dataclass
class Base64File:
    mime_type: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStoragePointer:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageFile:
    mime_type: str
    storage_path: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageError:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None
    error: str | None = None


File = (
    UrlFile | Base64File | ObjectStoragePointer | ObjectStorageFile | ObjectStorageError
)


@dataclass
class ContentBlockChatOutput1:
    text: str
    type: Literal['text']


@dataclass
class ContentBlockChatOutput2:
    data: Any
    type: Literal['unknown']
    model_provider_name: str | None = None


ContentBlockChatOutput = (
    ContentBlockChatOutput1
    | InferenceResponseToolCall
    | Thought
    | ContentBlockChatOutput2
)


@dataclass
class Tool:
    description: str
    parameters: Any
    name: str
    strict: bool | None = False


class ToolChoice1(Enum):
    none = 'none'
    auto = 'auto'
    required = 'required'


@dataclass
class ToolChoice2:
    specific: str


ToolChoice = ToolChoice1 | ToolChoice2


@dataclass
class ProviderToolScope1:
    model_name: str
    model_provider_name: str


ProviderToolScope = ProviderToolScope1 | None


@dataclass
class DatapointMetadataUpdate:
    name: str | None = None
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """


System = str | Arguments


InputMessageContent = (
    InputMessageContent1
    | InputMessageContent2
    | ToolCallWrapper
    | InputMessageContent3
    | InputMessageContent4
    | Thought
    | File
    | InputMessageContent5
)


@dataclass
class ProviderTool:
    tool: Any
    scope: ProviderToolScope | None = None


@dataclass
class InputMessage:
    role: Role
    content: list[InputMessageContent]


@dataclass
class DynamicToolParams:
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations (not persisted to database)
    """


@dataclass
class Input:
    system: System | None = None
    messages: list[InputMessage] | None = field(default_factory=lambda: [])


@dataclass
class UpdateChatDatapointRequest:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    output: list[ContentBlockChatOutput] | None = None
    """
    Chat datapoint output. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will overwrite the existing output.
    """
    tool_params: DynamicToolParams | None = None
    """
    Datapoint tool parameters. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    Metadata fields. If omitted, it will be left unchanged.
    """

# generated by datamodel-codegen:
#   filename:  UpdateDatapointRequest.json




@dataclass
class Arguments:
    pass


class Role(Enum):
    user = 'user'
    assistant = 'assistant'


@dataclass
class InputMessageContent1:
    text: str
    type: Literal['text']


@dataclass
class InputMessageContent2:
    name: str
    arguments: Arguments
    type: Literal['template']


@dataclass
class InputMessageContent3:
    name: str
    result: str
    id: str
    type: Literal['tool_result']


@dataclass
class InputMessageContent4:
    value: str
    type: Literal['raw_text']


@dataclass
class InputMessageContent5:
    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    type: Literal['unknown']
    model_provider_name: str | None = None
    """
    A fully-qualified name specifying when this content block should
    be included in the model provider input.
    """


@dataclass
class ToolCall:
    id: str
    name: str
    arguments: str


@dataclass
class InferenceResponseToolCall:
    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """
    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """


ToolCallWrapper = ToolCall | InferenceResponseToolCall


@dataclass
class ThoughtSummaryBlock1:
    text: str
    type: Literal['summary_text']


ThoughtSummaryBlock = ThoughtSummaryBlock1


@dataclass
class Thought:
    text: str | None = None
    signature: str | None = None
    """
    An optional signature - currently, this is only used with Anthropic,
    and is ignored by other providers.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    field_internal_provider_type: str | None = None
    """
    When set, this 'Thought' block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """


class Detail(Enum):
    low = 'low'
    high = 'high'
    auto = 'auto'


@dataclass
class UrlFile:
    url: str
    mime_type: str | None = None
    detail: Detail | None = None


@dataclass
class Base64File:
    mime_type: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStoragePointer:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageFile:
    mime_type: str
    storage_path: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageError:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None
    error: str | None = None


File = (
    UrlFile | Base64File | ObjectStoragePointer | ObjectStorageFile | ObjectStorageError
)


@dataclass
class ContentBlockChatOutput1:
    text: str
    type: Literal['text']


@dataclass
class ContentBlockChatOutput2:
    data: Any
    type: Literal['unknown']
    model_provider_name: str | None = None


ContentBlockChatOutput = (
    ContentBlockChatOutput1
    | InferenceResponseToolCall
    | Thought
    | ContentBlockChatOutput2
)


@dataclass
class Tool:
    description: str
    parameters: Any
    name: str
    strict: bool | None = False


class ToolChoice1(Enum):
    none = 'none'
    auto = 'auto'
    required = 'required'


@dataclass
class ToolChoice2:
    specific: str


ToolChoice = ToolChoice1 | ToolChoice2


@dataclass
class ProviderToolScope1:
    model_name: str
    model_provider_name: str


ProviderToolScope = ProviderToolScope1 | None


@dataclass
class DatapointMetadataUpdate:
    name: str | None = None
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """


@dataclass
class JsonDatapointOutputUpdate:
    raw: str
    """
    The raw output of the datapoint. For valid JSON outputs, this should be a JSON-serialized string.
    """


System = str | Arguments


InputMessageContent = (
    InputMessageContent1
    | InputMessageContent2
    | ToolCallWrapper
    | InputMessageContent3
    | InputMessageContent4
    | Thought
    | File
    | InputMessageContent5
)


@dataclass
class ProviderTool:
    tool: Any
    scope: ProviderToolScope | None = None


@dataclass
class InputMessage:
    role: Role
    content: list[InputMessageContent]


@dataclass
class DynamicToolParams:
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations (not persisted to database)
    """


@dataclass
class Input:
    system: System | None = None
    messages: list[InputMessage] | None = field(default_factory=lambda: [])


@dataclass
class UpdateChatDatapointRequest:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    output: list[ContentBlockChatOutput] | None = None
    """
    Chat datapoint output. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will overwrite the existing output.
    """
    tool_params: DynamicToolParams | None = None
    """
    Datapoint tool parameters. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    Metadata fields. If omitted, it will be left unchanged.
    """


@dataclass
class UpdateJsonDatapointRequest:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    output: JsonDatapointOutputUpdate | None = None
    """
    JSON datapoint output. If omitted, it will be left unchanged. If `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    This will be parsed and validated against output_schema, and valid `raw` values will be parsed and stored as `parsed`. Invalid `raw` values will
    also be stored, because we allow invalid outputs in datapoints by design.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    If not provided, the function's output schema will be used.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    Metadata fields. If omitted, it will be left unchanged.
    """


UpdateDatapointRequest = UpdateChatDatapointRequest | UpdateJsonDatapointRequest

# generated by datamodel-codegen:
#   filename:  UpdateDatapointsRequest.json




@dataclass
class Arguments:
    pass


class Role(Enum):
    user = 'user'
    assistant = 'assistant'


@dataclass
class InputMessageContent1:
    text: str
    type: Literal['text']


@dataclass
class InputMessageContent2:
    name: str
    arguments: Arguments
    type: Literal['template']


@dataclass
class InputMessageContent3:
    name: str
    result: str
    id: str
    type: Literal['tool_result']


@dataclass
class InputMessageContent4:
    value: str
    type: Literal['raw_text']


@dataclass
class InputMessageContent5:
    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    type: Literal['unknown']
    model_provider_name: str | None = None
    """
    A fully-qualified name specifying when this content block should
    be included in the model provider input.
    """


@dataclass
class ToolCall:
    id: str
    name: str
    arguments: str


@dataclass
class InferenceResponseToolCall:
    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """
    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """


ToolCallWrapper = ToolCall | InferenceResponseToolCall


@dataclass
class ThoughtSummaryBlock1:
    text: str
    type: Literal['summary_text']


ThoughtSummaryBlock = ThoughtSummaryBlock1


@dataclass
class Thought:
    text: str | None = None
    signature: str | None = None
    """
    An optional signature - currently, this is only used with Anthropic,
    and is ignored by other providers.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    field_internal_provider_type: str | None = None
    """
    When set, this 'Thought' block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """


class Detail(Enum):
    low = 'low'
    high = 'high'
    auto = 'auto'


@dataclass
class UrlFile:
    url: str
    mime_type: str | None = None
    detail: Detail | None = None


@dataclass
class Base64File:
    mime_type: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStoragePointer:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageFile:
    mime_type: str
    storage_path: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageError:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None
    error: str | None = None


File = (
    UrlFile | Base64File | ObjectStoragePointer | ObjectStorageFile | ObjectStorageError
)


@dataclass
class ContentBlockChatOutput1:
    text: str
    type: Literal['text']


@dataclass
class ContentBlockChatOutput2:
    data: Any
    type: Literal['unknown']
    model_provider_name: str | None = None


ContentBlockChatOutput = (
    ContentBlockChatOutput1
    | InferenceResponseToolCall
    | Thought
    | ContentBlockChatOutput2
)


@dataclass
class Tool:
    description: str
    parameters: Any
    name: str
    strict: bool | None = False


class ToolChoice1(Enum):
    none = 'none'
    auto = 'auto'
    required = 'required'


@dataclass
class ToolChoice2:
    specific: str


ToolChoice = ToolChoice1 | ToolChoice2


@dataclass
class ProviderToolScope1:
    model_name: str
    model_provider_name: str


ProviderToolScope = ProviderToolScope1 | None


@dataclass
class DatapointMetadataUpdate:
    name: str | None = None
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """


@dataclass
class JsonDatapointOutputUpdate:
    raw: str
    """
    The raw output of the datapoint. For valid JSON outputs, this should be a JSON-serialized string.
    """


System = str | Arguments


InputMessageContent = (
    InputMessageContent1
    | InputMessageContent2
    | ToolCallWrapper
    | InputMessageContent3
    | InputMessageContent4
    | Thought
    | File
    | InputMessageContent5
)


@dataclass
class ProviderTool:
    tool: Any
    scope: ProviderToolScope | None = None


@dataclass
class InputMessage:
    role: Role
    content: list[InputMessageContent]


@dataclass
class DynamicToolParams:
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations (not persisted to database)
    """


@dataclass
class Input:
    system: System | None = None
    messages: list[InputMessage] | None = field(default_factory=lambda: [])


@dataclass
class UpdateChatDatapointRequest:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    output: list[ContentBlockChatOutput] | None = None
    """
    Chat datapoint output. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will overwrite the existing output.
    """
    tool_params: DynamicToolParams | None = None
    """
    Datapoint tool parameters. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    Metadata fields. If omitted, it will be left unchanged.
    """


@dataclass
class UpdateJsonDatapointRequest:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    output: JsonDatapointOutputUpdate | None = None
    """
    JSON datapoint output. If omitted, it will be left unchanged. If `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    This will be parsed and validated against output_schema, and valid `raw` values will be parsed and stored as `parsed`. Invalid `raw` values will
    also be stored, because we allow invalid outputs in datapoints by design.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    If not provided, the function's output schema will be used.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    Metadata fields. If omitted, it will be left unchanged.
    """


UpdateDatapointRequest = UpdateChatDatapointRequest | UpdateJsonDatapointRequest


@dataclass
class UpdateDatapointsRequest:
    datapoints: list[UpdateDatapointRequest]
    """
    The datapoints to update.
    """

# generated by datamodel-codegen:
#   filename:  UpdateDatapointsResponse.json




@dataclass
class UpdateDatapointsResponse:
    ids: list[str]
    """
    The IDs of the datapoints that were updated.
    These are newly generated IDs for UpdateDatapoint requests, and they are the same IDs for UpdateDatapointMetadata requests.
    """

# generated by datamodel-codegen:
#   filename:  UpdateJsonDatapointRequest.json




@dataclass
class Arguments:
    pass


class Role(Enum):
    user = 'user'
    assistant = 'assistant'


@dataclass
class InputMessageContent1:
    text: str
    type: Literal['text']


@dataclass
class InputMessageContent2:
    name: str
    arguments: Arguments
    type: Literal['template']


@dataclass
class InputMessageContent3:
    name: str
    result: str
    id: str
    type: Literal['tool_result']


@dataclass
class InputMessageContent4:
    value: str
    type: Literal['raw_text']


@dataclass
class InputMessageContent5:
    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    type: Literal['unknown']
    model_provider_name: str | None = None
    """
    A fully-qualified name specifying when this content block should
    be included in the model provider input.
    """


@dataclass
class ToolCall:
    id: str
    name: str
    arguments: str


@dataclass
class InferenceResponseToolCall:
    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """
    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """


ToolCallWrapper = ToolCall | InferenceResponseToolCall


@dataclass
class ThoughtSummaryBlock1:
    text: str
    type: Literal['summary_text']


ThoughtSummaryBlock = ThoughtSummaryBlock1


@dataclass
class Thought:
    text: str | None = None
    signature: str | None = None
    """
    An optional signature - currently, this is only used with Anthropic,
    and is ignored by other providers.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    field_internal_provider_type: str | None = None
    """
    When set, this 'Thought' block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """


class Detail(Enum):
    low = 'low'
    high = 'high'
    auto = 'auto'


@dataclass
class UrlFile:
    url: str
    mime_type: str | None = None
    detail: Detail | None = None


@dataclass
class Base64File:
    mime_type: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStoragePointer:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageFile:
    mime_type: str
    storage_path: str
    data: str
    source_url: str | None = None
    detail: Detail | None = None


@dataclass
class ObjectStorageError:
    mime_type: str
    storage_path: str
    source_url: str | None = None
    detail: Detail | None = None
    error: str | None = None


File = (
    UrlFile | Base64File | ObjectStoragePointer | ObjectStorageFile | ObjectStorageError
)


@dataclass
class JsonDatapointOutputUpdate:
    raw: str
    """
    The raw output of the datapoint. For valid JSON outputs, this should be a JSON-serialized string.
    """


@dataclass
class DatapointMetadataUpdate:
    name: str | None = None
    """
    Datapoint name. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    """


System = str | Arguments


InputMessageContent = (
    InputMessageContent1
    | InputMessageContent2
    | ToolCallWrapper
    | InputMessageContent3
    | InputMessageContent4
    | Thought
    | File
    | InputMessageContent5
)


@dataclass
class InputMessage:
    role: Role
    content: list[InputMessageContent]


@dataclass
class Input:
    system: System | None = None
    messages: list[InputMessage] | None = field(default_factory=lambda: [])


@dataclass
class UpdateJsonDatapointRequest:
    id: str
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    output: JsonDatapointOutputUpdate | None = None
    """
    JSON datapoint output. If omitted, it will be left unchanged. If `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    This will be parsed and validated against output_schema, and valid `raw` values will be parsed and stored as `parsed`. Invalid `raw` values will
    also be stored, because we allow invalid outputs in datapoints by design.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    If not provided, the function's output schema will be used.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    Metadata fields. If omitted, it will be left unchanged.
    """

