"""
Auto-generated Python dataclasses from JSON schemas.

This file is generated from JSON schemas in the tensorzero-core crate.
Do not edit this file manually - it will be overwritten.

Generated from schemas in: tensorzero-core/clients/schemas/

To regenerate, run:
    python generate_schema_types.py
"""

from __future__ import annotations

from typing import Any, Literal
from uuid import UUID

from pydantic import BaseModel, ConfigDict, Field, RootModel, conint


class Model(RootModel[Any]):
    root: Any


class BooleanMetricFilter(BaseModel):
    metric_name: str
    value: bool


class ContentBlockChatOutputText(BaseModel):
    """
    InputMessages are validated against the input schema of the Function
    and then templated and transformed into RequestMessages for a particular Variant.
    They might contain tool calls or tool results along with text.
    The abstraction we use to represent this is ContentBlock, which is a union of Text, ToolCall, and ToolResult.
    ContentBlocks are collected into RequestMessages.
    These RequestMessages are collected into a ModelInferenceRequest,
    which should contain all information needed by a ModelProvider to perform the
    inference that is called for.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    text: str
    type: Literal["text"]


class CreateDatapointsFromInferenceOutputSource(RootModel[str]):
    root: str = Field(..., title="CreateDatapointsFromInferenceOutputSource")
    """
    Specifies the source of the output for the datapoint when creating datapoints from inferences.
    - `None`: Do not include any output in the datapoint.
    - `Inference`: Include the original inference output in the datapoint.
    - `Demonstration`: Include the latest demonstration feedback as output in the datapoint.
    """


class CreateDatapointsFromInferenceRequestParamsInferenceIds(BaseModel):
    """
    Create datapoints from specific inference IDs.
    """

    inference_ids: list[UUID]
    """
    The inference IDs to create datapoints from.
    """
    type: Literal["inference_ids"]


class CreateDatapointsResponse(BaseModel):
    """
    Response from creating datapoints.
    """

    ids: list[UUID]
    """
    The IDs of the newly-generated datapoints.
    """


class DatapointMetadataUpdate(BaseModel):
    """
    A request to update the metadata of a datapoint.
    """

    name: str | None = None
    """
    Datapoint name.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared. If specified as a value, it will
    be set to the provided value.
    """


class DeleteDatapointsRequest(BaseModel):
    """
    Request to delete datapoints from a dataset.
    """

    ids: list[UUID]
    """
    The IDs of the datapoints to delete.
    """


class DeleteDatapointsResponse(BaseModel):
    """
    Response containing the number of deleted datapoints.
    """

    num_deleted_datapoints: conint(ge=0)
    """
    The number of deleted datapoints.
    """


class Detail(RootModel[Literal["low", "high", "auto"]]):
    root: Literal["low", "high", "auto"] = Field(..., title="Detail")
    """
    Detail level for input images (affects fidelity and token cost)
    """


class ProviderExtraBody(BaseModel):
    """
    DEPRECATED: Use `ModelProvider` instead.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    model_provider_name: str
    """
    A fully-qualified model provider name in your configuration (e.g. `tensorzero::model_name::my_model::provider_name::my_provider`)
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    value: Any
    """
    The value to set the field to
    """


class ProviderExtraBodyDelete(BaseModel):
    """
    DEPRECATED: Use `ModelProviderDelete` instead.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    delete: Literal[True]
    """
    Set to true to remove the field from the model provider request's body
    """
    model_provider_name: str
    """
    A fully-qualified model provider name in your configuration (e.g. `tensorzero::model_name::my_model::provider_name::my_provider`)
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """


class VariantExtraBody(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    value: Any
    """
    The value to set the field to
    """
    variant_name: str
    """
    A variant name in your configuration (e.g. `my_variant`)
    """


class VariantExtraBodyDelete(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    delete: Literal[True]
    """
    Set to true to remove the field from the model provider request's body
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    variant_name: str
    """
    A variant name in your configuration (e.g. `my_variant`)
    """


class ModelProviderExtraBody(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model_name: str
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`)
    """
    value: Any
    """
    The value to set the field to
    """


class ModelProviderExtraBodyDelete(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    delete: Literal[True]
    """
    Set to true to remove the field from the model provider request's body
    """
    model_name: str
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`)
    """


class AlwaysExtraBody(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """
    value: Any
    """
    The value to set the field to
    """


class AlwaysExtraBodyDelete(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    delete: Literal[True]
    """
    Set to true to remove the field from the model provider request's body
    """
    pointer: str
    """
    A JSON Pointer to the field to update (e.g. `/enable_agi`)
    """


class ExtraBody(
    RootModel[
        ProviderExtraBody
        | ProviderExtraBodyDelete
        | VariantExtraBody
        | VariantExtraBodyDelete
        | ModelProviderExtraBody
        | ModelProviderExtraBodyDelete
        | AlwaysExtraBody
        | AlwaysExtraBodyDelete
    ]
):
    root: (
        ProviderExtraBody
        | ProviderExtraBodyDelete
        | VariantExtraBody
        | VariantExtraBodyDelete
        | ModelProviderExtraBody
        | ModelProviderExtraBodyDelete
        | AlwaysExtraBody
        | AlwaysExtraBodyDelete
    ) = Field(..., title="ExtraBody")


class ExtraBodyReplacementKindValue(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    value: Any


class ExtraBodyReplacementKind(RootModel[Literal["delete"] | ExtraBodyReplacementKindValue]):
    root: Literal["delete"] | ExtraBodyReplacementKindValue = Field(..., title="ExtraBodyReplacementKind")


class ProviderExtraHeader(BaseModel):
    """
    DEPRECATED: Use `ModelProvider` instead.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    model_provider_name: str
    """
    A fully-qualified model provider name in your configuration (e.g. `tensorzero::model_name::my_model::provider_name::my_provider`)
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    value: str
    """
    The value of the HTTP header (e.g. `feature1,feature2,feature3`)
    """


class ProviderExtraHeaderDelete(BaseModel):
    """
    DEPRECATED: Use `ModelProviderDelete` instead.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    delete: Literal[True]
    """
    Set to true to remove the header from the model provider request
    """
    model_provider_name: str
    """
    A fully-qualified model provider name in your configuration (e.g. `tensorzero::model_name::my_model::provider_name::my_provider`)
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """


class VariantExtraHeader(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    value: str
    """
    The value of the HTTP header (e.g. `feature1,feature2,feature3`)
    """
    variant_name: str
    """
    A variant name in your configuration (e.g. `my_variant`)
    """


class VariantExtraHeaderDelete(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    delete: Literal[True]
    """
    Set to true to remove the header from the model provider request
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    variant_name: str
    """
    A variant name in your configuration (e.g. `my_variant`)
    """


class ModelProviderExtraHeader(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    model_name: str
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`).
    """
    value: str
    """
    The value of the HTTP header (e.g. `feature1,feature2,feature3`)
    """


class ModelProviderExtraHeaderDelete(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    delete: Literal[True]
    """
    Set to true to remove the header from the model provider request
    """
    model_name: str
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`)
    """


class AlwaysExtraHeader(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """
    value: str
    """
    The value of the HTTP header (e.g. `feature1,feature2,feature3`)
    """


class AlwaysExtraHeaderDelete(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    delete: Literal[True]
    """
    Set to true to remove the header from the model provider request
    """
    name: str
    """
    The name of the HTTP header (e.g. `anthropic-beta`)
    """


class ExtraHeader(
    RootModel[
        ProviderExtraHeader
        | ProviderExtraHeaderDelete
        | VariantExtraHeader
        | VariantExtraHeaderDelete
        | ModelProviderExtraHeader
        | ModelProviderExtraHeaderDelete
        | AlwaysExtraHeader
        | AlwaysExtraHeaderDelete
    ]
):
    root: (
        ProviderExtraHeader
        | ProviderExtraHeaderDelete
        | VariantExtraHeader
        | VariantExtraHeaderDelete
        | ModelProviderExtraHeader
        | ModelProviderExtraHeaderDelete
        | AlwaysExtraHeader
        | AlwaysExtraHeaderDelete
    ) = Field(..., title="ExtraHeader")


class FloatComparisonOperator(RootModel[Literal["<", "<=", "=", ">", ">=", "!="]]):
    root: Literal["<", "<=", "=", ">", ">=", "!="]
    """
    Comparison operators for float metrics.
    """


class FloatMetricFilter(BaseModel):
    comparison_operator: FloatComparisonOperator
    metric_name: str
    value: float


class GetDatapointsRequest(BaseModel):
    """
    Request to get specific datapoints by their IDs.
    Used by the `POST /v1/datasets/{dataset_name}/get_datapoints` endpoint.
    """

    ids: list[UUID]
    """
    The IDs of the datapoints to retrieve. Required.
    """


class InferenceFilterFloatMetric(FloatMetricFilter):
    """
    Filter by the value of a float metric
    """

    type: Literal["float_metric"]


class InferenceFilterBooleanMetric(BooleanMetricFilter):
    """
    Filter by the value of a boolean metric
    """

    type: Literal["boolean_metric"]


class InferenceOutputSource(RootModel[str]):
    root: str
    """
    Source of an inference output when querying inferences. Users can choose this because there may be
    demonstration feedback (manually-curated output) for the inference that should be preferred.
    """


class InferenceResponseToolCall(BaseModel):
    """
    An InferenceResponseToolCall is a request by a model to call a Tool
    in the form that we return to the client / ClickHouse
    This includes some synactic sugar (parsing / validation of the tool arguments)
    in the `arguments` field and the name in the `name` field.
    We support looping this back through the TensorZero inference API via the ToolCallWrapper
    """

    arguments: Any | None = None
    """
    The arguments of the tool to call, validated against tool configs. If not present, it means the tool call arguments were invalid.
    """
    id: str
    """
    A Tool Call ID to match up with tool call responses. See #4058.
    """
    name: str | None = None
    """
    The name of the tool to call, validated against tool configs. If not present, it means the tool call was invalid.
    """
    raw_arguments: str
    """
    The raw arguments JSON string of the tool to call, as generated by the model.
    """
    raw_name: str
    """
    The name of the tool to call, as generated by the model.
    """


class InputMessageContentText(BaseModel):
    """
    InputMessages are validated against the input schema of the Function
    and then templated and transformed into RequestMessages for a particular Variant.
    They might contain tool calls or tool results along with text.
    The abstraction we use to represent this is ContentBlock, which is a union of Text, ToolCall, and ToolResult.
    ContentBlocks are collected into RequestMessages.
    These RequestMessages are collected into a ModelInferenceRequest,
    which should contain all information needed by a ModelProvider to perform the
    inference that is called for.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    text: str
    type: Literal["text"]


class InputMessageContentTemplate(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    arguments: dict[str, Any]
    name: str
    type: Literal["template"]


class InputMessageContentToolResult(BaseModel):
    """
    A ToolResult is the outcome of a ToolCall, which we may want to present back to the model
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    id: str
    name: str
    result: str
    type: Literal["tool_result"]


class InputMessageContentRawText(BaseModel):
    """
    Struct that represents raw text content that should be passed directly to the model
    without any template processing or validation
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    type: Literal["raw_text"]
    value: str


class JsonDatapointOutputUpdate(BaseModel):
    """
    A request to update the output of a JSON datapoint.

    We intentionally only accept the `raw` field, because JSON datapoints can contain invalid or malformed JSON for eval purposes.
    """

    raw: str | None = None
    """
    The raw output of the datapoint. For valid JSON outputs, this should be a JSON-serialized string.

    This will be parsed and validated against the datapoint's `output_schema`. Valid `raw` values will be parsed and stored as `parsed`, and
    invalid `raw` values will be stored as-is, because we allow invalid outputs in datapoints by design.
    """


class JsonInferenceOutput(BaseModel):
    parsed: Any | None = None
    """
    This is never omitted from the response even if it's None.
    """
    raw: str | None = None
    """
    This is never omitted from the response even if it's None. A `null` value indicates no output from the model.
    It's rare and unexpected from the model, but it's possible.
    """


class OpenAICustomToolFormatText(BaseModel):
    type: Literal["text"]


class OpenAIGrammarSyntax(RootModel[Literal["lark", "regex"]]):
    root: Literal["lark", "regex"]


class OrderDirection(RootModel[Literal["ascending", "descending"]]):
    root: Literal["ascending", "descending"]
    """
    The ordering direction.
    """


class ProviderToolScopeModelProvider(BaseModel):
    model_name: str
    provider_name: str | None = None


class RawText(BaseModel):
    """
    Struct that represents raw text content that should be passed directly to the model
    without any template processing or validation
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    value: str


class Role(RootModel[Literal["user", "assistant"]]):
    root: Literal["user", "assistant"] = Field(..., title="Role")


class StorageKindS3Compatible(BaseModel):
    """
    Configuration for the object storage backend
    Currently, we only support S3-compatible object storage and local filesystem storage
    We test against Amazon S3, GCS, Cloudflare R2, and Minio
    """

    allow_http: bool | None = None
    bucket_name: str | None = None
    endpoint: str | None = None
    prefix: str | None = ""
    """
    An extra prefix to prepend to the object key.
    This is only enabled in e2e tests, to prevent clashes between concurrent test runs.
    """
    region: str | None = None
    type: Literal["s3_compatible"]


class StorageKindFilesystem(BaseModel):
    """
    Configuration for the object storage backend
    Currently, we only support S3-compatible object storage and local filesystem storage
    We test against Amazon S3, GCS, Cloudflare R2, and Minio
    """

    path: str
    type: Literal["filesystem"]


class StorageKindDisabled(BaseModel):
    """
    Configuration for the object storage backend
    Currently, we only support S3-compatible object storage and local filesystem storage
    We test against Amazon S3, GCS, Cloudflare R2, and Minio
    """

    type: Literal["disabled"]


class StorageKind(RootModel[StorageKindS3Compatible | StorageKindFilesystem | StorageKindDisabled]):
    root: StorageKindS3Compatible | StorageKindFilesystem | StorageKindDisabled = Field(..., title="StorageKind")
    """
    Configuration for the object storage backend
    Currently, we only support S3-compatible object storage and local filesystem storage
    We test against Amazon S3, GCS, Cloudflare R2, and Minio
    """


class StoragePath(BaseModel):
    """
    Path to a file in an object storage backend.
    This is part of the public API for `File`s. In particular, this is useful for roundtripping
    unresolved inputs from stored inferences or datapoints, without requiring clients to fetch
    file data first.
    """

    kind: StorageKind
    path: str


class StoredInputMessageContentText(BaseModel):
    """
    InputMessages are validated against the input schema of the Function
    and then templated and transformed into RequestMessages for a particular Variant.
    They might contain tool calls or tool results along with text.
    The abstraction we use to represent this is ContentBlock, which is a union of Text, ToolCall, and ToolResult.
    ContentBlocks are collected into RequestMessages.
    These RequestMessages are collected into a ModelInferenceRequest,
    which should contain all information needed by a ModelProvider to perform the
    inference that is called for.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    text: str
    type: Literal["text"]


class StoredInputMessageContentTemplate(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    arguments: dict[str, Any]
    name: str
    type: Literal["template"]


class StoredInputMessageContentToolResult(BaseModel):
    """
    A ToolResult is the outcome of a ToolCall, which we may want to present back to the model
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    id: str
    name: str
    result: str
    type: Literal["tool_result"]


class StoredInputMessageContentRawText(BaseModel):
    """
    Struct that represents raw text content that should be passed directly to the model
    without any template processing or validation
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    type: Literal["raw_text"]
    value: str


class StoredInputMessageContentFile(BaseModel):
    """
    A file stored in an object storage backend, without data.
    This struct can be stored in the database. It's used by `StoredFile` (`StoredInput`).
    Note: `File` supports both `ObjectStorageFilePointer` and `ObjectStorageFile`.
    """

    detail: Detail | None = None
    filename: str | None = None
    mime_type: str
    source_url: str | None = None
    storage_path: StoragePath
    type: Literal["file"]


class System(RootModel[str | dict[str, Any]]):
    root: str | dict[str, Any] = Field(..., title="System")


class TagComparisonOperator(RootModel[Literal["=", "!="]]):
    root: Literal["=", "!="]
    """
    Comparison operators for tag filters.
    """


class TagFilter(BaseModel):
    """
    Filter by tag key-value pair.
    """

    comparison_operator: TagComparisonOperator
    key: str
    value: str


class Template(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    arguments: dict[str, Any]
    name: str


class Text(BaseModel):
    """
    InputMessages are validated against the input schema of the Function
    and then templated and transformed into RequestMessages for a particular Variant.
    They might contain tool calls or tool results along with text.
    The abstraction we use to represent this is ContentBlock, which is a union of Text, ToolCall, and ToolResult.
    ContentBlocks are collected into RequestMessages.
    These RequestMessages are collected into a ModelInferenceRequest,
    which should contain all information needed by a ModelProvider to perform the
    inference that is called for.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    text: str


class ThoughtSummaryBlockSummaryText(BaseModel):
    text: str
    type: Literal["summary_text"]


class ThoughtSummaryBlock(RootModel[ThoughtSummaryBlockSummaryText]):
    root: ThoughtSummaryBlockSummaryText = Field(..., title="ThoughtSummaryBlock")


class TimeComparisonOperator(RootModel[Literal["<", "<=", "=", ">", ">=", "!="]]):
    root: Literal["<", "<=", "=", ">", ">=", "!="]
    """
    Comparison operators for timestamps.
    """


class TimeFilter(BaseModel):
    """
    Filter by timestamp.
    """

    comparison_operator: TimeComparisonOperator
    time: str


class FunctionTool(BaseModel):
    """
    `FunctionTool` is a particular kind of tool that relies
    on the client to execute a function on their side (a ToolCall content block)
    and return the result on the next turn (a ToolCallResult).
    Notably, we assume there is a JSON schema `parameters` that specifies the
    set of arguments that the tool will accept.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    description: str
    name: str
    parameters: Any
    strict: bool | None = False
    """
    `strict` here specifies that TensorZero should attempt to use any facilities
    available from the model provider to force the model to generate an accurate tool call,
    notably OpenAI's strict tool call mode (https://platform.openai.com/docs/guides/function-calling#strict-mode).
    This imposes additional restrictions on the JSON schema that may vary across providers
    so we allow it to be configurable.
    """
    type: Literal["function"]


class ToolCall(BaseModel):
    arguments: str
    id: str
    name: str


class ToolCallWrapper(RootModel[ToolCall | InferenceResponseToolCall]):
    root: ToolCall | InferenceResponseToolCall = Field(..., title="ToolCallWrapper")
    """
    `ToolCallWrapper` helps us disambiguate between `ToolCall` (no `raw_*`) and `InferenceResponseToolCall` (has `raw_*`).
    Typically tool calls come from previous inferences and are therefore outputs of TensorZero (`InferenceResponseToolCall`)
    but they may also be constructed client side or through the OpenAI endpoint `ToolCall` so we support both via this wrapper.
    """


class ToolChoiceSpecific(BaseModel):
    """
    Forces the LLM to call a specific tool. The String is the name of the tool.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    specific: str


class ToolChoice(RootModel[Literal["none", "auto", "required"] | ToolChoiceSpecific]):
    root: Literal["none", "auto", "required"] | ToolChoiceSpecific = Field(..., title="ToolChoice")
    """
    Most inference providers allow the user to force a tool to be used
    and even specify which tool to be used.

    This enum is used to denote this tool choice.
    """


class ToolResult(BaseModel):
    """
    A ToolResult is the outcome of a ToolCall, which we may want to present back to the model
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    id: str
    name: str
    result: str


class Unknown(BaseModel):
    """
    Struct that represents an unknown provider-specific content block.
    We pass this along as-is without any validation or transformation.
    """

    data: Any
    """
    The underlying content block to be passed to the model provider.
    """
    model_name: str | None = None
    """
    A model name in your configuration (e.g. `my_gpt_5`) or a short-hand model name (e.g. `openai::gpt-5`)
    """
    provider_name: str | None = None
    """
    A provider name for the model you specified (e.g. `my_openai`)
    """


class UpdateDatapointMetadataRequest(BaseModel):
    """
    A request to update the metadata of a single datapoint.
    """

    id: UUID
    """
    The ID of the datapoint to update. Required.
    """
    name: str | None = None
    """
    Datapoint name.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared. If specified as a value, it will
    be set to the provided value.
    """


class UpdateDatapointsMetadataRequest(BaseModel):
    """
    Request to update metadata for one or more datapoints in a dataset.
    Used by the `PATCH /v1/datasets/{dataset_id}/datapoints/metadata` endpoint.
    """

    datapoints: list[UpdateDatapointMetadataRequest]
    """
    The datapoints to update metadata for.
    """


class UpdateDatapointsResponse(BaseModel):
    """
    A response to a request to update one or more datapoints in a dataset.
    """

    ids: list[UUID]
    """
    The IDs of the datapoints that were updated.
    These are newly generated IDs for UpdateDatapoint requests, and they are the same IDs for UpdateDatapointMetadata requests.
    """


class UrlFile(BaseModel):
    """
    A file that can be located at a URL
    """

    detail: Detail | None = None
    filename: str | None = None
    mime_type: str | None = None
    url: str


class Base64File(BaseModel):
    """
    A file already encoded as base64
    """

    data: str
    detail: Detail | None = None
    filename: str | None = None
    mime_type: str
    source_url: str | None = None


class ContentBlockChatOutputToolCall(InferenceResponseToolCall):
    """
    Defines the types of content block that can come from a `chat` function
    """

    type: Literal["tool_call"]


class ContentBlockChatOutputUnknown(Unknown):
    """
    Defines the types of content block that can come from a `chat` function
    """

    type: Literal["unknown"]


class TagDatapointFilter(TagFilter):
    """
    Filter by tag key-value pair
    """

    type: Literal["tag"]


class TimeDatapointFilter(TimeFilter):
    """
    Filter by datapoint update time
    """

    type: Literal["time"]


class DatapointOrderByTimestamp(BaseModel):
    """
    Creation timestamp of the datapoint.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["timestamp"]


class DatapointOrderBySearchRelevance(BaseModel):
    """
    Relevance score of the search query in the input and output of the datapoint.
    Requires a search query (experimental). If it's not provided, we return an error.

    Current relevance metric is very rudimentary (just term frequency), but we plan
    to improve it in the future.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["search_relevance"]


class DatapointOrderBy(RootModel[DatapointOrderByTimestamp | DatapointOrderBySearchRelevance]):
    root: DatapointOrderByTimestamp | DatapointOrderBySearchRelevance = Field(..., title="DatapointOrderBy")
    """
    Order by clauses for querying datapoints.
    """


class FileUrlFile(UrlFile):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["url"]


class FileBase64(Base64File):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["base64"]


class GetInferencesRequest(BaseModel):
    """
    Request to get specific inferences by their IDs.
    Used by the `POST /v1/inferences/get_inferences` endpoint.
    """

    function_name: str | None = None
    """
    Optional function name to filter by.
    Including this improves query performance since `function_name` is the first column
    in the ClickHouse primary key.
    """
    ids: list[UUID]
    """
    The IDs of the inferences to retrieve. Required.
    """
    output_source: InferenceOutputSource
    """
    Source of the inference output.
    Determines whether to return the original inference output or demonstration feedback
    (manually-curated output) if available.
    """


class InferenceFilterTag(TagFilter):
    """
    Filter by tag key-value pair
    """

    type: Literal["tag"]


class InferenceFilterTime(TimeFilter):
    """
    Filter by the timestamp of an inference.
    """

    type: Literal["time"]


class InputMessageContentToolCall(BaseModel):
    type: Literal["tool_call"]


class InputMessageContentUnknown(Unknown):
    """
    An unknown content block type, used to allow passing provider-specific
    content blocks (e.g. Anthropic's `redacted_thinking`) in and out
    of TensorZero.
    The `data` field holds the original content block from the provider,
    without any validation or transformation by TensorZero.
    """

    type: Literal["unknown"]


class ObjectStorageError(BaseModel):
    """
    A file that we failed to read from object storage.
    This struct can NOT be stored in the database.
    """

    detail: Detail | None = None
    error: str | None = None
    filename: str | None = None
    mime_type: str
    source_url: str | None = None
    storage_path: StoragePath


class ObjectStorageFile(BaseModel):
    """
    A file stored in an object storage backend, with data.
    This struct can NOT be stored in the database.
    Note: `File` supports both `ObjectStorageFilePointer` and `ObjectStorageFile`.
    """

    data: str
    detail: Detail | None = None
    filename: str | None = None
    mime_type: str
    source_url: str | None = None
    storage_path: StoragePath


class ObjectStoragePointer(BaseModel):
    """
    A file stored in an object storage backend, without data.
    This struct can be stored in the database. It's used by `StoredFile` (`StoredInput`).
    Note: `File` supports both `ObjectStorageFilePointer` and `ObjectStorageFile`.
    """

    detail: Detail | None = None
    filename: str | None = None
    mime_type: str
    source_url: str | None = None
    storage_path: StoragePath


class OpenAIGrammarDefinition(BaseModel):
    definition: str
    syntax: OpenAIGrammarSyntax


class OrderByTimestamp(BaseModel):
    """
    Creation timestamp of the item.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["timestamp"]


class OrderByMetric(BaseModel):
    """
    Value of a metric.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["metric"]
    name: str
    """
    The name of the metric to order by.
    """


class OrderBySearchRelevance(BaseModel):
    """
    Relevance score of the search query in the input and output of the item.
    Requires a search query (experimental). If it's not provided, we return an error.

    Current relevance metric is very rudimentary (just term frequency), but we plan
    to improve it in the future.
    """

    direction: OrderDirection
    """
    The ordering direction.
    """
    by: Literal["search_relevance"]


class OrderBy(RootModel[OrderByTimestamp | OrderByMetric | OrderBySearchRelevance]):
    root: OrderByTimestamp | OrderByMetric | OrderBySearchRelevance
    """
    Order by clauses for querying inferences.
    """


class ProviderToolScope(RootModel[ProviderToolScopeModelProvider | None]):
    root: ProviderToolScopeModelProvider | None = Field(..., title="ProviderToolScope")


class StoredInputMessageContentToolCall(ToolCall):
    type: Literal["tool_call"]


class StoredInputMessageContentUnknown(Unknown):
    type: Literal["unknown"]


class Thought(BaseModel):
    """
    Struct that represents a model's reasoning
    """

    field_internal_provider_type: str | None = Field(None, alias="_internal_provider_type")
    """
    Deprecated name for `provider_type` field, will be removed in 2026.2+.
    """
    provider_type: str | None = None
    """
    When set, this `Thought` block will only be used for providers
    matching this type (e.g. `anthropic`). Other providers will emit
    a warning and discard the block.
    """
    signature: str | None = None
    """
    An optional signature - currently, this is only used with Anthropic,
    and is ignored by other providers.
    """
    summary: list[ThoughtSummaryBlock] | None = None
    text: str | None = None


class ContentBlockChatOutputThought(Thought):
    """
    Defines the types of content block that can come from a `chat` function
    """

    type: Literal["thought"]


class ContentBlockChatOutput(
    RootModel[
        ContentBlockChatOutputText
        | ContentBlockChatOutputToolCall
        | ContentBlockChatOutputThought
        | ContentBlockChatOutputUnknown
    ]
):
    root: (
        ContentBlockChatOutputText
        | ContentBlockChatOutputToolCall
        | ContentBlockChatOutputThought
        | ContentBlockChatOutputUnknown
    ) = Field(..., title="ContentBlockChatOutput")
    """
    Defines the types of content block that can come from a `chat` function
    """


class FileObjectStoragePointer(ObjectStoragePointer):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["object_storage_pointer"]


class FileObjectStorage(ObjectStorageFile):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["object_storage"]


class FileObjectStorageError(ObjectStorageError):
    """
    A file for an inference or a datapoint.
    """

    file_type: Literal["object_storage_error"]


class File(RootModel[FileUrlFile | FileBase64 | FileObjectStoragePointer | FileObjectStorage | FileObjectStorageError]):
    root: FileUrlFile | FileBase64 | FileObjectStoragePointer | FileObjectStorage | FileObjectStorageError = Field(
        ..., title="File"
    )
    """
    A file for an inference or a datapoint.
    """


class InputMessageContentThought(Thought):
    type: Literal["thought"]


class InputMessageContentFile(BaseModel):
    type: Literal["file"]


class InputMessageContent(
    RootModel[
        InputMessageContentText
        | InputMessageContentTemplate
        | InputMessageContentToolCall
        | InputMessageContentToolResult
        | InputMessageContentRawText
        | InputMessageContentThought
        | InputMessageContentFile
        | InputMessageContentUnknown
    ]
):
    root: (
        InputMessageContentText
        | InputMessageContentTemplate
        | InputMessageContentToolCall
        | InputMessageContentToolResult
        | InputMessageContentRawText
        | InputMessageContentThought
        | InputMessageContentFile
        | InputMessageContentUnknown
    ) = Field(..., title="InputMessageContent")


class OpenAICustomToolFormatGrammar(BaseModel):
    grammar: OpenAIGrammarDefinition
    type: Literal["grammar"]


class OpenAICustomToolFormat(RootModel[OpenAICustomToolFormatText | OpenAICustomToolFormatGrammar]):
    root: OpenAICustomToolFormatText | OpenAICustomToolFormatGrammar


class ProviderTool(BaseModel):
    model_config = ConfigDict(
        extra="forbid",
    )
    scope: ProviderToolScope | None = None
    tool: Any


class StoredInputMessageContentThought(Thought):
    type: Literal["thought"]


class StoredInputMessageContent(
    RootModel[
        StoredInputMessageContentText
        | StoredInputMessageContentTemplate
        | StoredInputMessageContentToolCall
        | StoredInputMessageContentToolResult
        | StoredInputMessageContentRawText
        | StoredInputMessageContentThought
        | StoredInputMessageContentFile
        | StoredInputMessageContentUnknown
    ]
):
    root: (
        StoredInputMessageContentText
        | StoredInputMessageContentTemplate
        | StoredInputMessageContentToolCall
        | StoredInputMessageContentToolResult
        | StoredInputMessageContentRawText
        | StoredInputMessageContentThought
        | StoredInputMessageContentFile
        | StoredInputMessageContentUnknown
    ) = Field(..., title="StoredInputMessageContent")


class OpenAICustomTool(BaseModel):
    """
    `OpenAICustomTool` represents OpenAI's custom tool format, which allows
    for text or grammar-based tool definitions beyond standard function calling.
    Currently, this type is a wire + outbound + storage type so it forces a consistent format.
    This only applies to the Chat Completions API. The Responses API has a slightly different request
    shape so we implement a conversion in `responses.rs`.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    description: str | None = None
    format: OpenAICustomToolFormat | None = None
    name: str
    type: Literal["openai_custom"]


class Tool(RootModel[FunctionTool | OpenAICustomTool]):
    root: FunctionTool | OpenAICustomTool
    """
    `Tool` is the generic form for all tools that TensorZero itself manages.
    This includes function tools (the original kind) and OpenAI's custom tools
    (which support text and grammar formats). Future additions may include MCP and other standards.

    We store this type (serialized) in the Array(String) in the `dynamic_tools` column
    in the ChatInference, ChatInferenceDatapoint, and BatchModelInference tables.

    For the wire format, we use `DynamicTool` which wraps this enum with a custom deserializer
    that allows function tools to be specified without type tags for backward compatibility,
    while other tool types require explicit tagging.

    Notably, provider tools (like OpenAI websearch) are not part of this enum
    as there's not really anything we can do besides experiment with them.
    They are a separate type `ProviderTool`.
    """


class UpdateDynamicToolParamsRequest(BaseModel):
    """
    A request to update the dynamic tool parameters of a datapoint.
    """

    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured tools, that are also allowed.
    Modifying `additional_tools` DOES NOT automatically modify `allowed_tools`; `allowed_tools` must be explicitly updated to include
    new tools or exclude removed tools.
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is explicitly allowed to use.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared (we allow function-configured tools
    plus additional tools provided at inference time). If specified as a value, it will be set to the provided value.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared (we will use function-configured
    parallel tool calls). If specified as a value, it will be set to the provided value.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared (we will use function-configured
    tool choice). If specified as a value, it will be set to the provided value.
    """


class DynamicToolParams(BaseModel):
    """
    Wire/API representation of dynamic tool parameters for inference requests.

    This type is the **wire format** for tool configurations used in API requests and responses.
    It distinguishes between static tools (configured in the function) and dynamic tools
    (provided at runtime), allowing clients to reference pre-configured tools by name or
    provide new tools on-the-fly.

    # Purpose
    - Accept tool parameters in inference API requests (e.g., `/inference/{function_name}`)
    - Expose tool configurations in API responses for stored inferences
    - Support Python and TypeScript client bindings
    - Allow runtime customization of tool behavior

    # Fields
    - `allowed_tools`: Names of static tools from function config to use (subset selection)
    - `additional_tools`: New tools defined at runtime (not in static config)
    - `tool_choice`: Override the function's default tool choice strategy
    - `parallel_tool_calls`: Override whether parallel tool calls are enabled
    - `provider_tools`: Provider-specific tool configurations (not persisted to database)

    # Key Differences from ToolCallConfigDatabaseInsert
    - **Separate lists**: Maintains distinction between static (`allowed_tools`) and dynamic (`additional_tools`) tools
    - **By reference**: Static tools referenced by name, not duplicated
    - **Has provider_tools**: Can specify provider-specific tool configurations
    - **Has bindings**: Exposed to Python/TypeScript via `pyo3` and `ts_rs`

    # Conversion to Storage Format
    Converting from `DynamicToolParams` to `ToolCallConfigDatabaseInsert` is a **lossy** operation:
    1. Static tools (from `allowed_tools` names) are resolved from function config
    2. Dynamic tools (from `additional_tools`) are included as-is
    3. Both lists are merged into a single `tools_available` list
    4. The distinction between static and dynamic tools is lost
    5. `provider_tools` are dropped (not stored)

    Use `FunctionConfig::dynamic_tool_params_to_database_insert()` for this conversion.

    # Conversion from Storage Format
    Converting from `ToolCallConfigDatabaseInsert` back to `DynamicToolParams` reconstructs the original:
    1. `dynamic_tools` -> `additional_tools`
    2. `allowed_tools` -> `allowed_tools` (based on choice enum)
    3. Other fields copied directly

    Use `From<ToolCallConfigDatabaseInsert> for DynamicToolParams` for this conversion.

    # Example
    ```rust,ignore
    // API request with dynamic tool params
    let params = DynamicToolParams {
        allowed_tools: Some(vec!["calculator".to_string()]),  // Use only the calculator tool from config
        additional_tools: Some(vec![Tool {  runtime tool  }]),  // Add a new tool
        tool_choice: Some(ToolChoice::Required),
        parallel_tool_calls: Some(true),
        provider_tools: vec![],
    };

    // Convert to storage format
    let db_insert = function_config
        .dynamic_tool_params_to_database_insert(params, &static_tools)?
        .unwrap_or_default();

    // db_insert.tools_available now contains both the calculator tool (from config)
    // and the runtime tool (from additional_tools), merged together
    ```

    See also: [`ToolCallConfigDatabaseInsert`] for the storage/database format
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = []
    """
    Provider-specific tool configurations
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """


class InputMessage(BaseModel):
    """
    InputMessage and Role are our representation of the input sent by the client
    prior to any processing into LLM representations below.
    `InputMessage` has a custom deserializer that addresses legacy data formats that we used to support (see input_message.rs).
    """

    content: list[InputMessageContent]
    role: Role


class StoredInputMessage(BaseModel):
    """
    `StoredInputMessage` has a custom deserializer that addresses legacy data formats in the database (see below).
    """

    content: list[StoredInputMessageContent]
    role: Role


class Input(BaseModel):
    """
    API representation of an input to a model.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    messages: list[InputMessage] | None = []
    """
    Messages in the input.
    """
    system: System | None = None
    """
    System prompt of the input.
    """


class JsonInferenceDatapoint(BaseModel):
    auxiliary: str | None = None
    dataset_name: str
    episode_id: UUID | None = None
    function_name: str
    id: UUID
    input: Input
    is_custom: bool | None = False
    is_deleted: bool
    name: str | None = None
    output: JsonInferenceOutput | None = None
    output_schema: Any
    source_inference_id: UUID | None = None
    staled_at: str | None = None
    tags: dict[str, Any] | None = None
    updated_at: str


class StoredInput(BaseModel):
    """
    The input type that we directly store in ClickHouse.
    This is almost identical to `ResolvedInput`, but without `File` data.
    Only the object-storage path is actually stored in clickhouse
    (which can be used to re-fetch the file and produce a `ResolvedInput`).

    `StoredInputMessage` has a custom deserializer that addresses legacy data formats in the database.
    """

    model_config = ConfigDict(
        extra="forbid",
    )
    messages: list[StoredInputMessage] | None = []
    system: System | None = None


class StoredJsonInference(BaseModel):
    dispreferred_outputs: list[JsonInferenceOutput] | None = []
    episode_id: UUID
    function_name: str
    inference_id: UUID
    input: StoredInput
    output: JsonInferenceOutput
    output_schema: Any
    tags: dict[str, str] | None = {}
    timestamp: str
    variant_name: str


class UpdateChatDatapointRequestInternal(BaseModel):
    """
    An update request for a chat datapoint.
    """

    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured tools, that are also allowed.
    Modifying `additional_tools` DOES NOT automatically modify `allowed_tools`; `allowed_tools` must be explicitly updated to include
    new tools or exclude removed tools.
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is explicitly allowed to use.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared (we allow function-configured tools
    plus additional tools provided at inference time). If specified as a value, it will be set to the provided value.
    """
    id: UUID
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Metadata fields to update.
    Moving forward, don't nest these fields.
    """
    name: str | None = None
    """
    Datapoint name.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared. If specified as a value, it will
    be set to the provided value.
    """
    output: list[ContentBlockChatOutput] | None = None
    """
    Chat datapoint output.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared.
    Otherwise, it will overwrite the existing output (and can be an empty list).
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared (we will use function-configured
    parallel tool calls). If specified as a value, it will be set to the provided value.
    """
    provider_tools: list[ProviderTool] | None = None
    """
    Provider-specific tool configurations
    If omitted, it will be left unchanged. If specified as a value, it will be set to the provided value.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared.
    Otherwise, it will overwrite the existing tags.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared (we will use function-configured
    tool choice). If specified as a value, it will be set to the provided value.
    """
    tool_params: UpdateDynamicToolParamsRequest | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Datapoint tool parameters.
    Moving forward, don't nest these fields.
    """


class UpdateChatDatapointRequest(UpdateChatDatapointRequestInternal):
    """
    Request to update a chat datapoint.
    """

    type: Literal["chat"]


class UpdateJsonDatapointRequestInternal(BaseModel):
    """
    An update request for a JSON datapoint.
    """

    id: UUID
    """
    The ID of the datapoint to update. Required.
    """
    input: Input | None = None
    """
    Datapoint input. If omitted, it will be left unchanged.
    """
    metadata: DatapointMetadataUpdate | None = None
    """
    DEPRECATED (#4725 / 2026.2+): Metadata fields to update.
    Moving forward, don't nest these fields.
    """
    name: str | None = None
    """
    Datapoint name.

    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared. If specified as a value, it will
    be set to the provided value.
    """
    output: JsonDatapointOutputUpdate | None = None
    """
    JSON datapoint output.
    If omitted (which uses the default value `UNSET`), it will be left unchanged. If set to `None`, it will be cleared (represents edge case where
    inference succeeded but model didn't output relevant content blocks). Otherwise, it will overwrite the existing output.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. If omitted, it will be left unchanged. If specified as `null`, it will be set to `null`. If specified as a value, it will be set to the provided value.
    If not provided, the function's output schema will be used.
    """
    tags: dict[str, Any] | None = None
    """
    Datapoint tags. If omitted, it will be left unchanged. If empty, it will be cleared. Otherwise,
    it will be overwrite the existing tags.
    """


class ChatInferenceDatapoint(BaseModel):
    """
    Wire variant of ChatInferenceDatapoint for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces.
    """

    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    auxiliary: str | None = None
    dataset_name: str
    episode_id: UUID | None = None
    function_name: str
    id: UUID
    input: Input
    is_custom: bool | None = False
    is_deleted: bool
    name: str | None = None
    output: list[ContentBlockChatOutput] | None = None
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = []
    """
    Provider-specific tool configurations
    """
    source_inference_id: UUID | None = None
    staled_at: str | None = None
    tags: dict[str, Any] | None = None
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    updated_at: str


class CreateChatDatapointRequest(BaseModel):
    """
    A request to create a chat datapoint.
    """

    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    episode_id: UUID | None = None
    """
    Episode ID that the datapoint belongs to. Optional.
    """
    function_name: str
    """
    The function name for this datapoint. Required.
    """
    input: Input
    """
    Input to the function. Required.
    """
    name: str | None = None
    """
    Human-readable name for the datapoint. Optional.
    """
    output: list[ContentBlockChatOutput] | None = None
    """
    Chat datapoint output. Optional.
    """
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = []
    """
    Provider-specific tool configurations
    """
    tags: dict[str, Any] | None = None
    """
    Tags associated with this datapoint. Optional.
    """
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """


class CreateDatapointRequestChat(CreateChatDatapointRequest):
    """
    Request to create a chat datapoint.
    """

    type: Literal["chat"]


class CreateJsonDatapointRequest(BaseModel):
    """
    A request to create a JSON datapoint.
    """

    episode_id: UUID | None = None
    """
    Episode ID that the datapoint belongs to. Optional.
    """
    function_name: str
    """
    The function name for this datapoint. Required.
    """
    input: Input
    """
    Input to the function. Required.
    """
    name: str | None = None
    """
    Human-readable name for the datapoint. Optional.
    """
    output: JsonDatapointOutputUpdate | None = None
    """
    JSON datapoint output. Optional.
    """
    output_schema: Any | None = None
    """
    The output schema of the JSON datapoint. Optional.
    If not provided, the function's output schema will be used. If provided, it will be validated.
    """
    tags: dict[str, Any] | None = None
    """
    Tags associated with this datapoint. Optional.
    """


class DatapointChat(ChatInferenceDatapoint):
    """
    Wire variant of Datapoint enum for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces.
    """

    type: Literal["chat"]


class DatapointJson(JsonInferenceDatapoint):
    """
    Wire variant of Datapoint enum for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces.
    """

    type: Literal["json"]


class Datapoint(RootModel[DatapointChat | DatapointJson]):
    root: DatapointChat | DatapointJson = Field(..., title="Datapoint")
    """
    Wire variant of Datapoint enum for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces.
    """


class GetDatapointsResponse(BaseModel):
    """
    Response containing the requested datapoints.
    """

    datapoints: list[Datapoint]
    """
    The retrieved datapoints.
    """


class StoredChatInference(BaseModel):
    """
    Wire variant of StoredChatInference for API responses with Python/TypeScript bindings
    """

    additional_tools: list[Tool] | None = None
    """
    Tools that the user provided at inference time (not in function config), in addition to the function-configured
    tools, that are also allowed.
    """
    allowed_tools: list[str] | None = None
    """
    A subset of static tools configured for the function that the inference is allowed to use. Optional.
    If not provided, all static tools are allowed.
    """
    dispreferred_outputs: list[list[ContentBlockChatOutput]] | None = []
    episode_id: UUID
    function_name: str
    inference_id: UUID
    input: StoredInput
    output: list[ContentBlockChatOutput]
    parallel_tool_calls: bool | None = None
    """
    Whether to use parallel tool calls in the inference. Optional.
    If provided during inference, it will override the function-configured parallel tool calls.
    """
    provider_tools: list[ProviderTool] | None = []
    """
    Provider-specific tool configurations
    """
    tags: dict[str, str] | None = {}
    timestamp: str
    tool_choice: ToolChoice | None = None
    """
    User-specified tool choice strategy. If provided during inference, it will override the function-configured tool choice.
    Optional.
    """
    variant_name: str


class StoredInferenceChat(StoredChatInference):
    """
    Wire variant of StoredInference for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces
    """

    type: Literal["chat"]


class StoredInferenceJson(StoredJsonInference):
    """
    Wire variant of StoredInference for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces
    """

    type: Literal["json"]


class StoredInference(RootModel[StoredInferenceChat | StoredInferenceJson]):
    root: StoredInferenceChat | StoredInferenceJson
    """
    Wire variant of StoredInference for API responses with Python/TypeScript bindings
    This one should be used in all public interfaces
    """


class UpdateJsonDatapointRequest(UpdateJsonDatapointRequestInternal):
    """
    Request to update a JSON datapoint.
    """

    type: Literal["json"]


class UpdateDatapointRequest(RootModel[UpdateChatDatapointRequest | UpdateJsonDatapointRequest]):
    root: UpdateChatDatapointRequest | UpdateJsonDatapointRequest = Field(..., title="UpdateDatapointRequest")
    """
    A tagged request to update a single datapoint in a dataset.
    """


class UpdateDatapointsRequest(BaseModel):
    """
    Request to update one or more datapoints in a dataset.
    """

    datapoints: list[UpdateDatapointRequest]
    """
    The datapoints to update.
    """


class CreateDatapointRequestJson(CreateJsonDatapointRequest):
    """
    Request to create a JSON datapoint.
    """

    type: Literal["json"]


class CreateDatapointRequest(RootModel[CreateDatapointRequestChat | CreateDatapointRequestJson]):
    root: CreateDatapointRequestChat | CreateDatapointRequestJson = Field(..., title="CreateDatapointRequest")
    """
    A tagged request to create a single datapoint.
    """


class CreateDatapointsRequest(BaseModel):
    """
    Request to create datapoints manually.
    Used by the `POST /v1/datasets/{dataset_id}/datapoints` endpoint.
    """

    datapoints: list[CreateDatapointRequest]
    """
    The datapoints to create.
    """


class GetInferencesResponse(BaseModel):
    """
    Response containing the requested inferences.
    """

    inferences: list[StoredInference]
    """
    The retrieved inferences.
    """


class CreateDatapointsFromInferenceRequestParamsInferenceQuery(BaseModel):
    """
    Create datapoints from an inference query.
    """

    filters: InferenceFilter | None = None
    """
    Filters to apply when querying inferences, optional.
    """
    function_name: str
    """
    The function name to filter inferences by.
    """
    type: Literal["inference_query"]
    variant_name: str | None = None
    """
    Variant name to filter inferences by, optional.
    """


class CreateDatapointsFromInferenceRequestParams(
    RootModel[
        CreateDatapointsFromInferenceRequestParamsInferenceIds
        | CreateDatapointsFromInferenceRequestParamsInferenceQuery
    ]
):
    root: (
        CreateDatapointsFromInferenceRequestParamsInferenceIds
        | CreateDatapointsFromInferenceRequestParamsInferenceQuery
    ) = Field(..., title="CreateDatapointsFromInferenceRequestParams")
    """
    Parameters for creating datapoints from inferences.
    Can specify either a list of inference IDs or a query to find inferences.
    """


class AndDatapointFilter(BaseModel):
    """
    Logical AND of multiple filters
    """

    children: list[DatapointFilter]
    type: Literal["and"]


class OrDatapointFilter(BaseModel):
    """
    Logical OR of multiple filters
    """

    children: list[DatapointFilter]
    type: Literal["or"]


class NotDatapointFilter(BaseModel):
    """
    Logical NOT of a filter
    """

    child: DatapointFilter
    type: Literal["not"]


class DatapointFilter(
    RootModel[TagDatapointFilter | TimeDatapointFilter | AndDatapointFilter | OrDatapointFilter | NotDatapointFilter]
):
    root: TagDatapointFilter | TimeDatapointFilter | AndDatapointFilter | OrDatapointFilter | NotDatapointFilter = (
        Field(..., title="DatapointFilter")
    )
    """
    Filter tree for querying datapoints.
    This is similar to `InferenceFilter` but without metric filters, as datapoints don't have associated metrics.
    """


class InferenceFilterAnd(BaseModel):
    """
    Logical AND of multiple filters
    """

    children: list[InferenceFilter]
    type: Literal["and"]


class InferenceFilterOr(BaseModel):
    """
    Logical OR of multiple filters
    """

    children: list[InferenceFilter]
    type: Literal["or"]


class InferenceFilterNot(BaseModel):
    """
    Logical NOT of a filter
    """

    child: InferenceFilter
    type: Literal["not"]


class InferenceFilter(
    RootModel[
        InferenceFilterFloatMetric
        | InferenceFilterBooleanMetric
        | InferenceFilterTag
        | InferenceFilterTime
        | InferenceFilterAnd
        | InferenceFilterOr
        | InferenceFilterNot
    ]
):
    root: (
        InferenceFilterFloatMetric
        | InferenceFilterBooleanMetric
        | InferenceFilterTag
        | InferenceFilterTime
        | InferenceFilterAnd
        | InferenceFilterOr
        | InferenceFilterNot
    ) = Field(..., title="InferenceFilter")
    """
    Filters for querying inferences.
    """


class ListDatapointsRequest(BaseModel):
    """
    Request to list datapoints from a dataset with pagination and filters.
    Used by the `POST /v1/datasets/{dataset_id}/list_datapoints` endpoint.
    """

    filter: DatapointFilter | None = None
    """
    Optional filter to apply when querying datapoints.
    Supports filtering by tags, time, and logical combinations (AND/OR/NOT).
    """
    function_name: str | None = None
    """
    Optional function name to filter datapoints by.
    If provided, only datapoints from this function will be returned.
    """
    limit: conint(ge=0) | None = None
    """
    The maximum number of datapoints to return.
    Defaults to 20.
    """
    offset: conint(ge=0) | None = None
    """
    The number of datapoints to skip before starting to return results.
    Defaults to 0.
    """
    order_by: list[DatapointOrderBy] | None = None
    """
    Optional ordering criteria for the results.
    Supports multiple sort criteria (e.g., sort by timestamp then by search relevance).
    """
    page_size: conint(ge=0) | None = None
    """
    The maximum number of datapoints to return. Defaults to 20.
    Deprecated: please use `limit`. If `limit` is provided, `page_size` is ignored.
    """
    search_query_experimental: str | None = None
    """
    Text query to filter. Case-insensitive substring search over the datapoints' input and output.

    THIS FEATURE IS EXPERIMENTAL, and we may change or remove it at any time.
    We recommend against depending on this feature for critical use cases.

    Important limitations:
    - This requires an exact substring match; we do not tokenize this query string.
    - This doesn't search for any content in the template itself.
    - Quality is based on term frequency > 0, without any relevance scoring.
    - There are no performance guarantees (it's best effort only). Today, with no other
      filters, it will perform a full table scan, which may be extremely slow depending
      on the data volume.
    """


class ListInferencesRequest(BaseModel):
    """
    Request to list inferences with pagination and filters.
    Used by the `POST /v1/inferences/list_inferences` endpoint.
    """

    after: UUID | None = None
    """
    Optional inference ID to paginate after (exclusive).
    Returns inferences with IDs after this one (later in time).
    Cannot be used together with `before` or `offset`.
    """
    before: UUID | None = None
    """
    Optional inference ID to paginate before (exclusive).
    Returns inferences with IDs before this one (earlier in time).
    Cannot be used together with `after` or `offset`.
    """
    episode_id: UUID | None = None
    """
    Optional episode ID to filter inferences by.
    If provided, only inferences from this episode will be returned.
    """
    filter: InferenceFilter | None = None
    """
    Optional filter to apply when querying inferences.
    Supports filtering by metrics, tags, time, and logical combinations (AND/OR/NOT).
    """
    function_name: str | None = None
    """
    Optional function name to filter inferences by.
    If provided, only inferences from this function will be returned.
    """
    limit: conint(ge=0) | None = None
    """
    The maximum number of inferences to return.
    Defaults to 20.
    """
    offset: conint(ge=0) | None = None
    """
    The number of inferences to skip before starting to return results.
    Defaults to 0.
    """
    order_by: list[OrderBy] | None = None
    """
    Optional ordering criteria for the results.
    Supports multiple sort criteria (e.g., sort by timestamp then by metric).
    """
    output_source: InferenceOutputSource
    """
    Source of the inference output. Determines whether to return the original
    inference output or demonstration feedback (manually-curated output) if available.
    """
    search_query_experimental: str | None = None
    """
    Text query to filter. Case-insensitive substring search over the inferences' input and output.

    THIS FEATURE IS EXPERIMENTAL, and we may change or remove it at any time.
    We recommend against depending on this feature for critical use cases.

    Important limitations:
    - This requires an exact substring match; we do not tokenize this query string.
    - This doesn't search for any content in the template itself.
    - Quality is based on term frequency > 0, without any relevance scoring.
    - There are no performance guarantees (it's best effort only). Today, with no other
      filters, it will perform a full table scan, which may be extremely slow depending
      on the data volume.
    """
    variant_name: str | None = None
    """
    Optional variant name to filter inferences by.
    If provided, only inferences from this variant will be returned.
    """


CreateDatapointsFromInferenceRequestParamsInferenceQuery.model_rebuild()
AndDatapointFilter.model_rebuild()
OrDatapointFilter.model_rebuild()
NotDatapointFilter.model_rebuild()
InferenceFilterAnd.model_rebuild()
InferenceFilterOr.model_rebuild()
InferenceFilterNot.model_rebuild()
