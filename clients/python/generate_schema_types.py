#!/usr/bin/env python3
"""
Generate Python dataclasses from JSON Schema files.

This script reads JSON schema files generated from Rust types and creates
Python dataclasses that can be used with the TensorZero Python client.

Run this script from the clients/python directory:
    python generate_schema_types.py
"""

import subprocess
import sys
from pathlib import Path
from typing import List


def find_schema_files(schema_dir: Path) -> List[Path]:
    """Find all JSON schema files in the schema directory."""
    if not schema_dir.exists():
        print(f"Error: Schema directory not found: {schema_dir}", file=sys.stderr)
        sys.exit(1)

    schema_files = sorted(schema_dir.glob("*.json"))
    if not schema_files:
        print(f"Warning: No JSON schema files found in {schema_dir}", file=sys.stderr)

    return schema_files


def generate_dataclasses_from_schema(schema_file: Path, output_file: Path) -> None:
    """
    Generate Python dataclasses from a JSON schema file using datamodel-code-generator.

    Args:
        schema_file: Path to the JSON schema file
        output_file: Path to write the generated Python code
    """
    print(f"Generating dataclasses from {schema_file.name}...")

    try:
        # Use datamodel-code-generator to generate dataclasses
        # Note: We explicitly disable field-description to avoid syntax errors
        result = subprocess.run(
            [
                sys.executable,
                "-m",
                "datamodel_code_generator",
                "--input", str(schema_file),
                "--input-file-type", "jsonschema",
                "--output", str(output_file),
                "--output-model-type", "dataclasses.dataclass",
                "--use-standard-collections",  # Use list, dict instead of List, Dict
                "--use-union-operator",  # Use | instead of Union
                "--target-python-version", "3.9",
                "--disable-timestamp",  # Don't add generation timestamp
                "--use-annotated",  # Use Annotated for constraints
                "--field-constraints",  # Add field constraints
                "--use-field-description",
            ],
            capture_output=True,
            text=True,
            check=True
        )

        if result.stdout:
            print(result.stdout)
        if result.stderr:
            print(result.stderr, file=sys.stderr)

    except subprocess.CalledProcessError as e:
        print(f"Error generating dataclasses from {schema_file.name}:", file=sys.stderr)
        print(e.stderr, file=sys.stderr)
        sys.exit(1)
    except FileNotFoundError:
        print("Error: datamodel-code-generator not found.", file=sys.stderr)
        print("Install it with: pip install 'datamodel-code-generator[http]'", file=sys.stderr)
        sys.exit(1)


def combine_generated_files(temp_files: List[Path], output_file: Path) -> None:
    """
    Combine all generated dataclass files into a single module.

    Args:
        temp_files: List of temporary files containing generated code
        output_file: Final output file path
    """
    print(f"\nCombining generated dataclasses into {output_file}...")

    header = '''"""
Auto-generated Python dataclasses from JSON schemas.

This file is generated from JSON schemas in the tensorzero-core crate.
Do not edit this file manually - it will be overwritten.

Generated from schemas in: tensorzero-core/schemas/

To regenerate, run:
    python generate_schema_types.py
"""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Literal
from uuid import UUID

'''

    # Collect all generated code
    all_code_lines = []
    seen_imports = set()

    for temp_file in temp_files:
        if not temp_file.exists():
            continue

        code = temp_file.read_text()
        lines = code.split('\n')

        # Skip the auto-generated header and imports (we'll add our own)
        in_imports = False
        code_started = False
        skip_docstring = False

        for line in lines:
            stripped = line.strip()

            # Skip empty lines at the start
            if not stripped and not code_started:
                continue

            # Skip docstrings at top
            if stripped.startswith('"""') or stripped.startswith("'''"):
                if not code_started:
                    skip_docstring = not skip_docstring
                    continue
            if skip_docstring:
                continue

            # Track and skip imports (but remember additional ones)
            if stripped.startswith('from ') or stripped.startswith('import '):
                # Skip __future__ imports, standard library, and our header imports
                if '__future__' not in stripped and \
                   'dataclass' not in stripped and \
                   'typing' not in stripped and \
                   'Literal' not in stripped and \
                   'UUID' not in stripped and \
                   'Enum' not in stripped and \
                   'Any' not in stripped:
                    if stripped not in seen_imports:
                        seen_imports.add(stripped)
                continue

            # Start collecting code after imports
            if stripped and stripped.startswith('#'):
                # Comments after imports signal code start
                code_started = True

            if stripped and not stripped.startswith('from ') and not stripped.startswith('import '):
                code_started = True

            if code_started:
                all_code_lines.append(line)

    # Write combined file
    with output_file.open('w') as f:
        f.write(header)

        # Add any additional imports that were in generated files
        if seen_imports:
            f.write('\n')
            for import_line in sorted(seen_imports):
                f.write(import_line + '\n')

        f.write('\n\n')
        f.write('\n'.join(all_code_lines))
        f.write('\n')

    print(f"✓ Generated {output_file}")


def main() -> None:
    """Main entry point for the script."""
    # Determine paths
    script_dir = Path(__file__).parent
    schema_dir = script_dir / ".." / ".." / "schemas"
    schema_dir = schema_dir.resolve()
    output_dir = script_dir / "tensorzero"
    output_file = output_dir / "generated_types.py"
    temp_dir = script_dir / ".temp_schemas"

    print("=" * 70)
    print("Generating Python dataclasses from JSON schemas")
    print("=" * 70)
    print(f"Schema directory: {schema_dir}")
    print(f"Output file: {output_file}")
    print()

    # Find all schema files
    schema_files = find_schema_files(schema_dir)
    print(f"Found {len(schema_files)} schema files")
    print()

    # Create temp directory for individual generated files
    temp_dir.mkdir(exist_ok=True)
    temp_files = []

    try:
        # Generate dataclasses for each schema
        for schema_file in schema_files:
            temp_file = temp_dir / f"{schema_file.stem}.py"
            generate_dataclasses_from_schema(schema_file, temp_file)
            temp_files.append(temp_file)

        # Combine all generated files
        combine_generated_files(temp_files, output_file)

    finally:
        # Clean up temp files
        for temp_file in temp_files:
            if temp_file.exists():
                temp_file.unlink()
        if temp_dir.exists():
            temp_dir.rmdir()

    print()
    print("=" * 70)
    print("✓ Generation complete!")
    print("=" * 70)
    print()
    print("Generated types can be imported with:")
    print("    from tensorzero.generated_types import Input, DynamicToolParams, ...")


if __name__ == "__main__":
    main()
