nextest-version = { recommended = "0.9.87" }

[profile.default]
retries = { backoff = "fixed", count = 2, delay = "5s", jitter = true }
slow-timeout = { period = "10s", terminate-after = 3 }
# We look for tests with names containing "no_aws_credentials"
# These tests require that the surrounding environment has no AWS credentials
# (including in places like `~/.aws`)
# We don't have a good way of isolating these tests to ensure that they pass
# on developer machines (with AWS credentials set up), so we exclude them by default.
# On CI, we use the 'ci' profile, which runs all tests.
default-filter = "not test(no_aws_credentials)"

# Profiles config
# We use these profiles to define our major test groups.
# By using `default-filter` to specify our tests, we can further restrict the tests
# by using command-line '-E' flags, which are *intersected* with the default-filter
# If we instead defined these as cargo aliases with '-E' flags, then any additional
# command-line flags would be unioned with the predefined '-E' flags from the alias,
# which would prevent us from excluding more tests
# The profile can be set with '--profile <profile-name>' when using 'cargo nextest run',
# We also define cargo aliases for common use-cases (e.g. `cargo test-e2e`)
[profile.e2e]
retries = { backoff = "exponential", count = 4, delay = "5s", jitter = true, max-delay = "60s" }
default-filter = "not (test(batch)) and not (test(test_dummy_only) | test(clickhouse))"

[profile.batch]
default-filter = 'test(batch)'

[profile.clickhouse]
default-filter = 'test(test_dummy_only) | test(clickhouse)'

[profile.optimization]
default-filter = 'binary(optimization-live)'

[profile.optimization-mock]
default-filter = 'binary(optimization-mock)'


[[profile.optimization.overrides]]
# Settings for running optimization tests
# We may have to update this as we add faster optimization methods that shouldn't
# take as long
filter = 'test(slow_optimization)'
slow-timeout = { period = "21600s", terminate-after = 1 }
retries = { count = 0, backoff = "fixed", delay = "0s" }

# Note: use the following commands to debug test groups:
# cargo nextest show-config test-groups
# cargo nextest show-config test-groups --features e2e_tests

# Run E2E provider tests sequentially to avoid rate limits
[test-groups]
e2e_aws_bedrock = { max-threads = 2 }
e2e_aws_sagemaker_tgi = { max-threads = 1 }
# Our Sagemaker instance seems to be able to handle 2 concurrent requests
e2e_aws_sagemaker_openai = { max-threads = 2 }
e2e_groq = { max-threads = 1 }

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_bedrock::)'
test-group = 'e2e_aws_bedrock'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_sagemaker_openai::)'
test-group = 'e2e_aws_sagemaker_openai'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_sagemaker_tgi::)'
test-group = 'e2e_aws_sagemaker_tgi'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::groq::)'
test-group = 'e2e_groq'
# We have a low rate limit on Groq, so we often need to retry several times
retries = { backoff = "exponential", count = 8, delay = "5s", jitter = true, max-delay = "120s" }

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::vllm::)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::sglang::)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
filter = 'test(test_concurrent_clickhouse_migrations)'
# the test fails if migrations > 60s so we can kill it at 65
slow-timeout = { period = "65s" }

[[profile.default.overrides]]
# Settings for running batch tests
filter = 'test(batch)'
slow-timeout = { period = "15s", terminate-after = 3 }

[[profile.default.overrides]]
# Settings for running clickhouse tests, which can be very slow on ClickHouse Clouc
# (when spawning lots of concurrent inserts)
filter = 'test(clickhouse)'
slow-timeout = { period = "500s", terminate-after = 1 }

[[profile.default.overrides]]
# Settings for running unit tests
filter = 'not binary(e2e)'
retries = 0
slow-timeout = { period = "10s", terminate-after = 1 }

[[profile.default.overrides]]
filter = 'binary(e2e) and (test(providers::aws_bedrock::) or test(providers::aws_sagemaker))'
retries = { backoff = "exponential", count = 8, delay = "5s", jitter = true, max-delay = "120s" }
