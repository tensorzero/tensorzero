nextest-version = { recommended = "0.9.87" }

[profile.default]
retries = { backoff = "fixed", count = 2, delay = "5s", jitter = true }
slow-timeout = { period = "10s", terminate-after = 3 }
# We look for tests with names containing "no_aws_credentials"
# These tests require that the surrounding environment has no AWS credentials
# (including in places like `~/.aws`)
# We don't have a good way of isolating these tests to ensure that they pass
# on developer machines (with AWS credentials set up), so we exclude them by default.
# On CI, we use the 'ci' profile, which runs all tests.
default-filter = "not (test(no_aws_credentials) | test(export_bindings_) | test(export_schema_))"

[[profile.default.overrides]]
filter = 'test(evaluations)'
slow-timeout = { period = "20s", terminate-after = 5 }

# Profiles config
# We use these profiles to define our major test groups.
# By using `default-filter` to specify our tests, we can further restrict the tests
# by using command-line '-E' flags, which are *intersected* with the default-filter
# If we instead defined these as cargo aliases with '-E' flags, then any additional
# command-line flags would be unioned with the predefined '-E' flags from the alias,
# which would prevent us from excluding more tests
# The profile can be set with '--profile <profile-name>' when using 'cargo nextest run',
# We also define cargo aliases for common use-cases (e.g. `cargo test-e2e`)

# E2E test should be a catch-all, only excluding a few things:
# - batch tests
# - clickhouse tests
# - optimization mock tests run separately in mock-optimization-tests with the mock gateway config
# - binding generation
[profile.e2e]
retries = { backoff = "exponential", count = 4, delay = "5s", jitter = true, max-delay = "60s" }
default-filter = '''
not (test(batch) | test(test_dummy_only) | test(clickhouse::) | (test(db::) & !test(postgres) & !test(valkey)) | test(export_bindings_) | test(export_schema_) | binary(optimization-mock))
'''
junit.path = "junit.xml"

# E2E GEPA tests - integration tests with API calls and evaluation
[[profile.e2e.overrides]]
filter = 'binary(e2e_gepa)'
# Analysis or mutation using gpt-4.1-nano occasionally fails to generate a properly formatted output, so we add retries
slow-timeout = { period = "30s", terminate-after = 2 }

[profile.live-batch]
default-filter = 'test(batch) and not test(mock)'

[profile.mock-batch]
default-filter = 'test(batch) and test(mock)'

# ClickHouse-only tests are running across multiple Clickhouse versions and setups, so this is limited to only Clickhouse-specific tests:
# - test_dummy_only: basic sanity check
# - test(clickhouse::): tests with "clickhouse" in its module hierarchy
#   This excludes some endpoint tests with _clickhouse in the name
# - db::...: database test module excluding postgres/valkey specific tests
#   Some of the database tests are dynamically generated with _clickhouse and _postgres variants, and this only runs
#   the correct ones in the clickhouse test matrix.
#
# Note: all tests under db:: without postgres/valkey in the name/hierarchy are run in the clickhouse test matrix.
[profile.clickhouse]
default-filter = 'test(test_dummy_only) | test(clickhouse::) | (test(db::) & !test(postgres) & !test(valkey))'
junit.path = "junit.xml"

# Run only endpoint e2e tests (tests under tests/e2e/endpoints/)
# These run against both ClickHouse and Postgres-backed TensorZero gateway.
[profile.endpoints]
default-filter = 'binary(e2e) and test(endpoints::)'
junit.path = "junit.xml"
retries = { backoff = "exponential", count = 4, delay = "5s", jitter = true, max-delay = "60s" }

[profile.optimization]
default-filter = 'binary(optimization-live)'

[profile.optimization-mock]
default-filter = 'binary(optimization-mock)'

[profile.ci-unit.junit]
path = "junit.xml"

[[profile.optimization.overrides]]
# Settings for running optimization tests
# We may have to update this as we add faster optimization methods that shouldn't
# take as long
filter = 'test(slow_optimization)'
slow-timeout = { period = "21600s", terminate-after = 1 }
retries = { count = 0, backoff = "fixed", delay = "0s" }

# Note: use the following commands to debug test groups:
# cargo nextest show-config test-groups
# cargo nextest show-config test-groups --features e2e_tests

# Run E2E provider tests sequentially to avoid rate limits
[test-groups]
e2e_aws_bedrock = { max-threads = 2 }
e2e_aws_sagemaker_tgi = { max-threads = 1 }
# Our Sagemaker instance seems to be able to handle 2 concurrent requests
e2e_aws_sagemaker_openai = { max-threads = 2 }
e2e_groq = { max-threads = 1 }
# Run ClickHouse migration tests serially to avoid conflicts
clickhouse_migration = { max-threads = 1 }

[[profile.default.overrides]]
filter = 'test(test_otel_export)'
slow-timeout = { period = "60s", terminate-after = 4 }
# We sleep long enough that these tests should pass 100% of the time.
# Any flakiness indicates a potential bug that could cause us to lose spans,
# so we don't want to retry.
retries = 0

[[profile.default.overrides]]
filter = 'test(e2e_test_variant_failover)'
# This test makes lots of requests, so give it extra time for the slower Github runners.
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_bedrock::)'
test-group = 'e2e_aws_bedrock'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_sagemaker_openai::)'
test-group = 'e2e_aws_sagemaker_openai'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_sagemaker_tgi::)'
test-group = 'e2e_aws_sagemaker_tgi'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::groq::)'
test-group = 'e2e_groq'
# We have a low rate limit on Groq, so we often need to retry several times
retries = { backoff = "exponential", count = 8, delay = "5s", jitter = true, max-delay = "120s" }

[[profile.default.overrides]]
filter = 'test(test_clickhouse_migration_manager)'
test-group = 'clickhouse_migration'

[[profile.e2e.overrides]]
filter = 'test(experimentation)'
# We run with '-j 32', so prevent these relatively heavy tests from running in parallel with too many other things.
threads-required = 16

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::vllm::)'
slow-timeout = { period = "60s", terminate-after = 2 }

# Give these tests more time to run on CI, since we have a lot of
# load on the free Github runner
[[profile.default.overrides]]
filter = 'package(gateway)'
slow-timeout = { period = "20s", terminate-after = 2 }

# The proc-macro tests use `trybuild`, which runs compilation
# for various crates as part of the test. This can be slow on CI runners
# without a warm cache, so we give it a longer timeout.
[[profile.default.overrides]]
filter = 'package(tensorzero-derive)'
slow-timeout = { period = "30s", terminate-after = 3 }

[[profile.default.overrides]]
# Web search is very slow
filter = 'test(web_search)'
slow-timeout = { period = "120s", terminate-after = 2 }

# Image inference seems to be slow on many providers, so we give it a longer timeout
[[profile.default.overrides]]
filter = 'test(test_image_inference)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::sglang::)'
# The model we run on SGLang often fails to emit valid tool calls, so we need many retries
retries = { backoff = "fixed", count = 8, delay = "10s", jitter = true }
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
filter = 'test(test_concurrent_clickhouse_migrations)'
# the test fails if migrations > 60s so we can kill it at 65
slow-timeout = { period = "65s" }

[[profile.default.overrides]]
# Settings for running batch tests
filter = 'test(batch)'
slow-timeout = { period = "15s", terminate-after = 3 }

[[profile.default.overrides]]
# Settings for running clickhouse tests, which can be very slow on ClickHouse Cloud
# (when spawning lots of concurrent inserts)
filter = 'test(clickhouse)'
slow-timeout = { period = "900s", terminate-after = 1 }

[[profile.default.overrides]]
filter = 'test(test_concurrent_requests)'
# This test creates and uses lots of `reqwest::Client`s, which can be slow
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
# Settings for running mock optimization tests
filter = 'test(mock_optimization)'
retries = { backoff = "fixed", count = 2, delay = "5s", jitter = true }
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
# Settings for running unit tests
filter = 'not (binary(e2e) | test(export_bindings_) | test(export_schema_))'
retries = 0
slow-timeout = { period = "10s", terminate-after = 2 }

[[profile.default.overrides]]
filter = 'binary(e2e) and (test(providers::aws_bedrock::) or test(providers::aws_sagemaker))'
retries = { backoff = "exponential", count = 8, delay = "5s", jitter = true, max-delay = "120s" }

[[profile.default.overrides]]
# xAI reasoning multi-turn tests are slow due to reasoning model response times
filter = 'binary(e2e) and test(test_reasoning_multi_turn_)'
slow-timeout = { period = "60s", terminate-after = 2 }
