nextest-version = { recommended = "0.9.87" }

[profile.default]
retries = { backoff = "fixed", count = 2, delay = "5s", jitter = true }
slow-timeout = { period = "10s", terminate-after = 3 }
# We look for tests with names containing "no_aws_credentials"
# These tests require that the surrounding environment has no AWS credentials
# (including in places like `~/.aws`)
# We don't have a good way of isolating these tests to ensure that they pass
# on developer machines (with AWS credentials set up), so we exclude them by default.
# On CI, we use the 'ci' profile, which runs all tests.
default-filter = "not test(no_aws_credentials)"

[[profile.default.overrides]]
filter = 'test(evaluations)'
slow-timeout = { period = "20s", terminate-after = 3 }

# Profiles config
# We use these profiles to define our major test groups.
# By using `default-filter` to specify our tests, we can further restrict the tests
# by using command-line '-E' flags, which are *intersected* with the default-filter
# If we instead defined these as cargo aliases with '-E' flags, then any additional
# command-line flags would be unioned with the predefined '-E' flags from the alias,
# which would prevent us from excluding more tests
# The profile can be set with '--profile <profile-name>' when using 'cargo nextest run',
# We also define cargo aliases for common use-cases (e.g. `cargo test-e2e`)
[profile.e2e]
retries = { backoff = "exponential", count = 4, delay = "5s", jitter = true, max-delay = "60s" }
default-filter = "not (test(batch)) and not (test(test_dummy_only) | test(clickhouse) | test(db::))"
junit.path = "junit.xml"

[profile.batch]
default-filter = 'test(batch)'

[profile.clickhouse]
default-filter = 'test(test_dummy_only) | test(clickhouse) | test(db::)'
junit.path = "junit.xml"

[profile.optimization]
default-filter = 'binary(optimization-live)'

[profile.optimization-mock]
default-filter = 'binary(optimization-mock)'

[profile.ci-unit.junit]
path = "junit.xml"

[[profile.optimization.overrides]]
# Settings for running optimization tests
# We may have to update this as we add faster optimization methods that shouldn't
# take as long
filter = 'test(slow_optimization)'
slow-timeout = { period = "21600s", terminate-after = 1 }
retries = { count = 0, backoff = "fixed", delay = "0s" }

# Note: use the following commands to debug test groups:
# cargo nextest show-config test-groups
# cargo nextest show-config test-groups --features e2e_tests

# Run E2E provider tests sequentially to avoid rate limits
[test-groups]
e2e_aws_bedrock = { max-threads = 2 }
e2e_aws_sagemaker_tgi = { max-threads = 1 }
# Our Sagemaker instance seems to be able to handle 2 concurrent requests
e2e_aws_sagemaker_openai = { max-threads = 2 }
e2e_groq = { max-threads = 1 }
# Run ClickHouse migration tests serially to avoid conflicts
clickhouse_migration = { max-threads = 1 }

[[profile.default.overrides]]
filter = 'test(test_otel_export)'
slow-timeout = { period = "60s", terminate-after = 2 }
# We sleep long enough that these tests should pass 100% of the time.
# Any flakiness indicates a potential bug that could cause us to lose spans,
# so we don't want to retry.
retries = 0

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_bedrock::)'
test-group = 'e2e_aws_bedrock'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_sagemaker_openai::)'
test-group = 'e2e_aws_sagemaker_openai'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::aws_sagemaker_tgi::)'
test-group = 'e2e_aws_sagemaker_tgi'

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::groq::)'
test-group = 'e2e_groq'
# We have a low rate limit on Groq, so we often need to retry several times
retries = { backoff = "exponential", count = 8, delay = "5s", jitter = true, max-delay = "120s" }

[[profile.default.overrides]]
filter = 'test(test_clickhouse_migration_manager)'
test-group = 'clickhouse_migration'

[[profile.e2e.overrides]]
filter = 'test(experimentation)'
# We run with '-j 32', so prevent these relatively heavy tests from running in parallel with too many other things.
threads-required = 16

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::vllm::)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
# Web search is very slow
filter = 'test(websearch)'
slow-timeout = { period = "120s", terminate-after = 2 }

# Image inference seems to be slow on GCP Vertex Gemini, so we give it a longer timeout
[[profile.default.overrides]]
filter = 'test(providers::gcp_vertex_gemini::test_image_inference)'
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
filter = 'binary(e2e) and test(providers::sglang::)'
# The model we run on SGLang often fails to emit valid tool calls, so we need many retries
retries = { backoff = "fixed", count = 8, delay = "10s", jitter = true }
slow-timeout = { period = "60s", terminate-after = 2 }

[[profile.default.overrides]]
filter = 'test(test_concurrent_clickhouse_migrations)'
# the test fails if migrations > 60s so we can kill it at 65
slow-timeout = { period = "65s" }

[[profile.default.overrides]]
# Settings for running batch tests
filter = 'test(batch)'
slow-timeout = { period = "15s", terminate-after = 3 }

[[profile.default.overrides]]
# Settings for running clickhouse tests, which can be very slow on ClickHouse Cloud
# (when spawning lots of concurrent inserts)
filter = 'test(clickhouse)'
slow-timeout = { period = "900s", terminate-after = 1 }

[[profile.default.overrides]]
filter = 'test(test_concurrent_requests)'
# This test creates and uses lots of `reqwest::Client`s, which can be slow
slow-timeout = { period = "60s", terminate-after = 1 }

[[profile.default.overrides]]
# Settings for running unit tests
filter = 'not binary(e2e)'
retries = 0
slow-timeout = { period = "10s", terminate-after = 1 }

[[profile.default.overrides]]
filter = 'binary(e2e) and (test(providers::aws_bedrock::) or test(providers::aws_sagemaker))'
retries = { backoff = "exponential", count = 8, delay = "5s", jitter = true, max-delay = "120s" }
